{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- parts taken from https://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py\n",
    "#--- look there for nice graphics!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 225\n",
    "n_outliers = 25\n",
    "n_features = 2\n",
    "\n",
    "#------ the following generates a collection of data in R^2, the \"inliers\", mostly coming from one \n",
    "#------ normal distribution, then adds \"erroneous\" data, the \"outliers\", from another distribution\n",
    "\n",
    "# generate data\n",
    "gen_cov = np.eye(n_features)\n",
    "gen_cov[0, 0] = 2.\n",
    "# the following generates n_samples many vectors in R^{n_features}=R^2 using a \n",
    "# normal distribution with mean (0,0) (because not specified otherwise) and\n",
    "# covariance matrix gen_cov times gen_cov^T\n",
    "X = np.dot(np.random.randn(n_samples, n_features), gen_cov)\n",
    "# I don't know of a command that directly produces normal distributions with given mean and covariance matrix.\n",
    "# Instead one can produce _standard_ normally distributed data (i.e. mean zero, covariance matrix I), \n",
    "# in other words a sequence of independent normally distributed univariate data,\n",
    "# with the above command: np.random.randn(n_samples, n_features) \n",
    "# Then transform the result by multiplying with the \"square root\" of the desired covariance matrix \n",
    "# using the following formula: If X ~ N(x|m,S) then AX ~ N(x|Am, ASA^T) -- see the paragraph\n",
    "# before Remark 4.8.10 of the notes.\n",
    "print('Data points normally distributed with mean zero and the following covariance matrix:\\n',np.dot(gen_cov,gen_cov.T))\n",
    "\n",
    "# As a check let's compute the empirical mean vector and the empirical covariance matrix of the data \n",
    "# that we generated:\n",
    "print('Mean of the data:\\n[',np.mean(X.T[0]),', ',np.mean(X.T[1]), ']')\n",
    "print('Empirical covariance matrix:\\n',np.cov(X.T))\n",
    "# I use the transpose because np.cov by default interprets the elements of the nth row \n",
    "# as the nth coordinates of the samples, but our X is the other way round. \n",
    "# One could also use np.cov(X, rowvar=False) with the same effect.\n",
    "\n",
    "# we replace the last 25 data points by some outliers\n",
    "outliers_cov = np.eye(n_features)\n",
    "outliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.\n",
    "print('\\n Outliers normally distributed with mean zero and the following covariance matrix:\\n',np.dot(outliers_cov,outliers_cov.T))\n",
    "X[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the data set in a scatter plot\n",
    "# This function takes two arrays of vectors of length 2 and draws them in a scatter plot\n",
    "# the first list is drawn as black points, labeled 'inliers', \n",
    "# the second list is drawn as red points, labeled 'outliers'\n",
    "# one only needs to pass one list, the second is optional\n",
    "\n",
    "def draw_points(Points,Outliers=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # Plot data set\n",
    "    inlier_plot = ax.scatter(Points[:, 0], Points[:, 1],\n",
    "                         color='black', label='inliers')\n",
    "    if not(Outliers is None):\n",
    "        outlier_plot = ax.scatter(Outliers[:, 0], Outliers[:, 1],\n",
    "                          color='red', label='outliers')\n",
    "    ax.set_xlim(ax.get_xlim()[0], 10.)\n",
    "\n",
    "\n",
    "    # Create meshgrid of feature 1 and feature 2 values\n",
    "    xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n",
    "                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\n",
    "    zz = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # Add legend\n",
    "    if not(Outliers is None):\n",
    "        ax.legend([inlier_plot, outlier_plot],\n",
    "                  ['inliers', 'outliers'],\n",
    "                  loc=\"upper right\", borderaxespad=0)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "draw_points(X, X[-n_outliers: ,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- X is now the collection of all the data vectors.\n",
    "# Your task: Try to filter out the outliers from X, at least the bad ones,\n",
    "# without using that you know which elements of X are outliers.\n",
    "\n",
    "# ------------ General idea for finding outliers:-------------\n",
    "# 1. Compute the mean m of the data\n",
    "# 2. Compute the empirical covariance matrix C of the data\n",
    "# 3. Use it to define a distance between data points x and y:\n",
    "#     the square root of (x-y)^T C^(-1) (x-y) is called the Mahalanobis distance from x to y)\n",
    "# # The Mahalanobis distance can be seen as a metric (coming from an inner product given by C) \n",
    "# that is inherent in the data. This point of view is very nicely explained in the first answer of this forum post:\n",
    "# https://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance?rq=1&newreg=bc22f22ed57045d6be2d6adbee05e3a1\n",
    "#\n",
    "# 4. Now the idea is that outliers are those points that lie \"far away\" from the mean according to this metric.\n",
    "# \"Far away\" is of course not precise, but for example if one can safely assume that not more than 1/5 of\n",
    "# the data points are outliers, one can simply take the 4/5 of the points that are closest to the mean\n",
    "# and declare this subset free of outliers.\n",
    "# For this exercise we just search for 100 points of which we want to be pretty sure that they are not outliers.\n",
    "\n",
    "print('1. Mean of the contaminated data:\\n[',np.mean(X.T[0]),', ',np.mean(X.T[1]), ']')\n",
    "print('2. Empirical covariance matrix of the contaminated data:\\n',np.cov(X.T))\n",
    "\n",
    "\n",
    "# -------Your task (part a):--------\n",
    "# a)  Find the 100 points that are closest to the mean, in the sense described above, and plot them.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with this procedure is that the covariance matrix, which we use to define the distance\n",
    "# by which we then want to detect outliers, already is itself defined in terms of the contaminated data.\n",
    "# You can see above that is pretty different from the covariance matrix of the uncontaminated data.\n",
    "\n",
    "# A remedy that seems to work well in practice is to first find a better estimator for the uncontaminated \n",
    "# covariance matrix.\n",
    "\n",
    "\n",
    "# This estimator is called the Minimum Covariance Determinant Estimator.\n",
    "# It is computed as follows:\n",
    "\n",
    "# 1. Try to find that 100-element subset of X whose covariance matrix has the smallest determinant\n",
    "# Instead of really checking through all subsets, rather choose randomly 10000 such subsets\n",
    "# and take the one with smallest covariance matrix determinant out of these. Call it X_1.\n",
    "# --- you can e.g. use  sklearn.model_selection.train_test_split  for this random subset picking,\n",
    "# or np.random.choice for randomly picking indices between 0 and 124.\n",
    "\n",
    "# Then:\n",
    "# 2. compute the mean of X_1; call it m_1\n",
    "# 3. compute the empirical covariance matrix of X_1; call it C_1\n",
    "# This is the estimate for the covariance matrix of the uncontaminated data.\n",
    "\n",
    "# To remove outliers go on as follows:\n",
    "# 4. For each point x of our original X compute (x-m_1)^T C_1^(-1) (x-m_1)\n",
    "#        (let's call the square root of this number the C_1-Mahalanobis distance from x to m_1)\n",
    "# 5. Take the 100 points with smallest C_1-Mahalanobis distance from m_1.\n",
    "\n",
    "\n",
    "# -------Your task (part b):--------\n",
    "# b) Do these steps.\n",
    "#    Then plot the resulting set of 100 points.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABOUT USING THE Minimum Covariance Determinant Estimator IN PRACTICE:\n",
    "# In practice for this whole procedure one would simply use the following command:\n",
    "   from sklearn.covariance import MinCovDet\n",
    "   robust_cov = MinCovDet().fit(X)\n",
    "This directly does several iterations of the above process, I think...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
