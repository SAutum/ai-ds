{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f8ecc-c4c2-4035-8163-2d9dbec3c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm #optional, if you do not want to import remove tqdm() from loops!\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d943d-9359-43ad-9930-2903c8c6c45e",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "We again will be using the MNIST dataset. This time I prepared the dataset as a npy file. We will load the data visualize an example and the implement logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8baa6-58c4-498f-a6c0-e6f491d4e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if the file is in a different directory\n",
    "f_features = \"features.npy\"\n",
    "\n",
    "# change if the file is in a different directory\n",
    "f_labels = \"labels.npy\"\n",
    "\n",
    "# load the data\n",
    "features=np.load(f_features)\n",
    "labels=np.load(f_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6e785-2721-43a2-8246-d7920c5d7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(features[0,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6857c9-458c-4fe0-92c9-1b69a2491646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you know we need to split the data into training, validation and test\n",
    "x_train=features[0:4800,:,:]\n",
    "x_train=x_train.reshape((4800, 784))\n",
    "y_train=labels[0:4800].astype(int)\n",
    "\n",
    "x_val=features[4800:5400,:,:]\n",
    "x_val=x_val.reshape((600, 784))\n",
    "y_val=labels[4800:5400].astype(int)\n",
    "\n",
    "x_test=features[5400:6000,:,:]\n",
    "x_test=x_test.reshape((600, 784))\n",
    "y_test=labels[5400:6000].astype(int)\n",
    "\n",
    "# Note: Normally the split has to be random and stratified for the validation set and random for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4558976-9c71-4ef2-b8b3-710df7773806",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1667f-71d4-4563-920b-697305c06659",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression\n",
    "\n",
    "From the lecture we know that logistic regression is given by affined transformation of the data followed by applying the sigmoid function. Our first step is to implement the function we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc771958-1230-4e6f-aaa0-934be5c74c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, w):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : output tensor of dimension (N, C) \n",
    "\n",
    "    res should be the result of the matrix multiplication of an expanded feature tensor (1 column) with \n",
    "    the learnable parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    res=0\n",
    "    # TODO\n",
    "    # Implement the affine transformation\n",
    "    # Put your code here:\n",
    "\n",
    "    \n",
    "    # END\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442ce9b-1a33-4485-8593-9db7763e8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "\n",
    "w=np.ones((28*28+1,10)) # 28*28 are the number of features and the bias leads to +1\n",
    "res=layer(x_train, w)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d703a-3359-49a7-becb-223eec649917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need more function use the blocks below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9dd9c5-619d-4469-ae93-43c185a42af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edd4ea-7c4f-40d2-9b86-66342c7fc767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa2eb8-200d-4fec-bdd7-d6f904207be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e8333ed-eb24-45f0-b7c0-312a0c8b30b8",
   "metadata": {},
   "source": [
    "## 2.1 Loss Function\n",
    "In exercise sheet 0, we just guessed values, but now we are smarter! First we need to define an appropriate loss function. The dataset has ten target classes, so we want to implement cross-entropy loss:\n",
    "\n",
    "$\\mathcal{L}=\\sum_{y}1\\{\\hat{y}=y\\}(-\\log[p(y)])$ \n",
    "\n",
    "The $p(y)$ is given by the softmax function\n",
    "\n",
    "$p(y_i)=\\frac{e^{x_i}}{\\sum_ie^{x_i}}$\n",
    "\n",
    "So the softmax should return a vector representing the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26102408-2311-48ef-87e7-97b5537c3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : Prediction tensor of dimension (N, C). \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : Softmax transformed tensor of dimension (N,C)\n",
    "\n",
    "    res should be the result of the softmax transformation of y.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    res=0\n",
    "    # TODO \n",
    "    # Implement the softmax function\n",
    "    # Put your code here:\n",
    "\n",
    "\n",
    "    # END\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcbdb3-674a-4a11-8e49-4c4dbbf98d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : Prediction tensor of dimension (N,1)\n",
    "\n",
    "    res should be the classification of our model \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    res=0\n",
    "    # TODO\n",
    "    # Put your code here:\n",
    "\n",
    "    \n",
    "    # END\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3b1da-0f38-4675-b8db-c6445fbbc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need more function use the blocks below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63726f59-df1c-4047-bc59-bb5720ada436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0a507-b1b8-49d5-b096-833dcd8bceb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b90210-2504-4202-816c-e1ecf5d9d2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0888da5b-2695-43dd-be4f-c654942b1703",
   "metadata": {},
   "source": [
    "## 2.2 Optimization\n",
    "We have already learned about optimization algorithms. In this exercise we want to learn more about gradient descent, stochastic gradient descent and Newtonâ€™s method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3da02-c0ab-4744-9c6d-105752822069",
   "metadata": {},
   "source": [
    "### 2.2.1 Gradient Descent\n",
    "For gradient descent we need to updates our parameters using the steepest descent of the gradient with respect to the parameter. It is given by the equation:\n",
    "\n",
    "$w_{n+1}=w_n-\\epsilon_n\\nabla\\mathcal{L(w_n)}$\n",
    "\n",
    "$\\epsilon_n$ is the learning rate and a hyperparameter of our optimization approach. We can calculate the gradient by using the composition rule for derivatives.\n",
    "\n",
    "The challenge is to broadcast the right dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f68ab-a9f4-4c97-8d96-49ff6dbd9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classTensor(y, C):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : class vector of dimension N containing the true classes\n",
    "    C : number of classes\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : class tensor of dimension (N,C)\n",
    "\n",
    "    We want to transform the vector into a binary tensor. If res_ij=1 then it means that at sample i we have class j. Otherwise \n",
    "    res_ij=0.\n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "\n",
    "    res=0\n",
    "    # TODO\n",
    "    # Implement the tensor transformation\n",
    "    # Put your code here:\n",
    "\n",
    "    \n",
    "    # END\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bf99f-0a75-4f6b-94af-86cdfb26cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, w, learningRate):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : feature tensor of dimension (N,M)\n",
    "    y : class vector of dimension N containing the true classes\n",
    "    w : learnable parameters of dimension (M+1, C)\n",
    "\n",
    "    N is the number of samples, M the number of features and C the number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : updated learnable parameters\n",
    "\n",
    "    \"\"\"\n",
    "    res=0\n",
    "    # TODO \n",
    "    # Implement one update iteration\n",
    "    \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880a7c3-5bc5-4e51-88bb-b84be2b21a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need more function use the blocks below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a6a14-7f17-4f4a-9e0d-463b115a6689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65dcc67-9bc0-4487-977b-ed1eb70aaf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b85ea2-c3d3-4be0-b59f-e1e0c7ed034c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd4ca5c-6bc3-4aa4-ba35-d1b34d6a887a",
   "metadata": {},
   "source": [
    "### 2.2.2 Stochastic Gradient Descent\n",
    "In this section implement stochastic gradient descent by writing the function \"def stochasticGradientDescent(...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5d38b-9254-4260-98e8-6a93c96202f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Write the function stochasticGradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47222ce8-2836-4347-a18e-ac651ff9e95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a939d71-3e2b-4cdf-8c1d-21a4273a1cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0291e2-3120-420e-ba16-1eea017bd3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c488145-c036-488a-a736-46636c24cc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ab1fa70-d69a-4c19-b7f2-46c8109e8750",
   "metadata": {},
   "source": [
    "### 2.2.3 Newtonâ€™s method\n",
    "In this section implement Newton's method by writing the function \"def newtonMethod(...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553e22f-b7ca-4ca5-aee6-8a4e29ef6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Write the function newtonMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13971e-5f4f-476a-ad3c-02d65e0b79a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6aace5-0996-4c6e-bae9-961c6354e111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54b920-893e-4e51-9cea-4c5eee08962a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc8ab9-e8b9-4873-9d7b-f7f28ca00fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7fc664-a615-441d-bf07-235a6fb23a2e",
   "metadata": {},
   "source": [
    "# 3. Training\n",
    "Now use the MNIST dataset to train a classifier and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572200c7-7fa2-4312-953a-ae98ff6d5121",
   "metadata": {},
   "source": [
    "# 3.1 Train and Plot Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973b7dd-41a7-4486-b914-7a41360ee73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "acc_val=[]\n",
    "\n",
    "n_epochs=100\n",
    "learningRate=0.001\n",
    "\n",
    "# Maybe you find a better way to initialize the learnable parameters\n",
    "w=np.zeros((785,10))+1\n",
    "\n",
    "for e in tqdm(range(n_epochs)):\n",
    "\n",
    "    w=gradientDescent(x_train,y_train, w,learningRate)\n",
    "    pred=model(x_train,w)\n",
    "\n",
    "    acc_train.append(accuracy_score(y_train, pred))\n",
    "    acc_val.append(accuracy_score(y_val, model(x_val,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507856a8-a582-462f-86a1-33f2b787cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.plot(acc_val, c=\"orange\", label=\"val\", lw=1.5)\n",
    "ax.plot(acc_train, c=\"blue\", label=\"train\",lw=1.5)\n",
    "ax.grid(color='gray', linestyle='dashed', alpha=0.3)\n",
    "ax.legend(loc=\"lower right\", fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c448ab-9115-4691-85ee-e100287cafc0",
   "metadata": {},
   "source": [
    "## 3.2 TODO: Implement training for stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebada2-052f-4023-ad27-8634a11e69b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efef086-15fb-4f12-9f5b-f6191e9452ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b857a2-d003-4577-a07f-47572641ae1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5b9f06-9c7b-436d-9183-f3a80135835c",
   "metadata": {},
   "source": [
    "## 3.3 TODO: Implement training for newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ded60-0d06-44ed-9789-2a451bf4fc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27a630-6f5d-4659-abc3-e5a0a0c2901c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb4a7a-15a0-4426-9e45-7d365cda7378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
