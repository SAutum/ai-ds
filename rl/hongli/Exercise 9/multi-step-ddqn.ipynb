{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić\n",
    "\n",
    "### Multi-step double DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch needs to be installed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short PyTorch intro\n",
    "\n",
    "In this notebook we will use the PyTorch library to implement deep Q-networks.  \n",
    "The main advantages of PyTorch over NumPy are fast parallel computing on GPUs and automatic differentiation, i.e., gradients are computed for us automatically.\n",
    "\n",
    "In this notebook we will not make use of a GPU, since the network is fairly small and computation on the CPU might even be faster.\n",
    "\n",
    "PyTorch uses *tensors*, which are very similar to NumPy arrays, but they can be stored on a specific device (e.g., a GPU).  \n",
    "The following code creates a tensor of shape `(8,3)`, i.e., a matrix in $\\mathbb{R}^{8 \\times 3}$, filled with zeros and stores it in the main memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((8, 3))\n",
    "print(x)\n",
    "print()\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational efficiency (and for more stable training), we will work with *batches* of data.  \n",
    "For example, instead of updating the action value function for a single state, we update multiple states at once.  \n",
    "E.g., if the *batch size* is 64, then the tensors for a single update might have the following shapes:\n",
    "- `states`: `(64,5)`, i.e., the tensor contains 64 states and each state is a 5-dimensional vector\n",
    "- `actions`: `(64)`, i.e., the tensor contains 64 actions\n",
    "- `rewards`: `(64)`, i.e., the tensor contains 64 rewards\n",
    "\n",
    "If tensors have the same batch size, we can work with them as if they are single values.  \n",
    "E.g., if we have `rewards_0` with shape `(64)` and `rewards_1` with shape `(64)`, we can compute:  \n",
    "\n",
    "`rewards_0 + gamma * rewards1`  \n",
    "\n",
    "which will be a new tensor of shape `(64)`.\n",
    "\n",
    "We already provide the code that takes care of automatic differentiation and optimization,  \n",
    "so it should be sufficient if you understand the concept of tensors and batches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Networks (DQN)\n",
    "  \n",
    "As seen in the lecture, DQN learns an action-value function by minimizing the following mean squared error:\n",
    "$$\n",
    "\\mathbb{E}_{s_t, a_t, r_{t+1}, s_{t+1}} [ (y_t - \\hat{q}(s_t, a_t, w))^2 ],\n",
    "$$\n",
    "where $w$ are the parameters of the \"online\" network, $w^-$ are the parameters of the fixed target network, and\n",
    "$y_t$ is the update target, which is defined as\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\max_a \\hat{q}(s_{t+1}, a, w^-).\n",
    "$$\n",
    "\n",
    "**Double DQN:**  \n",
    "The idea of double DQN (https://arxiv.org/abs/1509.06461) is to apply double Q-learning to the deep Q-network.  \n",
    "The simplest way of doing this is to use the online network to select actions, but the fixed target network to compute the next action value.  \n",
    "This leads to the update target\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\hat{q}(s_{t+1}, \\arg\\max_a \\hat{q}(s_{t+1}, a, w), w^-).\n",
    "$$\n",
    "\n",
    "**Multi-step DQN:**  \n",
    "The DQN can also be extended to $n$-step returns (https://arxiv.org/abs/1710.02298), which leads to the following update target:\n",
    "$$\n",
    "y_t = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n \\max_a \\hat{q}(s_{t+n}, a, w^-)\n",
    "$$\n",
    "\n",
    "**Multi-step double DQN:**  \n",
    "By combining both approaches we get the following update target, which we will use in this exercise:\n",
    "$$\n",
    "\\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n \\hat{q}(s_{t+n}, \\arg\\max_a \\hat{q}(s_{t+n}, a, w), w^-)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay Memory\n",
    "DQN stores transitions in a replay memory. Batches of transitions are sampled from the memory and used to update the DQN.  \n",
    "We have already implemented most of the replay memory below. Your task is to extend the functionality to sample multiple steps at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity, rng):\n",
    "        # create a queue that removes old transitions when capacity is reached\n",
    "        self.transitions = collections.deque([], maxlen=capacity)\n",
    "\n",
    "        # random number generator used for sampling batches\n",
    "        self.rng = rng\n",
    "\n",
    "    def append(self, transition):\n",
    "        # append a transition (a tuple) to the queue\n",
    "        self.transitions.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, n_steps):\n",
    "        # randomly sample a list of indices\n",
    "        idx = self.rng.choice(len(self.transitions) - n_steps + 1, batch_size, replace=False)\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO Modify the code to support multi-step sampling. The parameter  #\n",
    "        # `n_steps` specifies how many steps should be sampled. The return    #\n",
    "        # value `sequence` should be a list of batches for each time step.    #\n",
    "        # This means that the result of the existing code below should be     #\n",
    "        # stored in sequence[0], and you have to add the following steps to   #\n",
    "        # the list (with the start indices from `idx`, but with an offset).   #\n",
    "        #######################################################################\n",
    "        # select the transitions using the indices\n",
    "        sequence = []\n",
    "        for j in range(n_steps):\n",
    "            transitions = [self.transitions[i + j] for i in idx]\n",
    "\n",
    "        # convert the list of transitions into multiple batches, one for each modality (states, actions, rewards, ...), i.e.\n",
    "        # transitions: (s_1, a_1, r_1, ...), ..., (s_n, a_n, r_n, ...)\n",
    "        # get converted into\n",
    "        # batches: (s_1, ..., s_n), (a_1, ..., a_n), (r_1, ..., r_n), ...\n",
    "        # they also get converted into PyTorch tensors\n",
    "            batches = tuple(torch.as_tensor(np.array(batch)) for batch in zip(*transitions))\n",
    "            sequence.append(batches)\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sample()...\n",
      "2/2 tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_replay_memory():\n",
    "    yield 'sample()'\n",
    "\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    memory = ReplayMemory(capacity=10, rng=rng)\n",
    "\n",
    "    transitions = [\n",
    "        (1, 2, 3),\n",
    "        (4, 5, 6),\n",
    "        (7, 8, 9),\n",
    "        (10, 11, 12),\n",
    "        (13, 14, 15),\n",
    "        (16, 17, 18)\n",
    "    ]\n",
    "\n",
    "    for transition in transitions:\n",
    "        memory.append(transition)\n",
    "\n",
    "    test_data = [\n",
    "        (4, 3, {(0, 0): [1, 4, 7], (1, 2): [11, 14, 17], (2, 3): [6, 9, 12]}),\n",
    "        (2, 2, {(0, 1): [1, 4], (2, 0): [12, 15]})\n",
    "    ]\n",
    "\n",
    "    for batch_size, n_steps, samples in test_data:\n",
    "        sequence = memory.sample(batch_size, n_steps)\n",
    "        yield isinstance(sequence, (tuple, list)), 'sequence must be a tuple or list'\n",
    "        yield len(sequence) == n_steps, 'Length of sequence is incorrect'\n",
    "\n",
    "        for batches in sequence:\n",
    "            yield isinstance(batches, (tuple, list)), 'Each value of sequence must be a tuple or list (containing the batches)'\n",
    "            for batch in batches:\n",
    "                yield torch.is_tensor(batch), 'Each batch must be a tensor'\n",
    "                yield batch.shape[0] == batch_size, 'Batch size is incorrect'\n",
    "\n",
    "        for (modality, index), values in samples.items():\n",
    "            for step in range(n_steps):\n",
    "                yield sequence[step][modality][index] == values[step], 'Tensor contains incorrect value'\n",
    "\n",
    "        yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_replay_memory())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-network\n",
    "\n",
    "The class `MultiStepDDQN` is a subclass of `torch.nn.Module`, which takes care of the automatic differentiation.  \n",
    "Your task is to finish the implementation of `compute_q()`, `compute_max_q()` and `compute_loss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepDDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        # create a simple neural network with two fully-connected layers\n",
    "        # and a ReLU nonlinearity\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def compute_q(self, states, actions):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions has shape (batch_size)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO Implement the computation of q(s,a), given states and actions   #\n",
    "        # using self.network. The neural network takes as input a batch of     #\n",
    "        # states and produces a tensor of size (batch_size, num_actions)       #\n",
    "        # HINT: You can use the function torch.gather with inputs              #\n",
    "        # q_all and actions                                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        # select q[s,a], which has shape (batch_size); use torch.gather\n",
    "        q = torch.gather(q_all, 1, actions.unsqueeze(1))\n",
    "\n",
    "        ########################################################################\n",
    "        # End of your code.                                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        return q\n",
    "\n",
    "    def compute_max_q(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO Implement the computation of max_a' q(s,a'), given states       #\n",
    "        # using self.network. The neural network takes as input a batch of     #\n",
    "        # states and produces a tensor of size (batch_size, num_actions)       #\n",
    "        ########################################################################\n",
    "\n",
    "        # select max_a' q[s,a'], which has shape (batch_size)\n",
    "        actions = self.compute_arg_max(states)\n",
    "        max_q = self.compute_q(states, actions)\n",
    "\n",
    "        ########################################################################\n",
    "        # End of your code.                                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        return max_q\n",
    "\n",
    "    def compute_arg_max(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        # select argmax_a' q[s,a'], which has shape (batch_size)\n",
    "        actions = q_all.argmax(dim=1)\n",
    "        return actions\n",
    "\n",
    "    def compute_loss(self, target_dqn, sequence, gamma):\n",
    "        # sequence contains a list of batches from the replay memory\n",
    "\n",
    "        # the number of steps is the length of the sequence\n",
    "        n_steps = len(sequence)\n",
    "\n",
    "        # sequence[0] contains the states and actions that should be updated\n",
    "        states_0, actions_0, rewards_0, terminations_0, next_states_0 = sequence[0]\n",
    "\n",
    "        # sequence[n_steps - 1] contains the next states that are used for bootstrapping\n",
    "        states_n, actions_n, rewards_n, terminations_n, next_states_n = sequence[n_steps - 1]\n",
    "\n",
    "        # turn off gradient computation\n",
    "        with torch.no_grad():\n",
    "            #######################################################################\n",
    "            # TODO Implement the multi-step DDQN targets as described above.      #\n",
    "            # You can see how we obtained the batches for the first and last time #\n",
    "            # step above. You need to do the same for all n steps in order to     #\n",
    "            # calculate the targets.                                              #\n",
    "            #                                                                     #\n",
    "            # Hint #1: The implementation for the normal 1-step DQN (without      #\n",
    "            # double Q-learning) would look like this:                            #\n",
    "            # max_q = target_dqn.compute_max_q(next_states_n)                     #\n",
    "            # targets = rewards_0 + gamma * (~terminations) * max_q               #\n",
    "            # Hint #2: Iterate backwards.                                         #\n",
    "            #######################################################################\n",
    "\n",
    "            actions_max = self.compute_arg_max(next_states_n)\n",
    "            max_q = target_dqn.compute_q(next_states_n, actions_max)\n",
    "            targets = (~terminations_n) * max_q * gamma ** n_steps\n",
    "\n",
    "            for k in (range(n_steps)):\n",
    "                states_k, actions_k, rewards_k, terminations_k, next_states_k =\\\n",
    "                    sequence[k]\n",
    "                targets += gamma ** k * rewards_k * (~terminations_k)\n",
    "\n",
    "\n",
    "            #######################################################################\n",
    "            # End of your code.                                                   #\n",
    "            #######################################################################\n",
    "\n",
    "        # compute predictions q[s,a]\n",
    "        q = self.compute_q(states_0, actions_0)\n",
    "\n",
    "        # compute mean squared error between q[s,a] and targets\n",
    "        loss = torch.mean((q - targets.detach()) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing compute_loss()...\n",
      "0/1 tests passed!\n",
      "Test #1 failed:\n",
      "Loss is incorrect (error = 0.2976400173477174)\n",
      "Loss is incorrect (error = 0.4582302895050048)\n",
      "Loss is incorrect (error = 0.37866271687866204)\n"
     ]
    }
   ],
   "source": [
    "def test_multi_ddqn():\n",
    "    yield 'compute_loss()'\n",
    "\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    state_dim = 5\n",
    "    num_actions = 3\n",
    "    ddqn = MultiStepDDQN(state_dim, num_actions)\n",
    "    with torch.no_grad():\n",
    "        layer1, layer2 = ddqn.network[0], ddqn.network[2]\n",
    "        layer1.weight[:] = torch.as_tensor(rng.normal(0, 0.01, layer1.weight.shape))\n",
    "        layer2.weight[:] = torch.as_tensor(rng.normal(0, 0.01, layer2.weight.shape))\n",
    "        nn.init.zeros_(layer1.bias)\n",
    "        nn.init.zeros_(layer2.bias)\n",
    "\n",
    "    memory = ReplayMemory(20, rng)\n",
    "    state = rng.standard_normal(state_dim, dtype=np.float32)\n",
    "    for _ in range(100):\n",
    "        action = rng.integers(num_actions)\n",
    "        reward = rng.standard_normal(dtype=np.float32)\n",
    "        terminated = rng.uniform(0.0, 1.0) < 0.2\n",
    "        next_state = rng.standard_normal(state_dim, dtype=np.float32)\n",
    "        memory.append((state, action, reward, terminated, next_state))\n",
    "        if terminated:\n",
    "            state = rng.standard_normal(state_dim, dtype=np.float32)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    gamma = 0.8\n",
    "    for batch_size, n_steps, expected_loss in [(6, 3, 2.01768165), (16, 0, 1.17322925), (2, 8, 2.35127176)]:\n",
    "        sequence = memory.sample(batch_size=6, n_steps=3)\n",
    "        loss = ddqn.compute_loss(ddqn, sequence, gamma).item()\n",
    "        yield np.isclose(loss, expected_loss), f'Loss is incorrect (error = {abs(expected_loss - loss)})'\n",
    "\n",
    "    yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_multi_ddqn())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole environment\n",
    "\n",
    "We will evaluate the agent on the CartPole environment.  \n",
    "You can read more about it here: https://www.gymlibrary.dev/environments/classic_control/cart_pole/  \n",
    "(we use the `-v0` version)\n",
    "\n",
    "Define a function to create the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = f'CartPole-v0'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We already implemented the training code below.  \n",
    "The agent should reach a score of 200 at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(reward_steps, reward_history)\n\u001b[1;32m    108\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplots_adjust(wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/pyplot.py:612\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/IPython/core/formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/IPython/core/formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/IPython/core/formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/IPython/core/pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 170\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2204\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2053\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2054\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:496\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:444\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    446\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    447\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/figure.py:3161\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3163\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3165\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/patches.py:632\u001b[0m, in \u001b[0;36mPatch.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    630\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m    631\u001b[0m affine \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_paths_with_artist_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Work around a bug in the PDF and SVG renderers, which\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# do not draw the hatches if the facecolor is fully\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# transparent, but do if it is None.\u001b[39;49;00m\n\u001b[1;32m    638\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/patches.py:617\u001b[0m, in \u001b[0;36mPatch._draw_paths_with_artist_properties\u001b[0;34m(self, renderer, draw_path_args_list)\u001b[0m\n\u001b[1;32m    614\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m PathEffectRenderer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_path_effects(), renderer)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m draw_path_args \u001b[38;5;129;01min\u001b[39;00m draw_path_args_list:\n\u001b[0;32m--> 617\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdraw_path_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    620\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rein_learning/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:131\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize random number generators\n",
    "seed = 1\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "num_steps = 30000        # number of steps in the environment\n",
    "batch_size = 64          # number of transitions in a batch\n",
    "replay_capacity = 1000   # number of transitions that are stored in memory\n",
    "gamma = 0.99             # discount factor\n",
    "n_steps = 3              # number of steps for multi-step target\n",
    "learning_rate = 0.001    # learning rate\n",
    "target_interval = 100    # synchronize the target network after this number of steps\n",
    "\n",
    "# decrease the epsilon-greedy probability linearly\n",
    "epsilon_start = 1.0     # start value\n",
    "epsilon_end = 0.05      # end value\n",
    "epsilon_steps = 5000    # number of steps to reach the end value\n",
    "\n",
    "# create the environment and replay memory\n",
    "env = create_env(seed)\n",
    "memory = ReplayMemory(replay_capacity, rng)\n",
    "\n",
    "# create the deep Q-network and optimizer\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "dqn = MultiStepDDQN(state_dim, num_actions)\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
    "\n",
    "# create the target Q-network\n",
    "target_dqn = copy.deepcopy(dqn)\n",
    "\n",
    "# store values for learning curves\n",
    "loss_steps = []\n",
    "loss_history = []\n",
    "reward_steps = []\n",
    "reward_history = []\n",
    "episode_i = 0\n",
    "reward_sum = 0.0\n",
    "\n",
    "# main training loop\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    update_plot = False\n",
    "\n",
    "    # compute the epsilon-greedy probability\n",
    "    epsilon = epsilon_start + (epsilon_end - epsilon_start) * min(step / epsilon_steps, 1.0)\n",
    "\n",
    "    # select an action\n",
    "    if rng.random() < epsilon:\n",
    "        action = rng.choice(num_actions)\n",
    "    else:\n",
    "        dqn.eval()  # switch to evaluation mode\n",
    "        # convert state to tensor, need to call unsqueeze(0) for a batch size of 1\n",
    "        action = dqn.compute_arg_max(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "\n",
    "    # execute the action\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # convert reward to float32 (should be faster)\n",
    "    reward = np.float32(reward)\n",
    "\n",
    "    # add the transition to replay memory\n",
    "    transition = (state, action, reward, terminated, next_state)\n",
    "    memory.append(transition)\n",
    "    reward_sum += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        reward_steps.append(episode_i)\n",
    "        reward_history.append(reward_sum)\n",
    "        episode_i += 1\n",
    "        state, _ = env.reset()\n",
    "        reward_sum = 0.0\n",
    "        update_plot = True\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # check if enough transitions in replay memory\n",
    "    if step >= batch_size + n_steps:\n",
    "        # sample a batch of transitions from the replay memory\n",
    "        sequence = memory.sample(batch_size, n_steps)\n",
    "        # minimize the loss function using SGD\n",
    "        dqn.train()  # switch to training mode\n",
    "        loss = dqn.compute_loss(target_dqn, sequence, gamma)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            loss_steps.append(step)\n",
    "            loss_history.append(loss.item())\n",
    "            update_plot = True\n",
    "\n",
    "    # synchronize the target network if necessary\n",
    "    if step > 0 and step % target_interval == 0:\n",
    "        target_dqn = copy.deepcopy(dqn)\n",
    "\n",
    "    # plot curves for the loss and sum of rewards\n",
    "    if update_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "        axes[0].set_xlabel('step')\n",
    "        axes[0].set_ylabel('loss')\n",
    "        axes[0].plot(loss_steps, loss_history)\n",
    "        axes[1].set_xlabel('episode')\n",
    "        axes[1].set_ylabel('sum of rewards')\n",
    "        axes[1].plot(reward_steps, reward_history)\n",
    "        plt.subplots_adjust(wspace=0.2)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dqn.eval()\n",
    "    state, _ = env.reset()\n",
    "    render()\n",
    "    reward_sum = 0.0\n",
    "\n",
    "    while True:\n",
    "        state = torch.as_tensor(state).unsqueeze(0)\n",
    "        action = dqn.compute_arg_max(state).item()\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        render(f'action: {action}, sum of rewards: {reward_sum:.2f}')\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rein_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
