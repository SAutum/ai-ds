{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. M. Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "\n",
    "In general, the update rule of policy gradient can be written as:\n",
    "$$\n",
    "w \\leftarrow w + \\alpha \\, \\Phi_t \\nabla_w \\log \\pi_w(a_t | s_t)\n",
    "$$\n",
    "where $w$ is the weight vector, $\\alpha$ is the learning rate, and $\\Phi_t \\in \\mathbb{R}$ indicates how good it was to select the action $a_t$ in the state $s_t$.  \n",
    "The default REINFORCE algorithm uses $\\Phi_t = \\gamma^t g_t$, i.e., the discounted return (calculated with a Monte Carlo sample).  \n",
    "\n",
    "As we will see in the lecture, there are other valid options for $\\Phi_t$ that have lower variance.  \n",
    "In this notebook, we will use $\\Phi_t = \\gamma^t (g_t - b)$, where $b$ (baseline) is the average of all returns that we have seen so far.  \n",
    "\n",
    "Your task is to implement the following algorithm.\n",
    "\n",
    "**REINFORCE with average return baseline:**\n",
    "- Initialize counter: $N \\leftarrow 0$\n",
    "- Initialize baseline: $b \\leftarrow 0$\n",
    "- Loop forever:\n",
    "  - Generate an episode $s_0, a_0, r_1, \\ldots, s_{T-1}, a_{T-1}, r_T$ following $\\pi_w$\n",
    "  - Initialize return: $g \\leftarrow 0$\n",
    "  - For $t = T - 1, \\ldots, 0$:  \n",
    "    - Compute return: $g \\leftarrow r_{t + 1} + \\gamma g$\n",
    "    - Increment counter: $N \\leftarrow N + 1$\n",
    "    - Update baseline: $b \\leftarrow b + (g - b) / N$\n",
    "    - Update weights: $w \\leftarrow w + \\alpha \\, \\gamma^t (g - b) \\, \\nabla_w \\log \\pi_w(a_t | s_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Policy\n",
    "\n",
    "In this notebook, we will implement a linear softmax policy, i.e.,\n",
    "$$\n",
    "\\pi_w(a | s) = \\frac{\\exp\\left( x(s, a)^\\top w \\right)}{\\sum_{a'} \\exp\\left( x(s, a')^\\top w \\right)},\n",
    "$$\n",
    "where $x(s, a) \\in \\mathbb{R}^d$ is a feature vector of the state and action, and $w \\in \\mathbb{R}^d$ is a weight vector.\n",
    "\n",
    "The score function of this policy, which is required for the REINFORCE algorithm, can be computed with\n",
    "$$\n",
    "\\nabla_w \\log \\pi_w(a | s) = x(s, a) - \\sum_{a'} \\pi(a' | s) \\, x(s, a').\n",
    "$$\n",
    "\n",
    "We will use the following features:\n",
    "$$\n",
    "x(s, a) = [\\ \\underbrace{0, \\ldots, 0}_{D \\,\\cdot\\, a},\\ s, \\underbrace{0, \\ldots, 0}_{D \\,\\cdot\\, (M - a - 1)}]^\\top \\in \\mathbb{R}^{D \\,\\cdot\\, M}\n",
    "$$\n",
    "where $s \\in \\mathbb{R}^D$, $a \\in \\{0, \\ldots, M - 1\\}$.  \n",
    "In other words, we put the entire state vector at a different location depending on the action, and fill the rest with zeros.  \n",
    "This has the effect that $x(s, a)^\\top w$ will use a different set of weights for each action, as the other ones are multiplied by zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the features as described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionFeatures:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        # check state space\n",
    "        if not (isinstance(observation_space, gym.spaces.Box) and len(observation_space.shape) == 1):\n",
    "            raise ValueError('Observation space must be a real-valued vector')\n",
    "\n",
    "        # check action space\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Action space must be discrete')\n",
    "\n",
    "        self.state_dim = observation_space.shape[0]\n",
    "        self.num_actions = action_space.n\n",
    "\n",
    "    def __call__(self, state, action):\n",
    "        state_dim = self.state_dim\n",
    "        # create vector with zeros\n",
    "        x = np.zeros(state_dim * self.num_actions, dtype=np.float32)\n",
    "        # copy state into its slot\n",
    "        i = action * state_dim\n",
    "        x[i:i + state_dim] = state\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following base class for learned policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # sample an action from pi(. | state)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def score_function(self, state, action):\n",
    "        # compute gradient of log pi(action | state)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, state, action, advantage):\n",
    "        # update the weights for the given state and action, weighted by the advantage\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class implements softmax policies as described above.  \n",
    "The learning rate `alpha` is also part of the policy.  \n",
    "The parameter `feature_fn` is a function that computes a feature vector for a given state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLinearPolicy(Policy):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, alpha, feature_fn, rng=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # learning rate\n",
    "\n",
    "        # check the action space\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Action space must be discrete')\n",
    "        self.num_actions = action_space.n\n",
    "\n",
    "        # function that converts a state and action into a feature vector\n",
    "        self.feature_fn = feature_fn\n",
    "\n",
    "        # determine the size of the feature vectors\n",
    "        # sample a random state and action and convert them to a feature vector\n",
    "        sample_features = feature_fn(observation_space.sample(), action_space.sample())\n",
    "        if not (isinstance(sample_features, np.ndarray) and len(sample_features.shape) == 1):\n",
    "            raise ValueError('Features must be real-valued vectors')\n",
    "\n",
    "        # size of feature vectors\n",
    "        self.feature_dim = sample_features.shape[0]\n",
    "\n",
    "        # initialize random number generator\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "\n",
    "        # reset the weights\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # create weight vector with size of feature vectors, initialize with zeros\n",
    "        self.weights = np.zeros(self.feature_dim, dtype=np.float32)\n",
    "\n",
    "    def compute_probs(self, state):\n",
    "        # utility function that computes the probabilities pi(. | state) for all actions,\n",
    "        # it also returns the computed feature vectors\n",
    "        w = self.weights\n",
    "        num_actions = self.num_actions\n",
    "        xs = []  # list of feature vectors for each action\n",
    "        #######################################################################\n",
    "        # TODO Compute the action probabilities for the given state using the #\n",
    "        # softmax as described above. Use self.feature_fn(state, action) to   #\n",
    "        # compute feature vectors. Also store the feature vectors in the list #\n",
    "        # `xs`, since they can be reused later.                               #\n",
    "        #######################################################################\n",
    "\n",
    "        probs = np.zeros(num_actions)\n",
    "        for action in range(num_actions):\n",
    "            x = self.feature_fn(state, action)\n",
    "            xs.append(x)\n",
    "            # compute the probability of the action\n",
    "            probs[action] = np.exp(np.dot(w, x))\n",
    "        probs /= np.sum(probs)\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        probs = probs.astype(np.float32, copy=False)\n",
    "        return probs, xs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # sample an action given a state\n",
    "        # compute action probabilities\n",
    "        probs, _ = self.compute_probs(state)\n",
    "        # sample from the distribution\n",
    "        action = np.random.choice(self.num_actions, p=probs)\n",
    "        return action\n",
    "\n",
    "    def score_function(self, state, action):\n",
    "        w = self.weights\n",
    "        num_actions = self.num_actions\n",
    "        #######################################################################\n",
    "        # TODO Compute the score function for the given state and action. Use #\n",
    "        # the function compute_probs() to obtain the probabilities and the    #\n",
    "        # feature vectors.                                                    #\n",
    "        #######################################################################\n",
    "\n",
    "        probs, xs = self.compute_probs(state)\n",
    "        score = xs[action] - np.dot(probs, xs)\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        score = score.astype(np.float32, copy=False)\n",
    "        return score\n",
    "\n",
    "    def update(self, state, action, advantage):\n",
    "        w = self.weights\n",
    "        alpha = self.alpha\n",
    "        #######################################################################\n",
    "        # TODO Update the weight vector as described above. (advantage = phi) #\n",
    "        #######################################################################\n",
    "\n",
    "        w += alpha * advantage * self.score_function(state, action)\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        self.weights = w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing compute_probs()...\n",
      "2/2 tests passed!\n",
      "\n",
      "Testing score_function()...\n",
      "2/2 tests passed!\n",
      "\n",
      "Testing update()...\n",
      "2/2 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# new test with fixed expected scores\n",
    "\n",
    "def test_softmax_policy():\n",
    "    rng = np.random.default_rng(seed=17)\n",
    "    obs_space = gym.spaces.Box(np.zeros(3, dtype=np.float32), np.ones(3, dtype=np.float32))\n",
    "    act_space = gym.spaces.Discrete(4)\n",
    "    features = ActionFeatures(obs_space, act_space)\n",
    "    policy = SoftmaxLinearPolicy(obs_space, act_space, 0.07, features, rng=rng)\n",
    "\n",
    "    sample_state = lambda: rng.uniform(0.0, 1.0, obs_space.shape).astype(np.float32, copy=False)\n",
    "    sample_action = lambda: rng.choice(act_space.n)\n",
    "\n",
    "    yield 'compute_probs()'\n",
    "\n",
    "    policy.weights = rng.standard_normal(policy.weights.shape).astype(np.float32, copy=False)\n",
    "\n",
    "    for expected_probs in [\n",
    "        np.array([0.90269875, 0.01361316, 0.04949653, 0.03419157], dtype=np.float32),\n",
    "        np.array([0.74848235, 0.05044989, 0.12121326, 0.07985448], dtype=np.float32)\n",
    "    ]:\n",
    "        state = sample_state()\n",
    "        probs, xs = policy.compute_probs(state)\n",
    "        if (yield from rl_tests.check_numpy_array(probs, 'probs', shape=expected_probs.shape, dtype=np.float32)):\n",
    "            yield np.allclose(probs, expected_probs), \\\n",
    "                f'Computed probabilities are incorrect (error = {np.sum(np.abs(probs - expected_probs))})'\n",
    "\n",
    "        yield isinstance(xs, list) and len(xs) == act_space.n, \\\n",
    "            f'xs must be a list of length {act_space.n} (got {len(xs)})'\n",
    "        for a, x in enumerate(xs):\n",
    "            yield np.allclose(x, features(state, a)), f'xs contains wrong feature vectors (for action {a})'\n",
    "        yield None\n",
    "\n",
    "    yield 'score_function()'\n",
    "\n",
    "    for expected_score in [\n",
    "        np.array([-0.0542655,  -0.04997427, -0.13998121,  0.14724284,  0.13559912,  0.37982202, -0.03655295, -0.0336624,  -0.09429058, -0.05642439, -0.05196244, -0.1455502 ], dtype=np.float32),\n",
    "        np.array([-0.31696385, -0.64511997, -0.5715783,  -0.02204133, -0.04486096, -0.03974695, 0.39505485,  0.8040593,   0.7123991,  -0.05604962, -0.1140784,  -0.10107382], dtype=np.float32)\n",
    "    ]:\n",
    "        score = policy.score_function(sample_state(), sample_action())\n",
    "        if (yield from rl_tests.check_numpy_array(score, 'score', shape=expected_score.shape, dtype=np.float32)):\n",
    "            yield np.allclose(score, expected_score), \\\n",
    "                f'Computed score is incorrect (error = {np.sum(np.abs(score - expected_score))})'\n",
    "        yield None\n",
    "\n",
    "    yield 'update()'\n",
    "\n",
    "    for expected_weights in [\n",
    "        np.array([-0.20219066, -2.27896578,  2.03422714, -2.17468828, -2.08132293, -1.2758126, 0.55199572,  1.78623307, -0.24874753, -0.38633042,  0.17751255, -0.36193669], dtype=np.float32),\n",
    "        np.array([-0.1025072,   0.16325331,  0.56471321, -0.81238355, -1.32787222, -0.94905908, 0.52641521, -0.85278105, -0.77391747,  1.15396637, -0.78571158, -0.89402242], dtype=np.float32)\n",
    "    ]:\n",
    "        policy.weights = rng.standard_normal(policy.weights.shape)\n",
    "        policy.update(sample_state(), sample_action(), rng.uniform(-1.0, 1.0))\n",
    "        yield np.allclose(policy.weights, expected_weights), \\\n",
    "            f'Updated weights are incorrect (error = {np.sum(np.abs(policy.weights - expected_weights))})'\n",
    "        yield None\n",
    "\n",
    "rl_tests.run_tests(test_softmax_policy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate the algorithm on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = f'CartPole-v1'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "pygame is not installed, run `pip install \"gymnasium[classic-control]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:258\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m create_env(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m render \u001b[38;5;241m=\u001b[39m rl_gui\u001b[38;5;241m.\u001b[39mcreate_renderer(env, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      5\u001b[0m render()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\ai-ds\\rl\\hongli\\Exercise 10\\rl_gui.py:23\u001b[0m, in \u001b[0;36mcreate_renderer\u001b[1;34m(env, fps, figsize)\u001b[0m\n\u001b[0;32m     21\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39mfigsize)\n\u001b[0;32m     22\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m img \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m     24\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m fig\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     )\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:301\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:361\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         )\n\u001b[1;32m--> 361\u001b[0m result \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     _check_render_return(env\u001b[38;5;241m.\u001b[39mrender_mode, result)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:261\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[classic-control]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install \"gymnasium[classic-control]\"`"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAD7CAYAAAAMyN1hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADMUlEQVR4nO3UsRHAIBDAsJD9d34m4FxCIU3gymtm5gPg6L8dAPA6owQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECEYJEIwSIBglQDBKgGCUAMEoAYJRAgSjBAhGCRCMEiAYJUAwSoBglADBKAGCUQIEowQIRgkQjBIgGCVAMEqAYJQAwSgBglECBKMECBv0jgXy3dACmAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum:.1f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the policy using REINFORCE.  \n",
    "You can play around with the hyperparameters and try to find a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m  \u001b[38;5;66;03m# number of episodes to train\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# create the environment\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m env \u001b[38;5;241m=\u001b[39m create_env(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# create the feature function\u001b[39;00m\n\u001b[0;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m ActionFeatures(env\u001b[38;5;241m.\u001b[39mobservation_space, env\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_env' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.00015      # learning rate\n",
    "gamma = 1.0          # discount factor\n",
    "num_episodes = 5000  # number of episodes to train\n",
    "\n",
    "# create the environment\n",
    "env = create_env(seed=42)\n",
    "# create the feature function\n",
    "features = ActionFeatures(env.observation_space, env.action_space)\n",
    "# create the softmax policy\n",
    "policy = SoftmaxLinearPolicy(env.observation_space, env.action_space, alpha, features)\n",
    "\n",
    "# store the sum of rewards per episode for plotting\n",
    "reward_sums = []\n",
    "\n",
    "# REINFORCE algorithm starts here\n",
    "# initialize counter and baseline\n",
    "count = 0\n",
    "baseline = 0.0\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    # generate an episode\n",
    "    state, _ = env.reset()\n",
    "    states = [state]\n",
    "    rewards = [0.0]\n",
    "    actions = []\n",
    "    while True:\n",
    "        # select action using the policy\n",
    "        action = policy.select_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # store the initial return (for plotting)\n",
    "    reward_sums.append(sum(rewards))\n",
    "\n",
    "    # update the policy using the episode\n",
    "    T = len(actions)\n",
    "    g = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        #######################################################################\n",
    "        # TODO Compute the return and update the counter and baseline as      #\n",
    "        # described in the algorithm at the top of this notebook.             #\n",
    "        # Then compute the advantage (= phi), which is used to update the     #\n",
    "        # weights in the line below your code.                                #\n",
    "        #######################################################################\n",
    "\n",
    "        g += gamma * g + rewards[t]\n",
    "        count += 1\n",
    "        baseline += (g - baseline) / count\n",
    "        advantage = g - baseline\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        # update weights\n",
    "        policy.update(states[t], actions[t], advantage)\n",
    "\n",
    "plt.title('CartPole')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Sum of rewards')\n",
    "plt.plot(reward_sums, c='C0', alpha=0.3)\n",
    "plt.plot(np.convolve(reward_sums, np.ones(100) / 100, mode='valid'), c='C0');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of training, the sum of rewards should be around 500, but it might be very noisy.  \n",
    "Evaluate the learned policy (run again if you have bad luck):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "state, _ = env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "\n",
    "while True:\n",
    "    action = policy.select_action(state)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'action: {action}, sum of rewards: {reward_sum:.1f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
