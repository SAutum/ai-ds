{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3UX-37U_MBB"
      },
      "source": [
        "# Exercise 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr5Zvjec_MBE"
      },
      "source": [
        "## Group ID: \n",
        "*   Person1\n",
        "*   Person2\n",
        "*   Person3\n",
        "\n",
        "## Exercise day: Tuesday/Wednesday\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description\n",
        "In this exercise you will implement a Tokenizer class that will be used to tokenize a given text.\n",
        "A token will be a short sequence of bytes and tokenization will be about splitting up the text into those short substrings and then representing it in terms of token ids.\n",
        "This allows language models to work on a better text representation than just using a character level model as in the last exercise.\n",
        "We will use the so called \"Byte Pair Encoding\" (BPE) algorithm to tokenize the text. The BPE algorithm is a simple algorithm that iteratively merges the most frequent pair of bytes/ids in the text. This notebook will guide you through the implementation of the Tokenizer class. To make the representation of the text easier, we will use integer ids to represent the bytes of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: The tokenizer is not part of the model, it is a preprocessing step that is used to tokenize the text before feeding it to the model. Therefore, it can be trained on a different dataset than the model.\n",
        "Once trained, the tokenizer is capable of encoding and decoding any given text to/from a list of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tasks\n",
        "1. Implement the `get_stats` method that will return a dictionary containing the frequency of each pair of characters in the text. (0.5 points)\n",
        "1. Implement the `merge` method that will merge the most frequent pair of characters in the text. (0.5 points)\n",
        "1. Implement the `fit` method that will train the tokenizer on the given text. (1 point)\n",
        "1. Implement the `encode` method that will encode the given text to a list of tokens. (1 point)\n",
        "1. Implement the `decode` method that will decode the given list of tokens to a text. (1 point)\n",
        "1. Implement the `BPE` class. This class should contain the previously implemented methods. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_stats(ids):\n",
        "    \"\"\" Returns a dictionary with the number of times each pair of ids appears in the input list \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
        "expected = {(1, 2): 3, (2, 3): 3, (3, 1): 2}\n",
        "assert get_stats(ids) == expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge(ids, pair, new_id):\n",
        "    \"\"\"Merge every pair of elements in ids that are equal to pair into a single element new_id\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [0,1,2,3,0,1,2,3]\n",
        "pair = (0,1)\n",
        "new_id = 4\n",
        "new_ids = merge(ids, pair, new_id)\n",
        "expected = [4,2,3,4,2,3]\n",
        "assert new_ids == expected, f\"Merge failed new_ids actual: {new_ids} expected: {expected}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit(ids, max_iter=1000):\n",
        "    \"\"\"Fit the model to the data by merging recursively the most common pairs of ids max_iter times or until no pair appears more than once. If two pairs have the same frequency, the one that appears first is chosen.\n",
        "    Returns the fitted model and a dictionary with the merges containing the new_id for each pair.\n",
        "    To ensure each new_id is unique (and our results are comparable), it is set to the maximum id in the list plus one.\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [0,1,2,3,0,1,2,3]\n",
        "num_iterations = 1\n",
        "new_ids, merges = fit(ids,num_iterations)\n",
        "assert new_ids == [4,2,3,4,2,3], f\"Wrong ids after fit {new_ids}\"\n",
        "assert merges == {(0,1):4}, f\"Wrong merges after fit {merges}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [0,1,2,3,0,1,2,3]\n",
        "num_iterations = 2\n",
        "new_ids, merges = fit(ids,num_iterations)\n",
        "assert new_ids == [5,3,5,3], f\"Wrong ids after fit {new_ids}\"\n",
        "assert merges == {(0,1):4, (4,2):5}, f\"Wrong merges after fit {merges}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(ids, merges):\n",
        "    \"\"\"Encode the input list of ids using the merges dictionary\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [0,1,2,3,0,1,2,3]\n",
        "merges = {(0,1):4}\n",
        "encoded_ids = encode(ids, merges)\n",
        "expected = [4,2,3,4,2,3]\n",
        "assert encoded_ids == expected, f\"Wrong encoded ids {encoded_ids} expected: {expected}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(ids, merges):\n",
        "    \"\"\"Decode the input list of ids using the merges dictionary\"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids = [6,6]\n",
        "merges = {(0,1):4, (4,2):5, (5,3):6}\n",
        "decoded_ids = decode(ids, merges)\n",
        "expected = [0,1,2,3,0,1,2,3]\n",
        "assert decoded_ids == expected, f\"Wrong decoded ids {decoded_ids} expected: {expected}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BPE:\n",
        "    def __init__(self, max_iter:int=1000):\n",
        "        pass\n",
        "\n",
        "    def fit(self, text):\n",
        "        pass\n",
        "\n",
        "    def encode(self, text):\n",
        "        pass\n",
        "\n",
        "    def decode(self, ids):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process. The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\" # cleaned version of the first paragraph of the Wikipedia page on LLMs\n",
        "ids = list(map(int, text.encode('utf-8')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe = BPE(max_iter=1000)\n",
        "bpe.fit(text)\n",
        "encoded_text = bpe.encode(text)\n",
        "decoded_text = bpe.decode(encoded_text)\n",
        "assert text == decoded_text, \"Decoding failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation: \n",
        "\n",
        "Let us check what a tokenizer will give us for training a simple language model.\n",
        "\n",
        "We will use the tiny Shakespeare dataset and train with and without tokenization and compare generated text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute tokenizer\n",
        "with open('input.txt') as f:\n",
        "    text = f.read()\n",
        "bpe = BPE(max_iter=1000)\n",
        "bpe.fit(text)\n",
        "enc = bpe.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(enc[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(bpe.decode(enc[:200]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, enc_text, blocksize):\n",
        "        self.data = enc_text\n",
        "        self.blocksize = blocksize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)-self.blocksize-1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx+self.blocksize]\n",
        "        y = self.data[idx+self.blocksize]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long), "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPNextTokenPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, embed_dim=32, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(block_size * embed_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        emb_flat = emb.view(-1, self.block_size * self.embed_dim)\n",
        "\n",
        "        h = torch.relu(self.bn1(self.fc1(emb_flat)))\n",
        "        h = torch.relu(self.bn2(self.fc2(h)))\n",
        "        logits = self.fc3(h)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate new text based on continuing provided one\n",
        "def generate(model, block_size, starting_text):\n",
        "    model.eval()\n",
        "    assert len(starting_text) >= block_size\n",
        "    x = torch.tensor(starting_text, dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            logits = model(x[-block_size:])\n",
        "            next_token = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
        "            starting_text = starting_text + [next_token.item()]\n",
        "            x = torch.cat([x, next_token.squeeze(0)])\n",
        "    return starting_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "def produce_example_text(model, block_size, tokenizer):\n",
        "    starting_text = \"\"\"We are accounted poor citizens, the patricians good.\n",
        "What authority surfeits on would relieve us: if they\n",
        "would yield us but the superfluity, while it were\n",
        "wholesome, we might guess they relieved us humanely;\n",
        "but they think we are too dear: the leanness that\n",
        "afflicts us, the object of our misery, is as an\n",
        "inventory to particularise their abundance;'\n",
        "\"\"\"\n",
        "    tokenized_starting_text = tokenizer.encode(starting_text)\n",
        "    continuation = generate(model, block_size, tokenized_starting_text)\n",
        "    return tokenizer.decode(continuation[len(tokenized_starting_text):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with untrained MLP:\n",
        "vocab_size = max(bpe.vocab)+1\n",
        "block_size = 32\n",
        "mlp_untrained = MLPNextTokenPredictor(vocab_size, block_size)\n",
        "mlp_untrained.to(device)\n",
        "continuation = produce_example_text(mlp_untrained, block_size, bpe)\n",
        "print(continuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, nr_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(nr_epochs):\n",
        "        avg_loss = 0.0\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item() / len(dataloader)\n",
        "        print(f\"Epoch {epoch} loss: {avg_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training on tokenized text\n",
        "block_size = 32\n",
        "vocab_size = max(bpe.vocab)+1\n",
        "nr_epochs = 15\n",
        "\n",
        "dataloader = DataLoader(TextDataset(enc, block_size), batch_size=32, shuffle=True, drop_last=True)\n",
        "mlp_tokenizer = MLPNextTokenPredictor(vocab_size, block_size)\n",
        "mlp_tokenizer.to(device)\n",
        "optimizer = optim.AdamW(mlp_tokenizer.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "train(mlp_tokenizer, dataloader, optimizer, nr_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate new text using tokenization based MLP\n",
        "continuation = produce_example_text(mlp_tokenizer, block_size, bpe)\n",
        "print(continuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training on original text\n",
        "block_size = 64 # let us give more context because of no tokenization\n",
        "# convert text to numbers\n",
        "char_enc = [ord(c) for c in text]\n",
        "vocab_size = max(char_enc) + 1\n",
        "nr_epochs = 15\n",
        "\n",
        "dataloader = DataLoader(TextDataset(char_enc, block_size), batch_size=32, shuffle=True, drop_last=True)\n",
        "mlp_wo_tokenizer = MLPNextTokenPredictor(vocab_size, block_size)\n",
        "mlp_wo_tokenizer.to(device)\n",
        "optimizer = optim.AdamW(mlp_wo_tokenizer.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "train(mlp_wo_tokenizer, dataloader, optimizer, nr_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate new text using non-tokenization based MLP\n",
        "class no_tok:\n",
        "    def encode(self, text):\n",
        "        return [ord(c) for c in text]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join([chr(i) for i in ids])\n",
        "\n",
        "t = produce_example_text(mlp_wo_tokenizer, block_size, no_tok())\n",
        "print(t)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
