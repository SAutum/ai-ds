{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group ID: \n",
    "## Exercise day: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This exercise sheet has the following subtasks:\n",
    "\n",
    "1. Normalization <br>\n",
    "    (a) Implement Layer Normalization (1.5 point) <br>\n",
    "    (b) Implement Group Normalization (1.5 point) <br>\n",
    "2. Imbalanced Classes <br>\n",
    "    (a) Implement Focal loss (1.5 point) <br>\n",
    "    (b) Train loop and comparison (0.5 point) <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Layer Normalization (1.5 point)\n",
    "\n",
    "Layer Normalization (LN) is a technique that normalizes the inputs across the features of a layer for each data point, rather than across the batch as in Batch Normalization. It is defined as:\n",
    "\n",
    "\n",
    "$\\text{LN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$,\n",
    "\n",
    "\n",
    "where:\n",
    "-   $x \\in \\mathbb{R}^{d_1\\times d_2 \\times \\dots \\times d_{R}}$ is a Tensor of order R\n",
    "-\t$\\mu$ is the mean of the features for the input tensor $x$ according to the last D dimensions.\n",
    "-\t$\\sigma^2$ is the variance of the features for the input tensor $x$ according to the last D dimensions.\n",
    "-\t$\\gamma$ and $\\beta$: Learnable parameters of shape $(d_{R-D+1}, d_{R-D+2}, \\dots, d_{R})$ used for scaling and shifting.\n",
    "-\t$\\epsilon$: A small constant added to the denominator for numerical stability.\n",
    "\n",
    "The following figure illustrates this for a Tensor $x$ of shape (N, C, H, W), where N is the batch size, C is the number of channels and H,W the spatial dimensions. The Layer Norm in this example is calculated over the **last three** dimension (C,H,W):\n",
    "\n",
    "![Layer Norm figure](https://pytorch.org/docs/stable/_images/layer_norm.jpg)<br>\n",
    "For more details you can refer to the paper [Layer Normalization (Ba et al.)](https://arxiv.org/abs/1607.06450)\n",
    "\n",
    "Task: Implement Layer Normalization as a `PyTorch Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormCustom(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(LayerNormCustom, self).__init__()\n",
    "        ### your code here ###\n",
    "        pass\n",
    "        ######################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### your code here ###\n",
    "        pass\n",
    "        ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test LayerNorm against the reference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test against PyTorch's implementation\n",
    "x = torch.randn(10, 20, 30)\n",
    "ln = nn.LayerNorm(x.shape[-2:])\n",
    "ln_custom = LayerNormCustom(x.shape[-2:])\n",
    "assert torch.allclose(ln(x), ln_custom(x), atol=1e-6)\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Group Normalization (1.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input Tensor:  $\\mathbf{x} \\in \\mathbb{R}^{N \\times C \\times \\dots}$ , where $N$ is the batch size, $C$ is the number of features (e.g., channels), and the remaining dimensions $(H, W, \\dots)$ are spatial or sequence dimensions.\n",
    "2.\tGroups: Divide the C channels/features into G groups (provided as an argument).\n",
    "3.\tNormalization: Normalize each group independently.\n",
    "\t-\tFor a group g, compute:\n",
    "\t-\tThe mean:  $\\mu_g$ \n",
    "\t-\tThe variance:  $\\sigma_g^2$ \n",
    "\t-\tApply normalization:\n",
    "\n",
    "$$\\hat{x}_g = \\frac{x_g - \\mu_g}{\\sqrt{\\sigma_g^2 + \\epsilon}}$$\n",
    "\n",
    "4.\tScale and Shift: Apply learnable parameters $\\gamma$ and $\\beta$:\n",
    "\n",
    "$y = \\gamma \\cdot \\hat{x} + \\beta$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are vectors of size $C$.\n",
    "\n",
    "The Figure 2. in the paper [Group Normalization](https://arxiv.org/abs/1803.08494) shows the different types of normalization techniques\n",
    "\n",
    "![Normalization](normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNormCustom(nn.Module):\n",
    "    def __init__(self, num_features, num_groups, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): Number of features (e.g., channels in an image).\n",
    "            num_groups (int): Number of groups to divide the features into.\n",
    "            eps (float): A small value to prevent division by zero.\n",
    "        \"\"\"\n",
    "        super(GroupNormCustom, self).__init__()\n",
    "        ### your code here ###\n",
    "        pass\n",
    "        ######################\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Group Normalization.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, C, ...).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor with the same shape as x.\n",
    "        \"\"\"\n",
    "        ### your code here ###\n",
    "        pass\n",
    "        ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test GroupNorm against the reference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 32\n",
    "# Random input tensor\n",
    "x = torch.randn(4, num_features, 16, 16)  # Shape (N, C, H, W)\n",
    "\n",
    "# Custom GroupNorm\n",
    "num_groups = 4 # must be a divisor of C\n",
    "custom_gn = GroupNormCustom(num_features=num_features, num_groups=num_groups)\n",
    "\n",
    "# PyTorch GroupNorm\n",
    "torch_gn = nn.GroupNorm(num_groups=num_groups, num_channels=8)\n",
    "torch_gn.weight.data = custom_gn.gamma.data.clone()  # Match gamma\n",
    "torch_gn.bias.data = custom_gn.beta.data.clone()  # Match beta\n",
    "\n",
    "# Compare outputs\n",
    "custom_out = custom_gn(x)\n",
    "torch_out = torch_gn(x)\n",
    "\n",
    "assert torch.allclose(custom_out, torch_out)\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Plain DeeperCNN with LayerNorm and GroupNorm\n",
    "The following part is just for demonstration purposes. If you implemented the Normalization correct, you should be able to run the following code without any errors.\n",
    "\n",
    "We will use LayerNorm and GroupNorm to make DeeperCNN (the one from the lecture) model work again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=10, hidden_dim_1=4, hidden_dim_2=8, nr_convs_per_block=2, activation=nn.ReLU):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "\n",
    "        self.ops = []\n",
    "        self.conv_block_1 = [\n",
    "            nn.Conv2d(in_channels=c_in, out_channels=hidden_dim_1, kernel_size=(3,3), padding=1),\n",
    "            activation(),\n",
    "        ]\n",
    "        for i in range(nr_convs_per_block-1):\n",
    "            self.conv_block_1.extend([\n",
    "            nn.Conv2d(in_channels=hidden_dim_1, out_channels=hidden_dim_1, kernel_size=(3,3), padding=1),\n",
    "            activation()\n",
    "            ])\n",
    "        self.conv_block_1.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)))\n",
    "        self.conv_block_1 = nn.Sequential(*self.conv_block_1)\n",
    "\n",
    "        self.conv_block_2 = [\n",
    "            nn.Conv2d(in_channels=hidden_dim_1, out_channels=hidden_dim_2, kernel_size=(3,3), padding=1, stride=1),\n",
    "            activation()\n",
    "        ]\n",
    "        for i in range(nr_convs_per_block-1):\n",
    "            self.conv_block_2.extend([\n",
    "            nn.Conv2d(in_channels=hidden_dim_2, out_channels=hidden_dim_2, kernel_size=(3,3), padding=1, stride=1),\n",
    "            activation()\n",
    "            ])\n",
    "        self.conv_block_2.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)))\n",
    "        self.conv_block_2 = nn.Sequential(*self.conv_block_2)\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Flatten(start_dim=-3, end_dim=-1),\n",
    "            nn.Linear(in_features=7*7*hidden_dim_2, out_features=c_out),\n",
    "            # nn.Softmax() -> no softmax, cross entropy loss in pytorch already applies softmax\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.MLP(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNNLayerNorm(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=10, hidden_dim_1=4, hidden_dim_2=8, nr_convs_per_block=2, activation=nn.ReLU):\n",
    "        super(DeeperCNNLayerNorm, self).__init__()\n",
    "\n",
    "        self.ops = []\n",
    "        self.conv_block_1 = [\n",
    "            nn.Conv2d(in_channels=c_in, out_channels=hidden_dim_1, kernel_size=(3,3), padding=1),\n",
    "            LayerNormCustom((hidden_dim_1,28,28)),\n",
    "            activation(),\n",
    "        ]\n",
    "        for i in range(nr_convs_per_block-1):\n",
    "            self.conv_block_1.extend([\n",
    "            nn.Conv2d(in_channels=hidden_dim_1, out_channels=hidden_dim_1, kernel_size=(3,3), padding=1),\n",
    "            LayerNormCustom((hidden_dim_1,28,28)),\n",
    "            activation()\n",
    "            ])\n",
    "        self.conv_block_1.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "        self.conv_block_1 = nn.Sequential(*self.conv_block_1)\n",
    "\n",
    "        self.conv_block_2 = [\n",
    "            nn.Conv2d(in_channels=hidden_dim_1, out_channels=hidden_dim_2, kernel_size=(3,3), padding=1, stride=1),\n",
    "            LayerNormCustom((hidden_dim_2,14,14)),\n",
    "            activation()\n",
    "        ]\n",
    "        for i in range(nr_convs_per_block-1):\n",
    "            self.conv_block_2.extend([\n",
    "            nn.Conv2d(in_channels=hidden_dim_2, out_channels=hidden_dim_2, kernel_size=(3,3), padding=1, stride=1),\n",
    "            LayerNormCustom((hidden_dim_2,14,14)),\n",
    "            activation()\n",
    "            ])\n",
    "        self.conv_block_2.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(*self.conv_block_2)\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Flatten(start_dim=-3, end_dim=-1),\n",
    "            nn.Linear(in_features=7*7*hidden_dim_2, out_features=c_out),\n",
    "            # nn.Softmax() -> no softmax, cross entropy loss in pytorch already applies softmax\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.MLP(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNNGroupNorm(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=10, hidden_dim_1=4, hidden_dim_2=8, nr_convs_per_block=2, activation=nn.ReLU):\n",
    "        super(DeeperCNNGroupNorm, self).__init__()\n",
    "\n",
    "        self.ops = []\n",
    "        self.conv_block_1 = [\n",
    "            nn.Conv2d(in_channels=c_in, out_channels=hidden_dim_1, kernel_size=(3,3), padding=1),\n",
    "            GroupNormCustom(hidden_dim_1, 2),\n",
    "            activation(),\n",
    "        ]\n",
    "        for i in range(nr_convs_per_block-1):\n",
    "            self.conv_block_1.extend([\n",
    "            nn.Conv2d(in_channels=hidden_dim_1, out_channels=hidden_dim_1, kernel_size=(3,3), padding=1),\n",
    "            GroupNormCustom(hidden_dim_1, 2),\n",
    "            activation()\n",
    "            ])\n",
    "        self.conv_block_1.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "        self.conv_block_1 = nn.Sequential(*self.conv_block_1)\n",
    "\n",
    "        self.conv_block_2 = [\n",
    "            nn.Conv2d(in_channels=hidden_dim_1, out_channels=hidden_dim_2, kernel_size=(3,3), padding=1, stride=1),\n",
    "            GroupNormCustom(hidden_dim_2, 2),\n",
    "            activation()\n",
    "        ]\n",
    "        for i in range(nr_convs_per_block-1):\n",
    "            self.conv_block_2.extend([\n",
    "            nn.Conv2d(in_channels=hidden_dim_2, out_channels=hidden_dim_2, kernel_size=(3,3), padding=1, stride=1),\n",
    "            GroupNormCustom(hidden_dim_2, 2),\n",
    "            activation()\n",
    "            ])\n",
    "        self.conv_block_2.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(*self.conv_block_2)\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Flatten(start_dim=-3, end_dim=-1),\n",
    "            nn.Linear(in_features=7*7*hidden_dim_2, out_features=c_out),\n",
    "            # nn.Softmax() -> no softmax, cross entropy loss in pytorch already applies softmax\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.MLP(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_train = datasets.FashionMNIST(root='data', train=True, download=True)\n",
    "fashion_mnist_test = datasets.FashionMNIST(root='data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fashion_mnist_train.data.reshape(-1, 1, 28, 28).float() / 255\n",
    "y_train = fashion_mnist_train.targets\n",
    "\n",
    "X_test = fashion_mnist_test.data.reshape(-1, 1, 28, 28).float() / 255\n",
    "y_test = fashion_mnist_test.targets\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, X_val, y_val):\n",
    "    # store model mode and set back to original mode after validation\n",
    "    model_mode = model.training\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    batch_size = 128\n",
    "    with torch.no_grad(): # do not record gradients\n",
    "        for i in range(0,X_val.shape[0],batch_size):\n",
    "            x = torch.Tensor(X_val[i:i+batch_size]).to(device)\n",
    "            x = x.reshape(-1, 1, 28, 28)\n",
    "            y_true = y_val[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # Get the predicted class (assuming y_pred is a Tensor with probabilities)\n",
    "            predicted_class = torch.argmax(y_pred.data, dim=1)\n",
    "            \n",
    "            # Check if the prediction is correct\n",
    "            correct_predictions += (predicted_class.cpu().numpy() == y_true).sum()\n",
    "            \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / X_val.shape[0]\n",
    "    model.train(model_mode)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_test, y_test, num_epochs=3, lr=0.1, loss_fn=None):\n",
    "    model.train()\n",
    "    val_acc = validate(model, X_test, y_test)\n",
    "    train_acc = validate(model, X_train, y_train)\n",
    "    loss_fn = F.cross_entropy if loss_fn is None else loss_fn\n",
    "    print(f\"Training model {model.__class__.__name__} with {loss_fn} loss function for {num_epochs} epochs\")\n",
    "    print(f\"Before training: validation accuracy: {val_acc}\")\n",
    "\n",
    "    # Parameters\n",
    "    batch_size = 32\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    val_accs = []\n",
    "    train_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        loss = 0.0\n",
    "        \n",
    "        for i in range(0, X_shuffled.shape[0], batch_size):\n",
    "            X_batch = torch.Tensor(X_shuffled[i:i + batch_size]).to(device)\n",
    "            X_batch = X_batch.reshape(-1,1,28,28)\n",
    "            y_batch = torch.Tensor(y_shuffled[i:i + batch_size]).to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            preds = model(X_batch)\n",
    "\n",
    "            mb_loss = loss_fn(preds, y_batch)\n",
    "            loss += mb_loss * X_batch.shape[0] / X_train.shape[0]\n",
    "\n",
    "            #for layer in model.modules():\n",
    "            #    layer.out.retain_grad() ### Just temproary\n",
    "\n",
    "            ### ZERO GRAD ###\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    p.grad.zero_()\n",
    "            # backward pass\n",
    "            mb_loss.backward()\n",
    "\n",
    "            # The weight update\n",
    "            for p in model.parameters():\n",
    "                p.data += -lr * p.grad\n",
    "\n",
    "        losses.append(loss)\n",
    "        val_acc = validate(model, X_test, y_test)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        train_acc = validate(model, X_train, y_train)\n",
    "        train_accs.append(train_acc)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}, validation accuracy: {val_acc}, train accuracy: {train_acc}\")\n",
    "\n",
    "    return losses, val_accs, train_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_convs_per_block = 8\n",
    "\n",
    "cnn_plain = DeeperCNN(nr_convs_per_block=nr_convs_per_block).to(device)\n",
    "cnn_layernorm = DeeperCNNLayerNorm(nr_convs_per_block=nr_convs_per_block).to(device)\n",
    "cnn_groupnorm = DeeperCNNGroupNorm(nr_convs_per_block=nr_convs_per_block).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we compare the three models, we will see that the plain CNN model will not learn anything meaningful. This is due to vanishing gradients in deeper networks. But we will also see that the LayerNorm and GroupNorm can help tackle this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "lr = 0.1\n",
    "\n",
    "train(cnn_plain, X_train, y_train, X_test, y_test, num_epochs=num_epochs, lr=lr)\n",
    "train(cnn_layernorm, X_train, y_train, X_test, y_test, num_epochs=num_epochs, lr=lr)\n",
    "train(cnn_groupnorm, X_train, y_train, X_test, y_test, num_epochs=num_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Imbalanced Classes\n",
    "The problem of imbalanced classes occurs in classification problems when one or more classes are significantly more common than others in the data. This imbalance can significantly affect the performance of machine learning models, as many algorithms tend to favour the dominant class and neglect the rarer classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will artificially construct an imbalanced dataset from the FashionMNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imbalanced_dataset(X, y, num_samples_per_class):\n",
    "    indices = []\n",
    "    for cls, num_samples in num_samples_per_class.items():\n",
    "        # Get indices of all samples for the current class\n",
    "        cls_indices = torch.where(y == cls)[0]\n",
    "        assert len(cls_indices) >= num_samples\n",
    "        indices.extend(cls_indices[:num_samples])\n",
    "    \n",
    "    # Create the new dataset\n",
    "    X_imbalanced = X[indices]\n",
    "    y_imbalanced = y[indices]\n",
    "    \n",
    "    return X_imbalanced, y_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = {k:(k+1)*600 for k in range(10)}\n",
    "samples_per_class_test = {k:(k+1)*100 for k in range(10)}\n",
    "\n",
    "X_train_imbalanced, y_train_imbalanced = create_imbalanced_dataset(X_train, y_train, samples_per_class)\n",
    "\n",
    "X_test_imbalanced, y_test_imbalanced = create_imbalanced_dataset(X_test, y_test, samples_per_class_test)\n",
    "\n",
    "imbalanced_labels = [target.item() for _,target in zip(X_train_imbalanced, y_train_imbalanced)]\n",
    "for k,v in Counter(imbalanced_labels).items():\n",
    "    print(f\"Label {k}: {v} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Implement Focal loss (1.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\alpha$-balanced focal loss is defined as: \n",
    "$$\n",
    "\\mathcal{L}_{\\text{focal}} = - \\alpha_{t} (1 - p_t)^\\gamma \\log(p_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the $\\alpha$-balanced focal loss with the following functionalities:\n",
    "- Support multi-class classification.\n",
    "- Accept parameters for $\\alpha$ (class balancing) and $\\gamma$ (focusing parameter).\n",
    "- Return the loss value for a batch of inputs and targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does focal loss help?\n",
    "- The $\\gamma$ parameter down-weights the loss assigned to well-classified examples. This is useful when the dataset is imbalanced, as it helps the model to focus on the hard examples.\n",
    "- The $\\alpha$ parameter is used to balance the importance of the classes. This is useful when the dataset is imbalanced, as it helps the model to focus on the rare classes.\n",
    "\n",
    "For more details, you can refer to the paper [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        ### your code here ###\n",
    "        pass\n",
    "        ######################\n",
    "    \n",
    "    def forward(self, scores, target):\n",
    "        ### your code here ###\n",
    "        pass\n",
    "        ######################       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Training loop and comparison (0.5 point)\n",
    "Train the model with the focal loss and compare the results with the model trained with the cross-entropy loss. You can use the provided code to train the model.\n",
    "After training, plot a confusion matrix for both models and compare the results in a few sentences.\n",
    "\n",
    "Subtasks:\n",
    "1. Train the model with both cross-entropy and focal loss (code is already given)\n",
    "2. Implement confusion matrix code\n",
    "3. Compare the results in a few sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with Focal Loss\n",
    "num_epochs = 10\n",
    "nr_convs_per_block = 2\n",
    "cnn_focal = DeeperCNN(nr_convs_per_block=nr_convs_per_block).to(device)\n",
    "cnn_ce = DeeperCNN(nr_convs_per_block=nr_convs_per_block).to(device)\n",
    "alpha = torch.Tensor([sum(samples_per_class.values()) / samples_per_class[k] for k in range(10)])\n",
    "alpha = alpha / alpha.sum()\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cnn_focal, X_train=X_train_imbalanced, y_train=y_train_imbalanced, X_test=X_test_imbalanced, y_test=y_test_imbalanced, num_epochs=num_epochs, lr=0.1, loss_fn=FocalLoss(alpha=alpha, gamma=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cnn_ce, X_train=X_train_imbalanced, y_train=y_train_imbalanced, X_test=X_test_imbalanced, y_test=y_test_imbalanced, num_epochs=num_epochs, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the focal loss with the plain cross entropy loss on the imbalanced dataset\n",
    "\n",
    "Here you have to implement the plot_confusion_matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare both models with confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    ### your code here ###\n",
    "    pass\n",
    "    ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for focal loss\n",
    "cnn_focal.eval()\n",
    "y_pred = []\n",
    "for i in range(0, X_test_imbalanced.shape[0], 32):\n",
    "    x = X_test_imbalanced[i:i+32].to(device)\n",
    "    y = y_test_imbalanced[i:i+32]\n",
    "    y_pred.extend(torch.argmax(cnn_focal(x), dim=1).cpu().numpy())\n",
    "plot_confusion_matrix(y_test_imbalanced, y_pred, classes=fashion_mnist_train.classes, title=\"Focal Loss\")\n",
    "\n",
    "# confusion matrix for cross entropy loss\n",
    "cnn_ce.eval()\n",
    "y_pred = []\n",
    "for i in range(0, X_test_imbalanced.shape[0], 32):\n",
    "    x = X_test_imbalanced[i:i+32].to(device)\n",
    "    y = y_test_imbalanced[i:i+32]\n",
    "    y_pred.extend(torch.argmax(cnn_ce(x), dim=1).cpu().numpy())\n",
    "plot_confusion_matrix(y_test_imbalanced, y_pred, classes=fashion_mnist_train.classes, title=\"Cross Entropy Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your interpretation of the results:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
