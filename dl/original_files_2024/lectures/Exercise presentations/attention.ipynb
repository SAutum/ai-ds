{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanism Example\n",
    "Using the sentence \"The cat sat on a mat\" and focusing on the word \"cat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create three vectors from each token:\n",
    "- Query: They can be thought of as the \"questions\" we ask to the keys. We use queries to search for specific information in our input data.\n",
    "- Key: They can be thought of as the \"labels\" for the input data in the context of attention. When we calculate attention scores, we compare the queries to these keys.\n",
    "- Value: Once we have our attention scores, we use them to create a weighted combination of the values. Think of values as the actual content associated with each key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create embeddings (for simplicity we create the embeddings here manually and make them very low-dimensional). \n",
    "\n",
    "Usually the embeddings are learnable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = {\n",
    "    \"The\": np.array([1, 0, 1]),\n",
    "    \"cat\": np.array([0, 1, 0]),\n",
    "    \"sat\": np.array([1, 1, 0]),\n",
    "    \"on\": np.array([0, 0, 1]),\n",
    "    \"a\": np.array([1, 0, 0]),\n",
    "    \"mat\": np.array([0, 1, 1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create $W_Q$, $W_K$, $W_V$ matrices (those matrices are learnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0]\n",
    "])\n",
    "\n",
    "W_K = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "\n",
    "W_V = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Calculate Query, Value and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: {'The': array([2, 1, 1]), 'cat': array([0, 1, 0]), 'sat': array([1, 1, 1]), 'on': array([1, 1, 0]), 'a': array([1, 0, 1]), 'mat': array([1, 2, 0])}\n",
      "K: {'The': array([0, 2, 1]), 'cat': array([1, 0, 1]), 'sat': array([1, 1, 1]), 'on': array([0, 1, 1]), 'a': array([0, 1, 0]), 'mat': array([1, 1, 2])}\n",
      "V: {'The': array([1, 0, 1]), 'cat': array([0, 1, 0]), 'sat': array([1, 1, 0]), 'on': array([0, 0, 1]), 'a': array([1, 0, 0]), 'mat': array([0, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "Q = {word: np.dot(embedding, W_Q) for word, embedding in embeddings.items()}\n",
    "K = {word: np.dot(embedding, W_K) for word, embedding in embeddings.items()}\n",
    "V = {word: np.dot(embedding, W_V) for word, embedding in embeddings.items()}\n",
    "\n",
    "print(\"Q:\", Q)\n",
    "print(\"K:\", K)\n",
    "print(\"V:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Calculate the attention scores and scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1.155, 'cat': 0.0, 'sat': 0.577, 'on': 0.577, 'a': 0.577, 'mat': 0.577}\n"
     ]
    }
   ],
   "source": [
    "def attention_scores(Q_word, K):\n",
    "    scores = {word: np.dot(Q_word, K_word) for word, K_word in K.items()}\n",
    "    return scores\n",
    "\n",
    "def scale_scores(scores, d_k):\n",
    "    return {word: round(score / np.sqrt(d_k), 3) for word, score in scores.items()}\n",
    "\n",
    "Q_cat = Q[\"cat\"]\n",
    "scores = attention_scores(Q_cat, K)\n",
    "scaled_scores = scale_scores(scores, d_k=3)\n",
    "\n",
    "print(scaled_scores)\n",
    "\n",
    "# We will see how does it work when we don't scale the scores\n",
    "unscaled_scores = scale_scores(scores, d_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Calculate the Softmax of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0.281, 'cat': 0.089, 'sat': 0.158, 'on': 0.158, 'a': 0.158, 'mat': 0.158}\n"
     ]
    }
   ],
   "source": [
    "def softmax(scores):\n",
    "    exp_scores = np.exp(list(scores.values()))\n",
    "    sum_exp_scores = np.sum(exp_scores)\n",
    "    return {word: round(exp_score / sum_exp_scores, 3) for word, exp_score in zip(scores.keys(), exp_scores)}\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "print(attention_weights)\n",
    "# For unscaled\n",
    "attention_weights_unscaled = softmax(unscaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Calculate the weighted average of attention weights weighted with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Scaled: {'The': 0.281, 'cat': 0.089, 'sat': 0.158, 'on': 0.158, 'a': 0.158, 'mat': 0.158}\n",
      "Attention Weights Unscaled: {'The': 0.384, 'cat': 0.052, 'sat': 0.141, 'on': 0.141, 'a': 0.141, 'mat': 0.141}\n",
      "Output for 'cat': [0.597 0.405 0.597]\n"
     ]
    }
   ],
   "source": [
    "output_cat = sum(attention_weights[word] * V[word] for word in V)\n",
    "\n",
    "print(\"Attention Weights Scaled:\", attention_weights)\n",
    "print(\"Attention Weights Unscaled:\", attention_weights_unscaled)\n",
    "print(\"Output for 'cat':\", output_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention weights indicate how much focus the word \"cat\" gives to each word in the sentence, including itself. The weights are normalized probabilities that sum up to 1.\n",
    "\n",
    "The output vector for \"cat\" is a weighted sum of the value vectors of all words, weighted by the attention weights.\n",
    "\n",
    "This output vector can be interpreted as the new representation of the word \"cat\" after considering the context provided by the entire sentence. It combines information from all the words, with more weight given to the words that \"cat\" pays more attention to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Example\n",
    "\n",
    "The Multi-Head Attention works anologusly to Self-Attention but instead we define matrices $W_Q$, $W_K$ and $W_V$ for each head.\n",
    "\n",
    "Moreover we have one matrix $W_O$ to combine them at the end.\n",
    "\n",
    "Using the sentence \"The cat sat on a mat\" and focusing on the word \"cat\" with two attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights Head 1: {'The': 0.281, 'cat': 0.089, 'sat': 0.158, 'on': 0.158, 'a': 0.158, 'mat': 0.158}\n",
      "Attention Weights Head 2: {'The': 0.424, 'cat': 0.042, 'sat': 0.134, 'on': 0.134, 'a': 0.134, 'mat': 0.134}\n",
      "Output for 'cat' Head 1: [0.597 0.405 0.597]\n",
      "Output for 'cat' Head 2: [0.31  0.692 1.002]\n",
      "Final Output for 'cat': [1.08429367 1.54793053 2.56099683]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = {\n",
    "    \"The\": np.array([1, 0, 1]),\n",
    "    \"cat\": np.array([0, 1, 0]),\n",
    "    \"sat\": np.array([1, 1, 0]),\n",
    "    \"on\": np.array([0, 0, 1]),\n",
    "    \"a\": np.array([1, 0, 0]),\n",
    "    \"mat\": np.array([0, 1, 1])\n",
    "}\n",
    "\n",
    "\n",
    "W_Q1 = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0]\n",
    "])\n",
    "\n",
    "W_K1 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 1]\n",
    "])\n",
    "\n",
    "W_V1 = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "W_Q2 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0]\n",
    "])\n",
    "\n",
    "W_K2 = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "W_V2 = np.array([\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "Q1 = {word: np.dot(embedding, W_Q1) for word, embedding in embeddings.items()}\n",
    "K1 = {word: np.dot(embedding, W_K1) for word, embedding in embeddings.items()}\n",
    "V1 = {word: np.dot(embedding, W_V1) for word, embedding in embeddings.items()}\n",
    "\n",
    "Q2 = {word: np.dot(embedding, W_Q2) for word, embedding in embeddings.items()}\n",
    "K2 = {word: np.dot(embedding, W_K2) for word, embedding in embeddings.items()}\n",
    "V2 = {word: np.dot(embedding, W_V2) for word, embedding in embeddings.items()}\n",
    "\n",
    "Q_cat1 = Q1[\"cat\"]\n",
    "scores1 = attention_scores(Q_cat1, K1)\n",
    "scaled_scores1 = scale_scores(scores1, d_k=3)\n",
    "attention_weights1 = softmax(scaled_scores1)\n",
    "\n",
    "Q_cat2 = Q2[\"cat\"]\n",
    "scores2 = attention_scores(Q_cat2, K2)\n",
    "scaled_scores2 = scale_scores(scores2, d_k=3)\n",
    "attention_weights2 = softmax(scaled_scores2)\n",
    "\n",
    "output_cat1 = sum(attention_weights1[word] * V1[word] for word in V1)\n",
    "output_cat2 = sum(attention_weights2[word] * V2[word] for word in V2)\n",
    "\n",
    "output_cat = np.concatenate([output_cat1, output_cat2])\n",
    "\n",
    "W_O = np.random.rand(output_cat.shape[0], 3) \n",
    "final_output_cat = np.dot(output_cat, W_O)\n",
    "\n",
    "print(\"Attention Weights Head 1:\", attention_weights1)\n",
    "print(\"Attention Weights Head 2:\", attention_weights2)\n",
    "print(\"Output for 'cat' Head 1:\", output_cat1)\n",
    "print(\"Output for 'cat' Head 2:\", output_cat2)\n",
    "print(\"Final Output for 'cat':\", final_output_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
