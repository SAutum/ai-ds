{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Idea: At training time,\n",
    "- Compute for every layer a mask over all neurons such that with probability $p$ an entry is taken.\n",
    "- Set activations of all entries in the mask to zero.\n",
    "\n",
    "![Dropout training time](Dropout.png)\n",
    "\n",
    "Implementation:\n",
    "\n",
    "- For every neuron add independent Bernoulli random variable $r$ with parameter $p$.\n",
    "- For every neuron activation $y$ compute $\\tilde{y} = (1-r) \\cdot y$ to be the activation used subsequently.\n",
    "\n",
    "![Dropout training time detail](Dropout-detail.png)\n",
    "\n",
    "Interpretation:\n",
    "- Instead of training one full network, we train an exponential number of thinned networks all at once.\n",
    "- Each thinned network must work well, hence reducing dependency on any single latent representation.\n",
    "    - Co-adaptation is avoided, independent features are encouraged.\n",
    "    - Overfitting is reduced, we learn an exponential number of ensembles.\n",
    "    - Redundant representations are encouraged, potentially improving generalization.\n",
    "- During test time we take the full network, which is an average over all thinned ones.\n",
    "\n",
    "At test time\n",
    "- Use the original unthinned network.\n",
    "- To account for the difference between thinned and fill network, multiply the weights of each layer by $1-p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pswoboda/miniconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 32, 32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
    "          \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# Example usage\n",
    "for images, labels in trainloader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def get_mask(self, x):\n",
    "        mask = torch.rand(*x.shape)<=self.p\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = self.get_mask(x)\n",
    "            x = x * mask\n",
    "        else:\n",
    "            x = x * (1-self.p)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def get_mask(self, x):\n",
    "        mask = torch.rand(*x.shape, device=device) <= (1-self.p)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = self.get_mask(x)\n",
    "            x = 1/(1-self.p) * x * mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check dropout\n",
    "\n",
    "d = Dropout(0.25)\n",
    "x = torch.rand(10, 10).to(device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, p=0.25):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        #Init_channels, channels, kernel_size, padding) \n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # FC layers\n",
    "        # Linear layer (64x4x4 -> 500)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "        \n",
    "        # Linear Layer (500 -> 10)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.elu(self.dropout(self.conv1(x))))\n",
    "        x = self.pool(F.elu(self.dropout(self.conv2(x))))\n",
    "        x = self.pool(F.elu(self.dropout(self.conv3(x))))\n",
    "        \n",
    "        # Flatten the image\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        #x = self.dropout(x)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #3*32*32 -> 32*32*32\n",
    "        self.dropout1 = nn.Dropout(p=dropout)        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        #32*32*32 -> 16*16*32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #16*16*32 -> 16*16*64\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        #16*16*64 -> 8*8*64\n",
    "        self.fc1 = nn.Linear(8*8*64, 1024)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout4 = nn.Dropout(p=dropout)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.conv1(x))\n",
    "        x = self.pool1(F.relu(x))\n",
    "        x = self.dropout2(self.conv2(x))\n",
    "        x = self.pool2(F.relu(x))\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        #self.num_flat_features(x) = 8*8*64 here.\n",
    "        #-1 means: get the rest a row (in this case is 16 mini-batches)\n",
    "        #pytorch nn only takes mini-batch as the input\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs=25, weight_decay=0.0000, model_checkpoint='model_cifar.pt'):\n",
    "    # Specify the Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Specify the optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "    \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data, target in trainloader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Manually add weight decay\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    if param.requires_grad:\n",
    "                        param.grad += weight_decay * param\n",
    "            \n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for data, target in testloader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(trainloader.dataset)\n",
    "        valid_loss = valid_loss/len(testloader.dataset)\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "    \n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), model_checkpoint)\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.290635 \tValidation Loss: 2.279601\n",
      "Validation loss decreased (inf --> 2.279601).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.221849 \tValidation Loss: 2.180125\n",
      "Validation loss decreased (2.279601 --> 2.180125).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.085038 \tValidation Loss: 2.061150\n",
      "Validation loss decreased (2.180125 --> 2.061150).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.968961 \tValidation Loss: 1.952566\n",
      "Validation loss decreased (2.061150 --> 1.952566).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.864512 \tValidation Loss: 1.859413\n",
      "Validation loss decreased (1.952566 --> 1.859413).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.778727 \tValidation Loss: 1.780695\n",
      "Validation loss decreased (1.859413 --> 1.780695).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.695100 \tValidation Loss: 1.700058\n",
      "Validation loss decreased (1.780695 --> 1.700058).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.628724 \tValidation Loss: 1.641476\n",
      "Validation loss decreased (1.700058 --> 1.641476).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.576710 \tValidation Loss: 1.604172\n",
      "Validation loss decreased (1.641476 --> 1.604172).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.534283 \tValidation Loss: 1.557818\n",
      "Validation loss decreased (1.604172 --> 1.557818).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.499954 \tValidation Loss: 1.529155\n",
      "Validation loss decreased (1.557818 --> 1.529155).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.468527 \tValidation Loss: 1.498844\n",
      "Validation loss decreased (1.529155 --> 1.498844).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.441403 \tValidation Loss: 1.469707\n",
      "Validation loss decreased (1.498844 --> 1.469707).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.411474 \tValidation Loss: 1.444428\n",
      "Validation loss decreased (1.469707 --> 1.444428).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.388435 \tValidation Loss: 1.420663\n",
      "Validation loss decreased (1.444428 --> 1.420663).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.363668 \tValidation Loss: 1.418256\n",
      "Validation loss decreased (1.420663 --> 1.418256).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.339941 \tValidation Loss: 1.381953\n",
      "Validation loss decreased (1.418256 --> 1.381953).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.319710 \tValidation Loss: 1.373400\n",
      "Validation loss decreased (1.381953 --> 1.373400).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.301031 \tValidation Loss: 1.344557\n",
      "Validation loss decreased (1.373400 --> 1.344557).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.282949 \tValidation Loss: 1.335373\n",
      "Validation loss decreased (1.344557 --> 1.335373).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model_dropout = CNN(dropout=0.2).to(device)\n",
    "train(model_dropout, n_epochs=20, model_checkpoint='model_cifar_dropout.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.298147 \tValidation Loss: 2.292824\n",
      "Validation loss decreased (inf --> 2.292824).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.284789 \tValidation Loss: 2.272804\n",
      "Validation loss decreased (2.292824 --> 2.272804).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.245980 \tValidation Loss: 2.201385\n",
      "Validation loss decreased (2.272804 --> 2.201385).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 2.137732 \tValidation Loss: 2.066096\n",
      "Validation loss decreased (2.201385 --> 2.066096).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 2.012112 \tValidation Loss: 1.948091\n",
      "Validation loss decreased (2.066096 --> 1.948091).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.916246 \tValidation Loss: 1.867050\n",
      "Validation loss decreased (1.948091 --> 1.867050).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.839795 \tValidation Loss: 1.791742\n",
      "Validation loss decreased (1.867050 --> 1.791742).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.761482 \tValidation Loss: 1.712122\n",
      "Validation loss decreased (1.791742 --> 1.712122).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.688459 \tValidation Loss: 1.645251\n",
      "Validation loss decreased (1.712122 --> 1.645251).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.621794 \tValidation Loss: 1.579941\n",
      "Validation loss decreased (1.645251 --> 1.579941).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.564659 \tValidation Loss: 1.535642\n",
      "Validation loss decreased (1.579941 --> 1.535642).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.516770 \tValidation Loss: 1.491328\n",
      "Validation loss decreased (1.535642 --> 1.491328).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.480199 \tValidation Loss: 1.453677\n",
      "Validation loss decreased (1.491328 --> 1.453677).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.447725 \tValidation Loss: 1.427293\n",
      "Validation loss decreased (1.453677 --> 1.427293).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.418397 \tValidation Loss: 1.408063\n",
      "Validation loss decreased (1.427293 --> 1.408063).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.392753 \tValidation Loss: 1.381810\n",
      "Validation loss decreased (1.408063 --> 1.381810).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.369699 \tValidation Loss: 1.366626\n",
      "Validation loss decreased (1.381810 --> 1.366626).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.347062 \tValidation Loss: 1.338129\n",
      "Validation loss decreased (1.366626 --> 1.338129).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.327007 \tValidation Loss: 1.321942\n",
      "Validation loss decreased (1.338129 --> 1.321942).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.306598 \tValidation Loss: 1.321031\n",
      "Validation loss decreased (1.321942 --> 1.321031).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model_wo_dropout = CNN(dropout=0.0).to(device)\n",
    "train(model_wo_dropout, n_epochs=20, model_checkpoint='model_cifar_wo_dropout.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.297493 \tValidation Loss: 2.290910\n",
      "Validation loss decreased (inf --> 2.290910).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.279932 \tValidation Loss: 2.262664\n",
      "Validation loss decreased (2.290910 --> 2.262664).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.223989 \tValidation Loss: 2.164115\n",
      "Validation loss decreased (2.262664 --> 2.164115).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 2.098789 \tValidation Loss: 2.023497\n",
      "Validation loss decreased (2.164115 --> 2.023497).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.971953 \tValidation Loss: 1.912690\n",
      "Validation loss decreased (2.023497 --> 1.912690).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.882899 \tValidation Loss: 1.833533\n",
      "Validation loss decreased (1.912690 --> 1.833533).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.802457 \tValidation Loss: 1.752810\n",
      "Validation loss decreased (1.833533 --> 1.752810).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.726368 \tValidation Loss: 1.680781\n",
      "Validation loss decreased (1.752810 --> 1.680781).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.657761 \tValidation Loss: 1.615260\n",
      "Validation loss decreased (1.680781 --> 1.615260).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.597371 \tValidation Loss: 1.557588\n",
      "Validation loss decreased (1.615260 --> 1.557588).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.548364 \tValidation Loss: 1.522009\n",
      "Validation loss decreased (1.557588 --> 1.522009).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.510409 \tValidation Loss: 1.480800\n",
      "Validation loss decreased (1.522009 --> 1.480800).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.475333 \tValidation Loss: 1.448063\n",
      "Validation loss decreased (1.480800 --> 1.448063).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.444191 \tValidation Loss: 1.428291\n",
      "Validation loss decreased (1.448063 --> 1.428291).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.413859 \tValidation Loss: 1.411749\n",
      "Validation loss decreased (1.428291 --> 1.411749).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.387429 \tValidation Loss: 1.374055\n",
      "Validation loss decreased (1.411749 --> 1.374055).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.363228 \tValidation Loss: 1.352394\n",
      "Validation loss decreased (1.374055 --> 1.352394).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.341139 \tValidation Loss: 1.332474\n",
      "Validation loss decreased (1.352394 --> 1.332474).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.320891 \tValidation Loss: 1.311005\n",
      "Validation loss decreased (1.332474 --> 1.311005).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.300268 \tValidation Loss: 1.298788\n",
      "Validation loss decreased (1.311005 --> 1.298788).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model_dropout_wd = CNN(dropout=0.0).to(device)\n",
    "train(model_dropout_wd, n_epochs=20, weight_decay=0.001, model_checkpoint='model_cifar_dropout_wd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_test_loss(model):\n",
    "    # track test loss\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    # iterate over test data\n",
    "    for data, target in testloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)    \n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(len(target.data)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(testloader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.335373\n",
      "\n",
      "Test Accuracy of airplane: 67% (678/1000)\n",
      "Test Accuracy of automobile: 58% (587/1000)\n",
      "Test Accuracy of  bird: 39% (398/1000)\n",
      "Test Accuracy of   cat: 33% (335/1000)\n",
      "Test Accuracy of  deer: 47% (470/1000)\n",
      "Test Accuracy of   dog: 47% (471/1000)\n",
      "Test Accuracy of  frog: 61% (619/1000)\n",
      "Test Accuracy of horse: 62% (623/1000)\n",
      "Test Accuracy of  ship: 65% (650/1000)\n",
      "Test Accuracy of truck: 61% (618/1000)\n",
      "\n",
      "Test Accuracy (Overall): 54% (5449/10000)\n"
     ]
    }
   ],
   "source": [
    "track_test_loss(model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.321031\n",
      "\n",
      "Test Accuracy of airplane: 65% (651/1000)\n",
      "Test Accuracy of automobile: 64% (646/1000)\n",
      "Test Accuracy of  bird: 29% (291/1000)\n",
      "Test Accuracy of   cat: 26% (264/1000)\n",
      "Test Accuracy of  deer: 42% (424/1000)\n",
      "Test Accuracy of   dog: 42% (426/1000)\n",
      "Test Accuracy of  frog: 62% (623/1000)\n",
      "Test Accuracy of horse: 71% (715/1000)\n",
      "Test Accuracy of  ship: 59% (590/1000)\n",
      "Test Accuracy of truck: 63% (638/1000)\n",
      "\n",
      "Test Accuracy (Overall): 52% (5268/10000)\n"
     ]
    }
   ],
   "source": [
    "track_test_loss(model_wo_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.298788\n",
      "\n",
      "Test Accuracy of airplane: 53% (533/1000)\n",
      "Test Accuracy of automobile: 70% (709/1000)\n",
      "Test Accuracy of  bird: 40% (409/1000)\n",
      "Test Accuracy of   cat: 21% (218/1000)\n",
      "Test Accuracy of  deer: 43% (433/1000)\n",
      "Test Accuracy of   dog: 49% (491/1000)\n",
      "Test Accuracy of  frog: 64% (640/1000)\n",
      "Test Accuracy of horse: 64% (648/1000)\n",
      "Test Accuracy of  ship: 69% (694/1000)\n",
      "Test Accuracy of truck: 57% (577/1000)\n",
      "\n",
      "Test Accuracy (Overall): 53% (5352/10000)\n"
     ]
    }
   ],
   "source": [
    "track_test_loss(model_dropout_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Suitable for small datasets that are easy to overfit.\n",
    "- Less used for big datasets where overfitting is no concern.\n",
    "- Was used extensively in earlier days (AlexNet, GoogLeNet, ...), not used much for current neural networks (Transformers, ConvNext, ...)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
