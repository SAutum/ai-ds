{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3UX-37U_MBB"
   },
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr5Zvjec_MBE"
   },
   "source": [
    "## Group ID: 20\n",
    "*  Andrea Tufo\n",
    "*  Hongli Lin\n",
    "*  Michele Paterson\n",
    "\n",
    "## Exercise day: Tuesday\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "In this exercise you will implement a Tokenizer class that will be used to tokenize a given text.\n",
    "A token will be a short sequence of bytes and tokenization will be about splitting up the text into those short substrings and then representing it in terms of token ids.\n",
    "This allows language models to work on a better text representation than just using a character level model as in the last exercise.\n",
    "We will use the so called \"Byte Pair Encoding\" (BPE) algorithm to tokenize the text. The BPE algorithm is a simple algorithm that iteratively merges the most frequent pair of bytes/ids in the text. This notebook will guide you through the implementation of the Tokenizer class. To make the representation of the text easier, we will use integer ids to represent the bytes of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The tokenizer is not part of the model, it is a preprocessing step that is used to tokenize the text before feeding it to the model. Therefore, it can be trained on a different dataset than the model.\n",
    "Once trained, the tokenizer is capable of encoding and decoding any given text to/from a list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Implement the `get_stats` method that will return a dictionary containing the frequency of each pair of characters in the text. (0.5 points)\n",
    "1. Implement the `merge` method that will merge the most frequent pair of characters in the text. (0.5 points)\n",
    "1. Implement the `fit` method that will train the tokenizer on the given text. (1 point)\n",
    "1. Implement the `encode` method that will encode the given text to a list of tokens. (1 point)\n",
    "1. Implement the `decode` method that will decode the given list of tokens to a text. (1 point)\n",
    "1. Implement the `BPE` class. This class should contain the previously implemented methods. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\" Returns a dictionary with the number of times each pair of ids appears in the input list \"\"\"\n",
    "    dict = {}\n",
    "    for i in range(len(ids) - 1):\n",
    "        # create a tuple with the pair of ids\n",
    "        pair = (ids[i], ids[(i+1)])\n",
    "        # if the pair is already in the dictionary, increment the count\n",
    "        if pair in dict:\n",
    "            dict[pair] += 1\n",
    "        # if the pair is not in the dictionary, add it\n",
    "        else:\n",
    "            dict[pair] = 1\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 3, (2, 3): 3, (3, 1): 2}\n"
     ]
    }
   ],
   "source": [
    "ids = [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
    "expected = {(1, 2): 3, (2, 3): 3, (3, 1): 2}\n",
    "assert get_stats(ids) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, new_id):\n",
    "    \"\"\"Merge every pair of elements in ids that are equal to pair into a single element new_id\"\"\"\n",
    "    _ids = ids.copy()\n",
    "    # the length of the list will change as we merge elements, so we need to check the length of the list at each iteration\n",
    "    i = 0\n",
    "    while i < len(_ids) - 1:\n",
    "        if _ids[i] == pair[0] and _ids[i+1] == pair[1]:\n",
    "            _ids[i] = new_id\n",
    "            _ids.pop(i+1)\n",
    "        i += 1\n",
    "\n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "pair = (0,1)\n",
    "new_id = 4\n",
    "new_ids = merge(ids, pair, new_id)\n",
    "expected = [4,2,3,4,2,3]\n",
    "assert new_ids == expected, f\"Merge failed new_ids actual: {new_ids} expected: {expected}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(ids, max_iter=1000):\n",
    "    \"\"\"Fit the model to the data by merging recursively the most common pairs of ids max_iter times or until no pair appears more than once. If two pairs have the same frequency, the one that appears first is chosen.\n",
    "    Returns the fitted model and a dictionary with the merges containing the new_id for each pair.\n",
    "    To ensure each new_id is unique (and our results are comparable), it is set to the maximum id in the list plus one.\n",
    "    \"\"\"\n",
    "    merges = {}\n",
    "    _ids = ids.copy()\n",
    "    for i in range(max_iter):\n",
    "        stats = get_stats(_ids)\n",
    "        # if there are no pairs, we are done\n",
    "        if not stats:\n",
    "            break\n",
    "\n",
    "        # get the most common pair\n",
    "        pair = max(stats, key=stats.get)\n",
    "\n",
    "        # assign a new id to the pair\n",
    "        new_id = max(_ids) + 1\n",
    "\n",
    "        # merge the pair\n",
    "        _ids = merge(_ids, pair, new_id)\n",
    "\n",
    "        # store the pair and its id\n",
    "        merges[pair] = new_id\n",
    "\n",
    "    return _ids, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "num_iterations = 1\n",
    "new_ids, merges = fit(ids,num_iterations)\n",
    "assert new_ids == [4,2,3,4,2,3], f\"Wrong ids after fit {new_ids}\"\n",
    "assert merges == {(0,1):4}, f\"Wrong merges after fit {merges}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "num_iterations = 2\n",
    "new_ids, merges = fit(ids,num_iterations)\n",
    "assert new_ids == [5,3,5,3], f\"Wrong ids after fit {new_ids}\"\n",
    "assert merges == {(0,1):4, (4,2):5}, f\"Wrong merges after fit {merges}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ids, merges):\n",
    "    \"\"\"Encode the input list of ids using the merges dictionary\"\"\"\n",
    "    _merges = merges.copy()\n",
    "    _ids = ids.copy()\n",
    "    for i in range(len(merges)):\n",
    "        pair = min(_merges, key=_merges.get)\n",
    "        pair_id = _merges.pop(pair)\n",
    "        j = 0\n",
    "        while j < len(_ids) - 1:\n",
    "            if _ids[j] == pair[0] and _ids[j+1] == pair[1]:\n",
    "                _ids[j] = pair_id\n",
    "                _ids.pop(j+1)\n",
    "            j += 1\n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "merges = {(0,1):4}\n",
    "encoded_ids = encode(ids, merges)\n",
    "expected = [4,2,3,4,2,3]\n",
    "assert encoded_ids == expected, f\"Wrong encoded ids {encoded_ids} expected: {expected}\"\n",
    "assert ids == [0,1,2,3,0,1,2,3], f\"Input ids should not be modified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, merges):\n",
    "    \"\"\"Decode the input list of ids using the merges dictionary\"\"\"\n",
    "    _merges = merges.copy()\n",
    "    _ids = ids.copy()\n",
    "    for i in range(len(merges)):\n",
    "        pair = max(_merges, key=_merges.get)\n",
    "        pair_id = _merges.pop(pair)\n",
    "        j = 0\n",
    "        while j < len(_ids):\n",
    "            if _ids[j] == pair_id:\n",
    "                _ids[j] = pair[0]\n",
    "                _ids.insert(j+1, pair[1])\n",
    "            j += 1\n",
    "\n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [6,6]\n",
    "merges = {(0,1):4, (4,2):5, (5,3):6}\n",
    "decoded_ids = decode(ids, merges)\n",
    "expected = [0,1,2,3,0,1,2,3]\n",
    "assert decoded_ids == expected, f\"Wrong decoded ids {decoded_ids} expected: {expected}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self, max_iter:int=1000):\n",
    "        self.max_iter = max_iter\n",
    "        self.vocab = {idx: bytes([idx]).decode(\"utf-8\", errors=\"replace\") for idx in range(256)}\n",
    "        self.merges = None\n",
    "\n",
    "    def fit(self, text):\n",
    "        # convert the input text to a list of ids using utf-8 encoding\n",
    "        # rewrite the code to use the utf-8 encoding (256 predefined encodings)\n",
    "        max_iter = self.max_iter\n",
    "\n",
    "        merges = {}\n",
    "        ids = list(map(int, text.encode(\"utf-8\")))\n",
    "        _ids = ids.copy()\n",
    "        for i in range(max_iter):\n",
    "            stats = get_stats(_ids)\n",
    "            if not stats:\n",
    "                break\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # assign a new id to the pair, ensuring it is unique\n",
    "            new_id = max(_ids + [256]) + 1\n",
    "            # merge the pair\n",
    "            _ids = merge(_ids, pair, new_id)\n",
    "            # store the pair and its id\n",
    "            merges[pair] = new_id\n",
    "\n",
    "        self.merges = merges\n",
    "        self.vocab.update({v: k for k, v in merges.items()})\n",
    "        return _ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.merges is None:\n",
    "            raise ValueError(\"Model has not been fitted\")\n",
    "        ids = list(map(int, text.encode(\"utf-8\")))\n",
    "        return encode(ids, self.merges)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if self.merges is None:\n",
    "            raise ValueError(\"Model has not been fitted\")\n",
    "        ids = decode(ids, self.merges)\n",
    "        # convert the list of ids back to a string\n",
    "        text = bytes(ids).decode(\"utf-8\", errors=\"replace\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 97, 100, 259, 32, 260, 97, 100, 259]\n",
      "{(97, 98): 257, (257, 114): 258, (258, 97): 259, (259, 99): 260}\n"
     ]
    }
   ],
   "source": [
    "# testing the BPE class\n",
    "text = \"abracadabra abracadabra\"\n",
    "bpe = BPE(max_iter=4)\n",
    "bpe.fit(text)\n",
    "encoded_text = bpe.encode(text)\n",
    "print(encoded_text)\n",
    "print(bpe.merges)\n",
    "decoded_text = bpe.decode(encoded_text)\n",
    "assert decoded_text == text, f\"Decoded text {decoded_text} does not match original text {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process. The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\" # cleaned version of the first paragraph of the Wikipedia page on LLMs\n",
    "ids = list(map(int, text.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE(max_iter=1000)\n",
    "bpe.fit(text)\n",
    "encoded_text = bpe.encode(text)\n",
    "decoded_text = bpe.decode(encoded_text)\n",
    "assert text == decoded_text, \"Decoding failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: \n",
    "\n",
    "Let us check what a tokenizer will give us for training a simple language model.\n",
    "\n",
    "We will use the tiny Shakespeare dataset and train with and without tokenization and compare generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-08 23:32:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  5.05MB/s    in 0.2s    \n",
      "\n",
      "2025-01-08 23:32:10 (5.05 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tokenizer\n",
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "# add 1000 vocab elements to the tokenizer\n",
    "bpe = BPE(max_iter=1000)\n",
    "bpe.fit(text)\n",
    "enc = bpe.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[727, 1179, 268, 985, 656, 539, 607, 99, 1028, 720, 102, 334, 369, 262, 526, 691, 855, 279, 65, 276, 970, 112, 575, 262, 855, 995, 1179, 268, 1035, 421, 809, 450, 404, 118, 341, 331, 729, 284, 405, 257, 774, 400, 356, 109, 560, 354, 65, 276, 268, 82, 281, 404, 118, 425, 371, 450, 404, 118, 425, 995, 1179, 268, 70, 300, 297, 262, 535, 501, 32, 67, 97, 105, 506, 787, 532, 506, 280, 295, 105, 761, 32, 271, 101, 329, 700, 988, 516, 322, 279, 65, 276, 268, 852, 501, 39, 393, 539, 501, 537, 995, 1179, 268, 1077, 506, 107, 388, 476, 504, 119, 841, 309, 358, 99, 868, 974, 618, 432, 368, 336, 365, 332, 73, 115, 1148, 574, 397, 405, 505, 354, 65, 276, 863, 270, 668, 116, 1180, 374, 275, 537, 376, 611, 343, 389, 100, 842, 58, 294, 119, 612, 722, 438, 464, 83, 952, 1179, 268, 1046, 257, 517, 347, 564, 99, 1001, 115, 995, 1179, 268, 852, 421, 871, 846, 906, 461, 269, 360, 1001, 378, 296, 112, 320, 336, 532, 267, 260, 103, 1181, 332, 572, 97, 117, 258, 269, 1072, 115, 334, 102, 101, 317, 260, 275, 289, 394, 310, 1019, 302, 442, 542, 416, 469, 603, 735, 121, 105, 101, 338, 506, 487, 1121, 422, 264, 1067, 117, 317, 351, 383, 1127, 343, 119, 264, 395, 383, 404, 281, 293, 303, 539, 109, 718, 953, 507, 670, 310, 307, 764, 341, 506, 104, 117, 445, 402, 121, 330, 487, 670, 909, 757, 421, 1002, 100, 315, 58, 305, 322, 267, 996, 986, 10, 935, 102, 307, 505, 260, 117, 378, 296, 111, 98, 106, 814, 259, 339, 324, 288, 321, 264, 351, 280, 355, 267, 10, 265, 118, 363, 269, 266, 284, 719, 116, 596, 466, 273, 1020, 731, 294, 98, 348, 100, 267, 365, 376, 324, 10, 490, 1182, 264, 1132, 280, 97, 406, 409, 400, 637, 32, 1077, 506, 310, 471, 820, 477, 357, 10, 324, 368, 105, 107, 671, 345, 539, 370, 810, 331, 107, 281, 542, 1185, 103, 653, 260, 501, 401, 10, 855, 478, 265, 291, 348, 103, 323, 301, 298, 391, 347, 328, 265, 274, 630, 620, 310, 471, 103, 439, 83, 952, 1179, 268, 87, 394, 282, 368, 344, 99, 1028, 281, 988, 532, 494, 266, 800, 311, 67, 97, 105, 506, 787, 532, 442, 354, 65, 276, 821, 886, 311, 424, 414, 300, 297, 58, 291, 692, 574, 706, 681, 103, 1214, 398, 1021, 375, 116, 121, 676, 952, 1179, 268, 67, 275, 115, 441, 323, 282, 732, 115, 610, 596, 346, 399, 277, 260, 100, 493, 301, 452, 846, 116, 987, 354, 727, 1179, 268, 86, 264, 266, 625, 682, 99, 394, 389, 1029, 527, 284, 804, 424, 406, 318, 540, 940, 269, 259, 301, 393, 487, 362, 399, 112, 438, 260, 424, 508, 491, 621, 292, 368, 114, 263, 100, 676, 952, 1179, 863, 612, 487, 855, 859, 367, 307, 532, 829, 947, 995, 1179, 268, 669, 407, 348, 284, 869, 609, 399, 571, 353, 493, 356, 109, 829, 108, 351, 399, 405, 540, 343, 284, 362, 496, 58, 523, 319, 283, 290, 116, 45, 436, 115, 532, 271, 99, 341, 843, 1122, 298, 395, 1029, 527, 284, 1003, 343, 605, 301, 452, 846, 116, 701, 399, 775, 343, 530, 10, 1038, 380, 433, 109, 1160, 285, 1030, 719, 116, 429, 586, 263, 100, 59, 514, 428, 291, 395, 321, 262, 101, 471, 392, 388, 305, 375, 116, 317, 1128, 257, 290, 452, 118, 300, 116, 117, 439, 83, 952, 1179, 268, 572, 399, 975, 104, 1190, 456, 452, 1228, 334, 303, 282, 294, 99, 846, 259, 97, 10, 794, 373, 265, 476, 371, 582, 288, 565, 551, 467, 1241, 1003, 399, 280, 99, 111, 118, 454, 829, 995, 1179, 268, 641, 401, 288, 565, 1078, 314, 110, 1028, 328, 389, 98, 273, 997, 488, 294, 99, 99, 442, 853, 115, 330, 399, 571, 32, 356, 466, 116, 378, 357, 283, 334, 112, 108, 117, 378, 284, 116, 300, 257, 551, 940, 454, 317, 413, 332, 572, 366, 263, 901, 421, 258, 281, 101, 632, 411, 830, 283, 441, 257, 798, 305, 99, 317, 603, 280, 114, 321, 271, 58, 514, 266, 297, 407, 539, 586, 320, 292, 291, 557, 63, 1214, 67, 847, 317, 404, 464, 65, 276, 268, 958, 398, 439, 727, 1179, 970, 290, 116, 33, 514, 270, 1204, 479, 979, 83, 952, 1179, 268, 87, 269, 463, 77, 271, 271, 105, 506, 65, 103, 336, 663, 97, 376, 493, 362, 571, 294, 108, 1166, 260, 584, 1098, 296, 988, 516, 322, 995, 1179, 268, 72, 692, 493, 104, 275, 485, 271, 597, 58, 289, 394, 494, 305, 450, 259, 686, 652, 464, 77, 927, 572, 517, 107, 39, 378, 329, 846, 116, 987, 843, 262, 265, 291, 418, 63, 514, 345, 919, 282, 10, 660, 298, 320, 737, 1108, 117, 98, 115, 632, 411, 109, 1141, 63, 788, 575, 262, 314, 860, 266, 282, 995, 1179, 268, 79, 334, 298, 442, 265, 507, 280, 328, 348, 501, 110, 400, 1121, 271, 320, 101, 59, 1022, 1099, 395, 659, 553, 108, 292, 478, 301, 116, 110, 718, 609, 539, 1233, 638, 284, 681, 278, 752, 32, 387, 289, 841, 276, 598, 690, 39, 417, 456, 353, 789, 115, 371, 1205, 266, 1003, 461, 269, 10, 490, 317, 269, 260, 358, 297, 980, 298, 391, 258, 115, 58, 1022, 780, 501, 289, 395, 358, 297, 980, 32, 740, 260, 116, 318, 1242, 889, 367, 762, 378, 329, 564, 971, 378, 109, 578, 104, 275, 485, 437, 420, 98, 324, 502, 87, 426, 535, 348, 498, 333, 451, 118, 281, 354, 727, 1179, 268, 852, 608, 1078, 801, 539, 421, 348, 100, 493, 375, 391, 100, 121, 1242, 314, 116, 767, 869, 971, 378, 894, 1151, 317, 568, 412, 1243, 395, 1191, 296, 112, 320, 336, 532, 267, 1007, 282, 371, 489, 525, 289, 614, 502, 1036, 1192, 1182, 264, 374, 265, 478, 100, 1172]\n"
     ]
    }
   ],
   "source": [
    "print(enc[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on\n"
     ]
    }
   ],
   "source": [
    "print(bpe.decode(enc[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, enc_text, blocksize):\n",
    "        self.data = enc_text\n",
    "        self.blocksize = blocksize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.blocksize-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x is a sequence of blocksize characters\n",
    "        x = self.data[idx:idx+self.blocksize]\n",
    "        # y here is the next character after x and will be used as the target\n",
    "        y = self.data[idx+self.blocksize]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNextTokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embed_dim=32, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(block_size * embed_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        emb_flat = emb.view(-1, self.block_size * self.embed_dim)\n",
    "\n",
    "        h = torch.relu(self.bn1(self.fc1(emb_flat)))\n",
    "        h = torch.relu(self.bn2(self.fc2(h)))\n",
    "        logits = self.fc3(h)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text based on continuing provided one\n",
    "def generate(model, block_size, starting_text):\n",
    "    model.eval()\n",
    "    assert len(starting_text) >= block_size\n",
    "    x = torch.tensor(starting_text, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            logits = model(x[-block_size:])\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "            starting_text = starting_text + [next_token.item()]\n",
    "            x = torch.cat([x, next_token.squeeze(0)])\n",
    "    return starting_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_example_text(model, block_size, tokenizer):\n",
    "    starting_text = \"\"\"We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely;\n",
    "but they think we are too dear: the leanness that\n",
    "afflicts us, the object of our misery, is as an\n",
    "inventory to particularise their abundance;'\n",
    "\"\"\"\n",
    "    tokenized_starting_text = tokenizer.encode(starting_text)\n",
    "    continuation = generate(model, block_size, tokenized_starting_text)\n",
    "    return tokenizer.decode(continuation[len(tokenized_starting_text):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!s That brot\n",
      "ese First  thoufrienday  allag.ise  be ess ce gra'tis a S thy o ecice I squeenmust oundWellsucht ind e\n",
      "ormentand UCend \u0019�.\n",
      "\n",
      "MENENIUS:\n",
      "ry That be po�lortdaugh��sservlack!  shall .\n",
      "\n",
      "S�.\n",
      "\n",
      "C\u001bWell�cond e, Ihandloois I'll tunshe 's  wBy \n",
      "\n",
      "itizenthee 3Be�VOLheavenLOUCESTgekingENble OMlackce wor that  do .\n",
      "\n",
      "KING fBe LIgaind, \n"
     ]
    }
   ],
   "source": [
    "# Compare with untrained MLP:\n",
    "vocab_size = max(bpe.vocab)+1\n",
    "block_size = 32\n",
    "mlp_untrained = MLPNextTokenPredictor(vocab_size, block_size)\n",
    "mlp_untrained.to(device)\n",
    "continuation = produce_example_text(mlp_untrained, block_size, bpe)\n",
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, nr_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(nr_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(dataloader)\n",
    "        print(f\"Epoch {epoch} loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 5.761291213815971\n",
      "Epoch 1 loss: 4.641136228356172\n",
      "Epoch 2 loss: 4.362469794041385\n",
      "Epoch 3 loss: 4.225270461966539\n",
      "Epoch 4 loss: 4.133124084775155\n",
      "Epoch 5 loss: 4.0659717723765\n",
      "Epoch 6 loss: 4.017929380843282\n",
      "Epoch 7 loss: 3.9784094860509662\n",
      "Epoch 8 loss: 3.950441330847554\n",
      "Epoch 9 loss: 3.9252919381540865\n",
      "Epoch 10 loss: 3.9090516801235884\n",
      "Epoch 11 loss: 3.8957211529821234\n",
      "Epoch 12 loss: 3.883009298268345\n",
      "Epoch 13 loss: 3.8708929613652647\n",
      "Epoch 14 loss: 3.861272952348069\n"
     ]
    }
   ],
   "source": [
    "# training on tokenized text\n",
    "block_size = 32\n",
    "vocab_size = max(bpe.vocab)+1\n",
    "nr_epochs = 15\n",
    "\n",
    "dataloader = DataLoader(TextDataset(enc, block_size), batch_size=32, shuffle=True, drop_last=True)\n",
    "mlp_tokenizer = MLPNextTokenPredictor(vocab_size, block_size)\n",
    "mlp_tokenizer.to(device)\n",
    "optimizer = optim.AdamW(mlp_tokenizer.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "train(mlp_tokenizer, dataloader, optimizer, nr_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which sight then bege, that courtes of strong\n",
      "Venvoiced on 'Hereforly too found then stonny\n",
      "That acvertenton yet as bithpenfor thee,\n",
      "Even all for the mind.\n",
      "\n",
      "LADY GREY:\n",
      "Are the lov-vendieit ans, from minb laime\n",
      "Bed. I have to be a mever and onquar set prince:\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "# generate new text using tokenization based MLP\n",
    "continuation = produce_example_text(mlp_tokenizer, block_size, bpe)\n",
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 2.0640120022869013\n",
      "Epoch 1 loss: 1.8186363585914431\n",
      "Epoch 2 loss: 1.7595932051131478\n",
      "Epoch 3 loss: 1.7322475972674714\n",
      "Epoch 4 loss: 1.7175702791725784\n",
      "Epoch 5 loss: 1.7072744956934305\n",
      "Epoch 6 loss: 1.7004626252215933\n",
      "Epoch 7 loss: 1.6947294822273882\n",
      "Epoch 8 loss: 1.6905548285940748\n",
      "Epoch 9 loss: 1.6877926724628949\n",
      "Epoch 10 loss: 1.6843374437282914\n",
      "Epoch 11 loss: 1.6820953924055169\n",
      "Epoch 12 loss: 1.679921470157962\n",
      "Epoch 13 loss: 1.67883879745275\n",
      "Epoch 14 loss: 1.6767194793188445\n"
     ]
    }
   ],
   "source": [
    "# training on original text\n",
    "block_size = 64 # let us give more context because of no tokenization\n",
    "# convert text to numbers\n",
    "char_enc = [ord(c) for c in text]\n",
    "vocab_size = max(char_enc) + 1\n",
    "nr_epochs = 15\n",
    "\n",
    "dataloader = DataLoader(TextDataset(char_enc, block_size), batch_size=32, shuffle=True, drop_last=True)\n",
    "mlp_wo_tokenizer = MLPNextTokenPredictor(vocab_size, block_size)\n",
    "mlp_wo_tokenizer.to(device)\n",
    "optimizer = optim.AdamW(mlp_wo_tokenizer.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "train(mlp_wo_tokenizer, dataloader, optimizer, nr_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll dayused you shous will agueet. What you.\n",
      "\n",
      "PRIEL:\n",
      "She hereath'd prothy, is upon at jor mer.\n",
      "\n",
      "LAD\n"
     ]
    }
   ],
   "source": [
    "# generate new text using non-tokenization based MLP\n",
    "class no_tok:\n",
    "    def encode(self, text):\n",
    "        return [ord(c) for c in text]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([chr(i) for i in ids])\n",
    "\n",
    "t = produce_example_text(mlp_wo_tokenizer, block_size, no_tok())\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
