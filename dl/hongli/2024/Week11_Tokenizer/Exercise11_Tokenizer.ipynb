{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3UX-37U_MBB"
   },
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr5Zvjec_MBE"
   },
   "source": [
    "## Group ID: \n",
    "*   Person1\n",
    "*   Person2\n",
    "*   Person3\n",
    "\n",
    "## Exercise day: Tuesday/Wednesday\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "In this exercise you will implement a Tokenizer class that will be used to tokenize a given text.\n",
    "A token will be a short sequence of bytes and tokenization will be about splitting up the text into those short substrings and then representing it in terms of token ids.\n",
    "This allows language models to work on a better text representation than just using a character level model as in the last exercise.\n",
    "We will use the so called \"Byte Pair Encoding\" (BPE) algorithm to tokenize the text. The BPE algorithm is a simple algorithm that iteratively merges the most frequent pair of bytes/ids in the text. This notebook will guide you through the implementation of the Tokenizer class. To make the representation of the text easier, we will use integer ids to represent the bytes of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The tokenizer is not part of the model, it is a preprocessing step that is used to tokenize the text before feeding it to the model. Therefore, it can be trained on a different dataset than the model.\n",
    "Once trained, the tokenizer is capable of encoding and decoding any given text to/from a list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Implement the `get_stats` method that will return a dictionary containing the frequency of each pair of characters in the text. (0.5 points)\n",
    "1. Implement the `merge` method that will merge the most frequent pair of characters in the text. (0.5 points)\n",
    "1. Implement the `fit` method that will train the tokenizer on the given text. (1 point)\n",
    "1. Implement the `encode` method that will encode the given text to a list of tokens. (1 point)\n",
    "1. Implement the `decode` method that will decode the given list of tokens to a text. (1 point)\n",
    "1. Implement the `BPE` class. This class should contain the previously implemented methods. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\" Returns a dictionary with the number of times each pair of ids appears in the input list \"\"\"\n",
    "    dict = {}\n",
    "    for i in range(len(ids) - 1):\n",
    "        # create a tuple with the pair of ids\n",
    "        pair = (ids[i], ids[(i+1)])\n",
    "        # if the pair is already in the dictionary, increment the count\n",
    "        if pair in dict:\n",
    "            dict[pair] += 1\n",
    "        # if the pair is not in the dictionary, add it\n",
    "        else:\n",
    "            dict[pair] = 1\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 3, (2, 3): 3, (3, 1): 2}\n"
     ]
    }
   ],
   "source": [
    "ids = [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
    "expected = {(1, 2): 3, (2, 3): 3, (3, 1): 2}\n",
    "assert get_stats(ids) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, new_id):\n",
    "    \"\"\"Merge every pair of elements in ids that are equal to pair into a single element new_id\"\"\"\n",
    "    _ids = ids.copy()\n",
    "    # the length of the list will change as we merge elements, so we need to check the length of the list at each iteration\n",
    "    i = 0\n",
    "    while i < len(_ids) - 1:\n",
    "        if _ids[i] == pair[0] and _ids[i+1] == pair[1]:\n",
    "            _ids[i] = new_id\n",
    "            _ids.pop(i+1)\n",
    "        i += 1\n",
    "\n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "pair = (0,1)\n",
    "new_id = 4\n",
    "new_ids = merge(ids, pair, new_id)\n",
    "expected = [4,2,3,4,2,3]\n",
    "assert new_ids == expected, f\"Merge failed new_ids actual: {new_ids} expected: {expected}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(ids, max_iter=1000):\n",
    "    \"\"\"Fit the model to the data by merging recursively the most common pairs of ids max_iter times or until no pair appears more than once. If two pairs have the same frequency, the one that appears first is chosen.\n",
    "    Returns the fitted model and a dictionary with the merges containing the new_id for each pair.\n",
    "    To ensure each new_id is unique (and our results are comparable), it is set to the maximum id in the list plus one.\n",
    "    \"\"\"\n",
    "    merges = {}\n",
    "    _ids = ids.copy()\n",
    "    for i in range(max_iter):\n",
    "        stats = get_stats(_ids)\n",
    "        # if there are no pairs, we are done\n",
    "        if not stats:\n",
    "            break\n",
    "\n",
    "        # get the most common pair\n",
    "        pair = max(stats, key=stats.get)\n",
    "\n",
    "        # assign a new id to the pair\n",
    "        new_id = max(_ids) + 1\n",
    "\n",
    "        # merge the pair\n",
    "        _ids = merge(_ids, pair, new_id)\n",
    "\n",
    "        # store the pair and its id\n",
    "        merges[pair] = new_id\n",
    "\n",
    "    return _ids, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "num_iterations = 1\n",
    "new_ids, merges = fit(ids,num_iterations)\n",
    "assert new_ids == [4,2,3,4,2,3], f\"Wrong ids after fit {new_ids}\"\n",
    "assert merges == {(0,1):4}, f\"Wrong merges after fit {merges}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "num_iterations = 2\n",
    "new_ids, merges = fit(ids,num_iterations)\n",
    "assert new_ids == [5,3,5,3], f\"Wrong ids after fit {new_ids}\"\n",
    "assert merges == {(0,1):4, (4,2):5}, f\"Wrong merges after fit {merges}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ids, merges):\n",
    "    \"\"\"Encode the input list of ids using the merges dictionary\"\"\"\n",
    "    _merges = merges.copy()\n",
    "    _ids = ids.copy()\n",
    "    for i in range(len(merges)):\n",
    "        pair = min(_merges, key=_merges.get)\n",
    "        pair_id = _merges.pop(pair)\n",
    "        j = 0\n",
    "        while j < len(_ids) - 1:\n",
    "            if _ids[j] == pair[0] and _ids[j+1] == pair[1]:\n",
    "                _ids[j] = pair_id\n",
    "                _ids.pop(j+1)\n",
    "            j += 1\n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [0,1,2,3,0,1,2,3]\n",
    "merges = {(0,1):4}\n",
    "encoded_ids = encode(ids, merges)\n",
    "expected = [4,2,3,4,2,3]\n",
    "assert encoded_ids == expected, f\"Wrong encoded ids {encoded_ids} expected: {expected}\"\n",
    "assert ids == [0,1,2,3,0,1,2,3], f\"Input ids should not be modified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, merges):\n",
    "    \"\"\"Decode the input list of ids using the merges dictionary\"\"\"\n",
    "    _merges = merges.copy()\n",
    "    _ids = ids.copy()\n",
    "    for i in range(len(merges)):\n",
    "        pair = max(_merges, key=_merges.get)\n",
    "        pair_id = _merges.pop(pair)\n",
    "        j = 0\n",
    "        while j < len(_ids):\n",
    "            if _ids[j] == pair_id:\n",
    "                _ids[j] = pair[0]\n",
    "                _ids.insert(j+1, pair[1])\n",
    "            j += 1\n",
    "\n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [6,6]\n",
    "merges = {(0,1):4, (4,2):5, (5,3):6}\n",
    "decoded_ids = decode(ids, merges)\n",
    "expected = [0,1,2,3,0,1,2,3]\n",
    "assert decoded_ids == expected, f\"Wrong decoded ids {decoded_ids} expected: {expected}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self, max_iter:int=1000):\n",
    "        self.max_iter = max_iter\n",
    "        self.vocab = {idx: bytes([idx]).decode(\"utf-8\", errors=\"replace\") for idx in range(256)}\n",
    "        self.merges = None\n",
    "\n",
    "    def fit(self, text):\n",
    "        # convert the input text to a list of ids using utf-8 encoding\n",
    "        # rewrite the code to use the utf-8 encoding (256 predefined encodings)\n",
    "        max_iter = self.max_iter\n",
    "\n",
    "        merges = {}\n",
    "        ids = list(map(int, text.encode(\"utf-8\")))\n",
    "        _ids = ids.copy()\n",
    "        for i in range(max_iter):\n",
    "            stats = get_stats(_ids)\n",
    "            if not stats:\n",
    "                break\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # assign a new id to the pair, ensuring it is unique\n",
    "            new_id = max(_ids + [256]) + 1\n",
    "            # merge the pair\n",
    "            _ids = merge(_ids, pair, new_id)\n",
    "            # store the pair and its id\n",
    "            merges[pair] = new_id\n",
    "\n",
    "        self.merges = merges\n",
    "        self.vocab.update({v: k for k, v in merges.items()})\n",
    "        return _ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        if self.merges is None:\n",
    "            raise ValueError(\"Model has not been fitted\")\n",
    "        ids = list(map(int, text.encode(\"utf-8\")))\n",
    "        return encode(ids, self.merges)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if self.merges is None:\n",
    "            raise ValueError(\"Model has not been fitted\")\n",
    "        ids = decode(ids, self.merges)\n",
    "        # convert the list of ids back to a string\n",
    "        text = bytes(ids).decode(\"utf-8\", errors=\"replace\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 97, 100, 259, 32, 260, 97, 100, 259]\n",
      "{(97, 98): 257, (257, 114): 258, (258, 97): 259, (259, 99): 260}\n"
     ]
    }
   ],
   "source": [
    "# testing the BPE class\n",
    "text = \"abracadabra abracadabra\"\n",
    "bpe = BPE(max_iter=4)\n",
    "bpe.fit(text)\n",
    "encoded_text = bpe.encode(text)\n",
    "print(encoded_text)\n",
    "print(bpe.merges)\n",
    "decoded_text = bpe.decode(encoded_text)\n",
    "assert decoded_text == text, f\"Decoded text {decoded_text} does not match original text {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process. The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\" # cleaned version of the first paragraph of the Wikipedia page on LLMs\n",
    "ids = list(map(int, text.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE(max_iter=1000)\n",
    "bpe.fit(text)\n",
    "encoded_text = bpe.encode(text)\n",
    "decoded_text = bpe.decode(encoded_text)\n",
    "assert text == decoded_text, \"Decoding failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: \n",
    "\n",
    "Let us check what a tokenizer will give us for training a simple language model.\n",
    "\n",
    "We will use the tiny Shakespeare dataset and train with and without tokenization and compare generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-08 23:32:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  5.05MB/s    in 0.2s    \n",
      "\n",
      "2025-01-08 23:32:10 (5.05 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tokenizer\n",
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "bpe = BPE(max_iter=5)\n",
    "bpe.fit(text)\n",
    "enc = bpe.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 105, 114, 115, 259, 67, 105, 116, 105, 122, 101, 110, 58, 10, 66, 101, 102, 111, 114, 257, 119, 257, 112, 114, 111, 99, 101, 101, 261, 97, 110, 121, 32, 102, 117, 114, 258, 101, 114, 44, 32, 104, 101, 97, 114, 32, 109, 257, 115, 112, 101, 97, 107, 46, 10, 10, 65, 108, 108, 58, 10, 83, 112, 101, 97, 107, 44, 32, 115, 112, 101, 97, 107, 46, 10, 10, 70, 105, 114, 115, 259, 67, 105, 116, 105, 122, 101, 110, 58, 10, 89, 111, 117, 32, 97, 114, 257, 97, 108, 108, 32, 114, 101, 115, 111, 108, 118, 101, 261, 114, 97, 258, 101, 114, 32, 116, 111, 32, 100, 105, 257, 258, 97, 110, 32, 116, 111, 32, 102, 97, 109, 105, 115, 104, 63, 10, 10, 65, 108, 108, 58, 10, 82, 101, 115, 111, 108, 118, 101, 100, 46, 32, 114, 101, 115, 111, 108, 118, 101, 100, 46, 10, 10, 70, 105, 114, 115, 259, 67, 105, 116, 105, 122, 101, 110, 58, 10, 70, 105, 114, 115, 116, 44, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 67, 97, 105, 117, 260, 77, 97, 114, 99, 105, 117, 260, 105, 260, 99, 104, 105, 101, 102, 32, 101, 110, 101, 109, 121, 32, 116, 111, 32, 258, 257, 112, 101, 111, 112, 108, 101, 46, 10, 10, 65, 108, 108, 58, 10, 87, 257, 107, 110, 111, 119, 39, 116, 44, 32, 119, 257, 107, 110, 111, 119, 39, 116, 46, 10, 10, 70, 105, 114, 115, 259, 67, 105, 116, 105, 122, 101, 110, 58, 10, 76, 101, 259, 117, 260, 107, 105, 108, 108, 32, 104, 105, 109, 44, 32, 97, 110, 261, 119, 101, 39, 108, 108, 32, 104, 97, 118, 257, 99, 111, 114, 110, 32, 97, 259, 111, 117, 114, 32, 111, 119, 110, 32, 112, 114, 105, 99, 101, 46, 10, 73, 115, 39, 259, 97, 32, 118, 101, 114, 100, 105, 99, 116, 63, 10, 10, 65, 108, 108, 58, 10, 78, 111, 32, 109, 111, 114, 257, 116, 97, 108, 107, 105, 110, 103, 32, 111, 110, 39, 116, 59, 32, 108, 101, 259, 105, 259, 98, 257, 100, 111, 110, 101, 58, 32, 97, 119, 97, 121, 44, 32, 97, 119, 97, 121, 33, 10, 10, 83, 101, 99, 111, 110, 261, 67, 105, 116, 105, 122, 101, 110, 58, 10, 79, 110, 257, 119, 111, 114, 100, 44, 32, 103, 111, 111, 261, 99, 105, 116, 105, 122, 101, 110, 115, 46, 10, 10, 70, 105, 114, 115, 259, 67, 105, 116, 105, 122, 101, 110, 58, 10, 87, 257, 97, 114, 257, 97, 99, 99, 111, 117, 110, 116, 101, 261, 112, 111, 111, 114, 32, 99, 105, 116, 105, 122, 101, 110, 115, 44, 32, 258, 257, 112, 97, 116, 114, 105, 99, 105, 97, 110, 260, 103, 111, 111, 100, 46, 10, 87, 104, 97, 259, 97, 117, 258, 111, 114, 105, 116, 121, 32, 115, 117, 114, 102, 101, 105, 116, 260, 111, 110, 32, 119, 111, 117, 108, 261, 114, 101, 108, 105, 101, 118, 257, 117, 115, 58, 32, 105, 102, 32, 258, 101, 121, 10, 119, 111, 117, 108, 261, 121, 105, 101, 108, 261, 117, 260, 98, 117, 259, 258, 257, 115, 117, 112, 101, 114, 102, 108, 117, 105, 116, 121, 44, 32, 119, 104, 105, 108, 257, 105, 259, 119, 101, 114, 101, 10, 119, 104, 111, 108, 101, 115, 111, 109, 101, 44, 32, 119, 257, 109, 105, 103, 104, 259, 103, 117, 101, 115, 260, 258, 101, 121, 32, 114, 101, 108, 105, 101, 118, 101, 261, 117, 260, 104, 117, 109, 97, 110, 101, 108, 121, 59, 10, 98, 117, 259, 258, 101, 121, 32, 258, 105, 110, 107, 32, 119, 257, 97, 114, 257, 116, 111, 111, 32, 100, 101, 97, 114, 58, 32, 258, 257, 108, 101, 97, 110, 110, 101, 115, 260, 258, 97, 116, 10, 97, 102, 102, 108, 105, 99, 116, 260, 117, 115, 44, 32, 258, 257, 111, 98, 106, 101, 99, 259, 111, 102, 32, 111, 117, 114, 32, 109, 105, 115, 101, 114, 121, 44, 32, 105, 260, 97, 260, 97, 110, 10, 105, 110, 118, 101, 110, 116, 111, 114, 121, 32, 116, 111, 32, 112, 97, 114, 116, 105, 99, 117, 108, 97, 114, 105, 115, 257, 258, 101, 105, 114, 32, 97, 98, 117, 110, 100, 97, 110, 99, 101, 59, 32, 111, 117, 114, 10, 115, 117, 102, 102, 101, 114, 97, 110, 99, 257, 105, 260, 97, 32, 103, 97, 105, 110, 32, 116, 111, 32, 258, 101, 109, 32, 76, 101, 259, 117, 260, 114, 101, 118, 101, 110, 103, 257, 258, 105, 260, 119, 105, 258, 10, 111, 117, 114, 32, 112, 105, 107, 101, 115, 44, 32, 101, 114, 257, 119, 257, 98, 101, 99, 111, 109, 257, 114, 97, 107, 101, 115, 58, 32, 102, 111, 114, 32, 258, 257, 103, 111, 100, 260, 107, 110, 111, 119, 32, 73, 10, 115, 112, 101, 97, 107, 32, 258, 105, 260, 105, 110, 32, 104, 117, 110, 103, 101, 114, 32, 102, 111, 114, 32, 98, 114, 101, 97, 100, 44, 32, 110, 111, 259, 105, 110, 32, 258, 105, 114, 115, 259, 102, 111, 114, 32, 114, 101, 118, 101, 110, 103, 101, 46, 10, 10, 83, 101, 99, 111, 110, 261, 67, 105, 116, 105, 122, 101, 110, 58, 10, 87, 111, 117, 108, 261, 121, 111, 117, 32, 112, 114, 111, 99, 101, 101, 261, 101, 115, 112, 101, 99, 105, 97, 108, 108, 121, 32, 97, 103, 97, 105, 110, 115, 259, 67, 97, 105, 117, 260, 77, 97, 114, 99, 105, 117, 115, 63, 10, 10, 65, 108, 108, 58, 10, 65, 103, 97, 105, 110, 115, 259, 104, 105, 109, 32, 102, 105, 114, 115, 116, 58, 32, 104, 101, 39, 260, 97, 32, 118, 101, 114, 121, 32]\n"
     ]
    }
   ],
   "source": [
    "print(enc[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Ma\n"
     ]
    }
   ],
   "source": [
    "print(bpe.decode(enc[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, enc_text, blocksize):\n",
    "        self.data = enc_text\n",
    "        self.blocksize = blocksize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-self.blocksize-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x is a sequence of blocksize characters\n",
    "        x = self.data[idx:idx+self.blocksize]\n",
    "        # y here is the next character after x and will be used as the target\n",
    "        y = self.data[idx+self.blocksize]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNextTokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embed_dim=32, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(block_size * embed_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        emb_flat = emb.view(-1, self.block_size * self.embed_dim)\n",
    "\n",
    "        h = torch.relu(self.bn1(self.fc1(emb_flat)))\n",
    "        h = torch.relu(self.bn2(self.fc2(h)))\n",
    "        logits = self.fc3(h)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text based on continuing provided one\n",
    "def generate(model, block_size, starting_text):\n",
    "    model.eval()\n",
    "    assert len(starting_text) >= block_size\n",
    "    x = torch.tensor(starting_text, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            logits = model(x[-block_size:])\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "            starting_text = starting_text + [next_token.item()]\n",
    "            x = torch.cat([x, next_token.squeeze(0)])\n",
    "    return starting_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_example_text(model, block_size, tokenizer):\n",
    "    starting_text = \"\"\"We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely;\n",
    "but they think we are too dear: the leanness that\n",
    "afflicts us, the object of our misery, is as an\n",
    "inventory to particularise their abundance;'\n",
    "\"\"\"\n",
    "    tokenized_starting_text = tokenizer.encode(starting_text)\n",
    "    continuation = generate(model, block_size, tokenized_starting_text)\n",
    "    return tokenizer.decode(continuation[len(tokenized_starting_text):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�����k�o�B�\u0001\u001cEG�d �+m��\u0018�Me �3>0�F���\u0015�h��xW\u0010e��2\u001ae \fs ce�����+\u0018\u000e��E���L{��\u0002>c�s\u00036th���]��HXm\u001f>\u001d\u0005�G\n"
     ]
    }
   ],
   "source": [
    "# Compare with untrained MLP:\n",
    "vocab_size = max(bpe.vocab)+1\n",
    "block_size = 32\n",
    "mlp_untrained = MLPNextTokenPredictor(vocab_size, block_size)\n",
    "mlp_untrained.to(device)\n",
    "continuation = produce_example_text(mlp_untrained, block_size, bpe)\n",
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, nr_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(nr_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(dataloader)\n",
    "        print(f\"Epoch {epoch} loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 2.1924059386978287\n",
      "Epoch 1 loss: 1.9405911034747791\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m mlp_tokenizer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(mlp_tokenizer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[205], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, nr_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mvocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training on tokenized text\n",
    "block_size = 32\n",
    "vocab_size = max(bpe.vocab)+1\n",
    "nr_epochs = 15\n",
    "\n",
    "dataloader = DataLoader(TextDataset(enc, block_size), batch_size=32, shuffle=True, drop_last=True)\n",
    "mlp_tokenizer = MLPNextTokenPredictor(vocab_size, block_size)\n",
    "mlp_tokenizer.to(device)\n",
    "optimizer = optim.AdamW(mlp_tokenizer.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "train(mlp_tokenizer, dataloader, optimizer, nr_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whear her boy parth holour! speakd up the o\n",
      "phonoure quigle-by the swrut. Camite;\n",
      "But whose fromisints masbuse \n"
     ]
    }
   ],
   "source": [
    "# generate new text using tokenization based MLP\n",
    "continuation = produce_example_text(mlp_tokenizer, block_size, bpe)\n",
    "print(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 2.062389275977341\n",
      "Epoch 1 loss: 1.815641518650402\n",
      "Epoch 2 loss: 1.7578229227564515\n",
      "Epoch 3 loss: 1.7307396914719837\n",
      "Epoch 4 loss: 1.715839851543524\n",
      "Epoch 5 loss: 1.7057468593120597\n",
      "Epoch 6 loss: 1.6994272638220285\n",
      "Epoch 7 loss: 1.694440029821013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m mlp_wo_tokenizer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(mlp_wo_tokenizer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_wo_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[205], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, nr_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mvocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/torch/optim/optimizer.py:952\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 952\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training on original text\n",
    "block_size = 64 # let us give more context because of no tokenization\n",
    "# convert text to numbers\n",
    "char_enc = [ord(c) for c in text]\n",
    "vocab_size = max(char_enc) + 1\n",
    "nr_epochs = 15\n",
    "\n",
    "dataloader = DataLoader(TextDataset(char_enc, block_size), batch_size=32, shuffle=True, drop_last=True)\n",
    "mlp_wo_tokenizer = MLPNextTokenPredictor(vocab_size, block_size)\n",
    "mlp_wo_tokenizer.to(device)\n",
    "optimizer = optim.AdamW(mlp_wo_tokenizer.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "train(mlp_wo_tokenizer, dataloader, optimizer, nr_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K vad sTriciusid,\n",
      "ory full.\n",
      "Forthy:\n",
      "He wish exter's my cloot thou mither.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now, new;\n"
     ]
    }
   ],
   "source": [
    "# generate new text using non-tokenization based MLP\n",
    "class no_tok:\n",
    "    def encode(self, text):\n",
    "        return [ord(c) for c in text]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([chr(i) for i in ids])\n",
    "\n",
    "t = produce_example_text(mlp_wo_tokenizer, block_size, no_tok())\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
