{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group ID: \n",
    "## Exercise day: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_train = datasets.FashionMNIST(root='data', train=True, download=True)\n",
    "fashion_mnist_test = datasets.FashionMNIST(root='data', train=False, download=True)\n",
    "\n",
    "X_train = fashion_mnist_train.data.reshape(-1, 1, 28, 28).float() / 255\n",
    "y_train = fashion_mnist_train.targets\n",
    "\n",
    "X_test = fashion_mnist_test.data.reshape(-1, 1, 28, 28).float() / 255\n",
    "y_test = fashion_mnist_test.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset and DataLoader\n",
    "\n",
    "To make the training process easier, we will use the PyTorch `Dataset` and `DataLoader` classes. The Dataset class is an abstract class representing a dataset, while the DataLoader class provides an iterable over a dataset. In this case, we use the `TensorDataset` to wrap our data and the DataLoader class to iterate over the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "With all the introduction steps done, we can now start implementing the `GoogLeNet` architecture.\n",
    "In this  exercise, you will implement a small version of the GoogLeNet architecture. The GoogLeNet architecture is a deep convolutional neural network that was proposed by Szegedy et al. in 2014. The architecture is known for its use of `inception modules`, which are modules that perform multiple convolutions with different filter sizes and then concatenate the results. The architecture also uses `auxiliary classifiers` to help with training.\n",
    "This notebook will guide you through the implementation and there will be the following subtasks you need to complete:\n",
    "\n",
    "1. Implement the Inception module (1 points)\n",
    "2. Implement the auxiliary classifier (1 point)\n",
    "3. Implement the BabyGoogLeNet architecture (2 point)\n",
    "4. Train the model on FashionMNIST (1 point)\n",
    "\n",
    "For a more detailed explanation of the GoogLeNet architecture, you can refer to the [original paper](https://arxiv.org/abs/1409.4842).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Module\n",
    "\n",
    "The following figure shows the structure of the inception module, where we here only refer to the subfigure (b) Inception module with dimesion reduction:\n",
    "\n",
    "![Inception Module](inception_module.png)\n",
    "\n",
    "The inception module is a module that performs multiple convolutions with different filter sizes and then concatenates the results. The module consists of four branches, each of which performs a different operation. The first branch performs a 1x1 convolution, the second branch performs a 1x1 convolution followed by a 3x3 convolution, the third branch performs a 1x1 convolution followed by a 5x5 convolution, and the fourth branch performs a 3x3 max pooling followed by a 1x1 convolution. The outputs of the four branches are then concatenated along the channel dimension.\n",
    "After each convolution operation, a ReLU activation function is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the Inception module (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following PyTorch functions will be useful for implementing the different modules in this exercise:\n",
    "\n",
    "- `nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)`: Creates a 2D convolutional layer with the specified number of input and output channels, kernel size, stride, and padding.\n",
    "\n",
    "- `nn.ReLU()`: Creates a ReLU activation function.\n",
    "\n",
    "- `nn.LeakyReLU()`: Creates a Leaky ReLU activation function.\n",
    "\n",
    "- `torch.cat(tensors, dim)`: Concatenates the given sequence of tensors along the given dimension.\n",
    "\n",
    "- `torch.flatten(input, start_dim, end_dim)`: Flattens a contiguous range of dims into a tensor.\n",
    "\n",
    "- `nn.MaxPool2d(kernel_size, stride, padding)`: Creates a 2D max pooling layer with the specified kernel size, stride, and padding.\n",
    "\n",
    "- `nn.AvgPool2d(kernel_size, stride, padding)`: Creates a 2D average pooling layer with the specified kernel size, stride, and padding.\n",
    "\n",
    "- `nn.AdaptiveAvgPool2d(output_size)`: Creates an adaptive average pooling layer that outputs a tensor with the specified output size.\n",
    "\n",
    "- `nn.Sequential(*args)`: A sequential container that stores a sequence of layers. The layers will be executed in order. e.g `nn.Sequential(nn.Conv2d(1, 1, 3), nn.ReLU(), nn.Conv2d(1, 1, 3))` will create a sequential model with two convolutional layers and a ReLU activation function inbetween.\n",
    "\n",
    "You can also use any other PyTorch functions that you find useful. You can refer to the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you need to keep in mind using PyTorch is that each building block you implement should be a subclass of `nn.Module`. This is because PyTorch uses a dynamic computation graph, and subclassing `nn.Module` allows PyTorch to keep track of the parameters of the model. Specify the layers you want to use in the `__init__` method and define the forward pass in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        ### Your code here ###\n",
    "\n",
    "        # allocate 1/4 of the output channels to each branch\n",
    "        branch_out_channels = out_channels // 4\n",
    "        # first branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_out_channels, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # second branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, branch_out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # third branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, branch_out_channels, kernel_size=5, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # fourth branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, branch_out_channels, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #####################Ã¤\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Your code here ###\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        # B x C x H x W\n",
    "        out = torch.cat((branch1, branch2, branch3, branch4), -3)\n",
    "        return out\n",
    "\n",
    "        ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the InceptionModule output shape\n",
    "module = InceptionModule(1, 64)\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "assert module(x).shape == torch.Size([1, 64, 28, 28]), f\"Shape was {module(x).shape} instead of torch.Size([1, 64, 28, 28])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the auxiliary classifier (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auxiliary classifier is a small classifier that is added to the network to help with training. The auxiliary classifier is added after the first and second inception module and consists of a 3x3 average pooling layer followed by a 1x1 convolutional layer, LeakyReLU activation, Adaptive average pooling with output size (1,1) and a fully connected layer. The auxiliary classifier is used to provide additional supervision to the network during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the auxiliary classifier with the following structure:\n",
    "\n",
    "- 3x3 average pooling layer with stride 2\n",
    "- 1x1 convolutional layer\n",
    "- LeakyReLU activation function\n",
    "- Adaptive average pooling layer with output size (1, 1)\n",
    "- Fully connected layer\n",
    "\n",
    "The in_channels and out_channels of the convolutional layer are specified as arguments to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxillaryClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(AuxillaryClassifier, self).__init__()\n",
    "        ### Your code here ###\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # 3x3 average pooling\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2),\n",
    "            # 1x1 conv\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            # adaptive average pooling with output size 1x1\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            # fully connected layer, has to be flattened first\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out_channels, num_classes)\n",
    "\n",
    "        )\n",
    "\n",
    "        #####################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Your code here ###\n",
    "        return self.layers(x)\n",
    "        ######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AuxillaryClassifier output shape\n",
    "module = AuxillaryClassifier(64, 64, 10)\n",
    "x = torch.randn(4, 64, 28, 28)\n",
    "\n",
    "assert module(x).shape == torch.Size([4, 10]), f\"Shape was {module(x).shape} instead of torch.Size([4, 10])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement the BabyGoogLeNet architecture (2 point)\n",
    "\n",
    "The GoogLeNet architecture consists of multiple inception modules, followed by a global average pooling layer and a fully connected layer. The architecture also uses auxillary classifiers to help with training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original GoogLeNet architecture, we will use a smaller version of the architecture in this exercise. The architecture consists of three inception modules, followed by a global average pooling layer and a fully connected layer. The architecture also uses an auxillary classifier after the first and second inception module.\n",
    "\n",
    "Implement the BabyGoogLeNet architecture with the following structure:\n",
    "\n",
    "- CNN block:\n",
    "    - 3x3 convolutional layer with 16 filters and stride 1 and padding 1\n",
    "    - LeakyReLU activation function\n",
    "    - 3x3 max pooling layer with stride 2 and padding 1\n",
    "- Inception module 1 (16 input_channels and 32 output_channels)\n",
    "- Auxillary classifier 1 (32 input_channels, 64 output_channels and 10 output_channels)\n",
    "- max pooling layer with kernel size 3 and stride 2 and padding 1\n",
    "- Inception module 2 (32 input_channels and 64 output_channels)\n",
    "- Auxillary classifier 2 (64 input_channels, 128 output_channels and 10 output_channels)\n",
    "- max pooling layer with kernel size 3 and stride 2 and padding 1\n",
    "- Inception module 3 (64 input_channels and 128 output_channels)\n",
    "- Global average pooling layer with output size 1 using `nn.AdaptiveAvgPool2d`\n",
    "- Fully connected layer\n",
    "\n",
    "As the auxillary classifiers are only used during training, you have to distinguish between the training and evaluation mode. You can do this by checking the model's mode using `model.training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyGoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BabyGoogLeNet, self).__init__()\n",
    "        ### Your code here ###\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3,\n",
    "                      padding=1, stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.inception1 = InceptionModule(16, 32)\n",
    "        self.aux1 = AuxillaryClassifier(32, 64, 10)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.inception2 = InceptionModule(32, 64)\n",
    "        self.aux2 = AuxillaryClassifier(64, 128, 10)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.inception3 = InceptionModule(64, 128)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "        ######################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Your code here ###\n",
    "        # if self.training, return the auxillary classifiers as well\n",
    "        x = self.cnn(x)\n",
    "        x = self.inception1(x)\n",
    "\n",
    "        if self.training:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.inception2(x)\n",
    "\n",
    "        if self.training:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.inception3(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x, aux1, aux2\n",
    "        else:\n",
    "            return x\n",
    "        ######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the BabyGoogLeNet output shape\n",
    "model = BabyGoogLeNet()\n",
    "x = torch.randn(4, 1, 28, 28)\n",
    "\n",
    "# Training mode\n",
    "output, aux1, aux2 = model(x)\n",
    "assert output.shape == torch.Size([4, 10]), f\"Shape was {output.shape} instead of torch.Size([4, 10])\"\n",
    "assert aux1.shape == torch.Size([4, 10]), f\"Shape was {aux1.shape} instead of torch.Size([4, 10])\"\n",
    "assert aux2.shape == torch.Size([4, 10]), f\"Shape was {aux2.shape} instead of torch.Size([4, 10])\"\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "output = model(x)\n",
    "assert output.shape == torch.Size([4, 10]), f\"Shape was {output.shape} instead of torch.Size([4, 10])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    model.train()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement the training loop (1 point)\n",
    "\n",
    "You will also implement parts of the training loop for the BabyGoogLeNet architecture. The following steps have to be implemented:\n",
    "\n",
    "- Forward pass: Pass the input through the network to get the output.\n",
    "- Calculate the loss: Calculate the loss using the outputs (main output and the two auxiliary outputs) and the target.\n",
    "- Backward pass: Perform backpropagation to calculate the gradients.\n",
    "- Update the parameters: Update the weights of the model.\n",
    "- Zero the gradients: Zero the gradients of the model after updating the weights. For this you can use the `nn.Module.zero_grad()` function.\n",
    "\n",
    "The loss function should be of the following form:\n",
    "\n",
    "$$ loss(target, output, aux\\_output_1, aux\\_output_2, w_{aux}) = \n",
    "\\\\ \\text{criterion}(output, target) + w_{aux} \\times \\text{criterion}(aux\\_output_1, target) + w_{aux} \\times \\text{criterion}(aux\\_output_2, target) $$\n",
    "\n",
    "where `criterion` is the cross-entropy loss function and $w_{aux}$ is a hyperparameter that controls the weight of the auxillary classifiers. We set it to 0.9 for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, train_loader, test_loader, learning_rate, n_epochs, aux_loss_weight, device):\n",
    "    print(f'Training on {device} with learning rate of {learning_rate} for {n_epochs} epochs')\n",
    "    # Transfer model to device, this is necessary if we want to use GPU acceleration\n",
    "    model.to(device)\n",
    "    for i in range(n_epochs):\n",
    "        # Set model to training mode, this is necessary because some layers like the auxillary classifiers in our model behave differently in training mode and evaluation mode\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device) # Model and data have to be on the same device\n",
    "            # Your code here #\n",
    "            # Forward pass\n",
    "            y_pred, aux1, aux2 = model(X_batch)\n",
    "            # Calculate loss\n",
    "            loss = criterion(y_pred, y_batch) + \\\n",
    "                             aux_loss_weight * (criterion(aux1, y_batch) + \\\n",
    "                                                criterion(aux2, y_batch))\n",
    "            epoch_loss += loss.item()\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            for param in model.parameters():\n",
    "                param.data -= learning_rate * param.grad\n",
    "            # Zero gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            ##################\n",
    "        with torch.no_grad():\n",
    "            # Set model to evaluation mode\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_acc = validate(model, test_loader, device)\n",
    "        train_acc = validate(model, train_loader, device)\n",
    "        print(f'Epoch {i+1}, train loss {epoch_loss/len(train_loader)}, test loss: {val_loss/len(test_loader)}, train acc: {train_acc}, test acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BabyGoogLeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "aux_loss_weight = 0.9 # Scales the loss of the auxillary classifiers\n",
    "lr = 0.1  # 0.1 and 0.2 should work well enough\n",
    "n_epochs = 25\n",
    "# Set the device to 'cuda' if you have a GPU available, otherwise set it to 'mps' if you have Metal Performance Shaders (Apple's API for programming metal GPU) available, otherwise set it to 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for at least 25 epochs.\n",
    "Your model should achieve a validation accuracy of roughly 90% now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda with learning rate of 0.1 for 25 epochs\n",
      "Epoch 1, train loss 5.114478130330409, test loss: 1.05544536850255, train acc: 0.5681166666666667, test acc: 0.5636\n",
      "Epoch 2, train loss 2.4984487278629213, test loss: 0.864342112639907, train acc: 0.7153833333333334, test acc: 0.7098\n",
      "Epoch 3, train loss 1.7412569794192243, test loss: 0.4867699741368081, train acc: 0.8323333333333334, test acc: 0.821\n",
      "Epoch 4, train loss 1.4577088203511512, test loss: 0.3892122449199106, train acc: 0.8656833333333334, test acc: 0.8499\n",
      "Epoch 5, train loss 1.3113781955323494, test loss: 0.4390952677293948, train acc: 0.8534, test acc: 0.8409\n",
      "Epoch 6, train loss 1.2245876280737837, test loss: 0.41525159709772486, train acc: 0.8578833333333333, test acc: 0.8442\n",
      "Epoch 7, train loss 1.1591968031198994, test loss: 0.36937206053430105, train acc: 0.8734666666666666, test acc: 0.8582\n",
      "Epoch 8, train loss 1.1009990716539722, test loss: 0.40119828549539965, train acc: 0.8629333333333333, test acc: 0.8455\n",
      "Epoch 9, train loss 1.0527776707845455, test loss: 0.3423683595885137, train acc: 0.8839333333333333, test acc: 0.8694\n",
      "Epoch 10, train loss 1.0106913162383444, test loss: 0.3779919750181733, train acc: 0.8766333333333334, test acc: 0.8579\n",
      "Epoch 11, train loss 0.9782892301646885, test loss: 0.3375747935122745, train acc: 0.88835, test acc: 0.871\n",
      "Epoch 12, train loss 0.9497903341105752, test loss: 0.32798818826295767, train acc: 0.89775, test acc: 0.8814\n",
      "Epoch 13, train loss 0.9223560837985102, test loss: 0.3163144225907174, train acc: 0.90465, test acc: 0.8837\n",
      "Epoch 14, train loss 0.9020378651585914, test loss: 0.28893935713608554, train acc: 0.9154833333333333, test acc: 0.8951\n",
      "Epoch 15, train loss 0.8808942757753421, test loss: 0.3753890568853184, train acc: 0.8801, test acc: 0.8553\n",
      "Epoch 16, train loss 0.8640752741014526, test loss: 0.28335717624160134, train acc: 0.9180666666666667, test acc: 0.8973\n",
      "Epoch 17, train loss 0.8498759984906549, test loss: 0.29544787531255917, train acc: 0.9170166666666667, test acc: 0.8947\n",
      "Epoch 18, train loss 0.8348001430728542, test loss: 0.29054505335297554, train acc: 0.9161833333333333, test acc: 0.8987\n",
      "Epoch 19, train loss 0.8199973398370783, test loss: 0.30969530400956513, train acc: 0.91575, test acc: 0.8921\n",
      "Epoch 20, train loss 0.8005340476788437, test loss: 0.29109747959360194, train acc: 0.9202166666666667, test acc: 0.8991\n",
      "Epoch 21, train loss 0.7871380288844932, test loss: 0.2919853677035897, train acc: 0.9181833333333334, test acc: 0.893\n",
      "Epoch 22, train loss 0.7747503870458745, test loss: 0.3185865647473912, train acc: 0.91155, test acc: 0.8827\n",
      "Epoch 23, train loss 0.7626174781114053, test loss: 0.30022166517509774, train acc: 0.9236166666666666, test acc: 0.8974\n",
      "Epoch 24, train loss 0.7540740329446569, test loss: 0.2674037060540193, train acc: 0.9339666666666666, test acc: 0.906\n",
      "Epoch 25, train loss 0.7436584812332826, test loss: 0.30226896750699184, train acc: 0.9203833333333333, test acc: 0.8883\n"
     ]
    }
   ],
   "source": [
    "train(model, criterion, train_loader, test_loader, lr, n_epochs, aux_loss_weight, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.8883\n"
     ]
    }
   ],
   "source": [
    "print(f'validation accuracy: {validate(model, test_loader, device)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
