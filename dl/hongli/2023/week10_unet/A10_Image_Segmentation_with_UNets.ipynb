{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c52529",
   "metadata": {
    "id": "a6c52529"
   },
   "source": [
    "HHU Deep Learning, WS2023/24, 20.20.2023\n",
    "\n",
    "Lecture: Prof. Dr. Markus Kollmann\n",
    "\n",
    "Exercises: Nikolas Adaloglou, Felix Michels\n",
    "\n",
    "# Assignment 10 - Cityscapes Image Segmentation with a U-Net\n",
    "\n",
    "---\n",
    "\n",
    "Submit the solved notebook (not a zip) with your full name plus assingment number for the filename as an indicator, e.g `max_mustermann_a1.ipynb` for assignment 1. If we feel like you have genuinely tried to solve the exercise, you will receive 1 point for this assignment, regardless of the quality of your solution.\n",
    "\n",
    "## <center> DUE FRIDAY 12.01.2024 2:30 pm </center>\n",
    "\n",
    "Drop-off link: [https://uni-duesseldorf.sciebo.de/s/1p7L30VTOH1Wkmy](https://uni-duesseldorf.sciebo.de/s/1p7L30VTOH1Wkmy)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a9222",
   "metadata": {
    "id": "6c3a9222"
   },
   "source": [
    "## Part I. Preparation\n",
    "\n",
    "For this exercise you need to install `torchmetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wm6miXIzZvAh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm6miXIzZvAh",
    "outputId": "c6a583c9-5819-4ee7-f81a-5b054e873b51"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0YT9fJmhzB9I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YT9fJmhzB9I",
    "outputId": "d89ce6a1-c011-4c65-a0c9-cf0bc7d36749"
   },
   "outputs": [],
   "source": [
    "!wget -c https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a10_unet/data.tar.gz\n",
    "!tar xf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028037a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a028037a",
    "outputId": "ee293275-91dc-495b-c857-e824532cd265"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import relu\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchmetrics.classification import MulticlassJaccardIndex as iou\n",
    "\n",
    "from utils import *\n",
    "from torchsummary import summary\n",
    "\n",
    "ignore_index=250\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "os.makedirs(\"./figs/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31260539",
   "metadata": {
    "id": "31260539"
   },
   "source": [
    "## Part II. U-Net Implementation\n",
    "\n",
    "The first part of this exercise is about implementing the U-Net architecture in a practical and flexible way. You will implement the U-Net via 3 classes:\n",
    "\n",
    "- `Conv2d`: An extension of the nn.Conv2d class with additional functionality for our U-Net.\n",
    "- `UNetBlock`: A block with different layers of fixed spatial resolution. This is the building block of our U-Net and uses the Conv2d class.\n",
    "- `UNet`: The complete model, using `UNetBlock` to piece together the architecture.\n",
    "\n",
    "Your final U-Net model should have the following features:\n",
    "\n",
    "- Variable kernel size\n",
    "- Variable model width (number of channels in each conv layer)\n",
    "- Variable model depth (number of feature resolution levels and number of `UNetBlock`s per level)\n",
    "- Optional dropout regularization\n",
    "\n",
    "Below is an illustration of a U-Net architecture for medical image segmentation with 5 resolution levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d625f",
   "metadata": {
    "id": "cb6d625f"
   },
   "source": [
    "![u-net-architecture.png](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a10_unet/figs/u-net-architecture.png)\n",
    "<center> <b> Figure: </b> Classical U-Net architecture from the original <a href=\"https://arxiv.org/abs/1505.04597\" title=\"paper\"> paper. </a>  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58781307",
   "metadata": {
    "id": "58781307"
   },
   "source": [
    "**Task:** Implement the `Conv2d` class below, which fullfills 3 functions:\n",
    "\n",
    "- **Regular case:** This class becomes a regular `nn.Conv2d` layer using its argument for `kernel`.\n",
    "- **Down-sampling case:** This class becomes a series of a `nn.MaxPool2d` with `kernel_size=2` and a `nn.Conv2d` layer using `kernel_size=3`.\n",
    "- **Up-sampling case:** This class becomes a series of an `nn.Upsample` layer with `scale_factor=2`, `mode='bilinear'` and `align_corners=True`, and a `nn.Conv2d` layer using `kernel_size=3`.\n",
    "\n",
    "**Optional** (See Part VIII for details)\n",
    "- If `pooling=False` use strided convlution instead of max pooling.\n",
    "- If `bilinear=False` use transposed convolution instead of bilinear upsamling.\n",
    "\n",
    "For all cases set padding such that the output height and width stay the same or change by factor of two (or one half).\n",
    "Also, use the appropriate number of `in_channels`, `out_channels` and pass the `bias` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb8170",
   "metadata": {
    "id": "16bb8170"
   },
   "outputs": [],
   "source": [
    "class Conv2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=0, bias=True,\n",
    "                    up=False, down=False, pooling=True, bilinear=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels:        Number of input channels for this block.\n",
    "            out_channels:       Number of output channels for this block.\n",
    "            kernel:             Kernel size for the first convolution.\n",
    "            bias:               Bias parameter for the convolution.\n",
    "            up/down:            Whether the first convolution is in up- or down-sampling mode.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert not (up and down), 'up and down cannot be both True'\n",
    "        assert not (kernel and (up or down)), 'Cannot use kernel with up/down sampling'\n",
    "        assert kernel or up or down, 'Must use kernel or up or down sampling'\n",
    "        ### START CODE HERE ### (approx. 10 lines)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9f433",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5de9f433",
    "outputId": "ea749b72-81a3-404c-e652-83af98fb6901"
   },
   "outputs": [],
   "source": [
    "def test_conv2d():\n",
    "    x = torch.zeros(16, 64, 100, 100)\n",
    "    convs = [Conv2d(64, 1, 3), Conv2d(64, 1, down=True), Conv2d(64, 1, down=True), Conv2d(64, 1, up=True), Conv2d(64, 1, up=True)]\n",
    "    for conv in convs:\n",
    "        print(f'Output shape: {conv(x).shape}.')\n",
    "\n",
    "test_conv2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VNHjt8yRkhPO",
   "metadata": {
    "id": "VNHjt8yRkhPO"
   },
   "source": [
    "### Expected result\n",
    "```\n",
    "Output shape: torch.Size([16, 1, 100, 100]).\n",
    "Output shape: torch.Size([16, 1, 50, 50]).\n",
    "Output shape: torch.Size([16, 1, 50, 50]).\n",
    "Output shape: torch.Size([16, 1, 200, 200]).\n",
    "Output shape: torch.Size([16, 1, 200, 200]).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f439b5",
   "metadata": {
    "id": "49f439b5"
   },
   "source": [
    "**Task:** Implement the `UNetBlock` class below. Make sure to read to docstring for more information on the arguemnts. One block has the following structure:\n",
    "\n",
    "1. Copy the input for a potential residual connection.\n",
    "2. BatchNorm2d -> ReLU activation -> Conv2d with potential down-/up-sampling.\n",
    "3. BatchNorm2d -> ReLU activation -> optional dropout -> Conv2d with kernel_size 3.\n",
    "4. Add the residual connection here, if the shape of x didn't change in the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3425e3f",
   "metadata": {
    "id": "e3425e3f"
   },
   "outputs": [],
   "source": [
    "class UNetBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=0, up=False, down=False,\n",
    "                         dropout=0, eps=1e-5, bilinear=True, pooling=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels:        Number of input channels for this block.\n",
    "            out_channels:       Number of output channels for this block.\n",
    "            kernel:             Kernel size for the first convolution.\n",
    "            up/down:            Whether the first convolution is in up- or down-sampling mode.\n",
    "            dropout:            Dropout probability for dropout before the second conv.\n",
    "            eps:                Epsilon parameter for BatchNorm2d.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ### (approx. 6 lines)\n",
    "        ### END CODE HERE ###\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43827c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f43827c",
    "outputId": "319840ad-10ac-4cc9-d438-9e1f9f2f7043"
   },
   "outputs": [],
   "source": [
    "def test_unet_block():\n",
    "    x = torch.zeros(16, 64, 100, 100)\n",
    "    blocks = [UNetBlock(64, 1, 3, dropout=0.1), UNetBlock(64, 1, down=True), UNetBlock(64, 1, down=True), UNetBlock(64, 1, up=True), UNetBlock(64, 1, up=True)]\n",
    "    for block in blocks:\n",
    "        print(f'Output shape: {block(x).shape}.')\n",
    "\n",
    "test_unet_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g_YNmxt4t9xR",
   "metadata": {
    "id": "g_YNmxt4t9xR"
   },
   "source": [
    "### Expected result\n",
    "```\n",
    "Output shape: torch.Size([16, 1, 100, 100]).\n",
    "Output shape: torch.Size([16, 1, 50, 50]).\n",
    "Output shape: torch.Size([16, 1, 50, 50]).\n",
    "Output shape: torch.Size([16, 1, 200, 200]).\n",
    "Output shape: torch.Size([16, 1, 200, 200]).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61213aa",
   "metadata": {
    "id": "b61213aa"
   },
   "source": [
    "**Task:** Now, armed with the `Conv2d` and `UNetBlock` classes, we can build the whole architecture. Use the illustration above the help you here. Tips and requirements for the implementation:\n",
    "\n",
    "- **Encoder-Decoder architecture:** Implement the encoder and decoder separately. Each of them loop through the resolution levels. The variable `channel_mult` specifies the number of levels, i.e. `[1, 2, 4]` would be 3 levels with channel multipliers 1, 2 and 4 respectively.\n",
    "- **Variable model width.** The `in_channels` and `out_channels` of every layer are a multiple of the `base_channels`. The multiple is specified in the `channel_mult` variable for each level. The only execeptions are the very first and very last layer (layer != level).\n",
    "- **Variable model depth.** This U-Net contains `len(channel_mult)` levels with `num_blocks` `UNetBlock`s each.\n",
    "- **At the beginning of every level**, there is either a down-sample or up-sample `UNetBlock`, with two exceptions. In the first level (level 0) of the encoder there is a `Conv2d` with kernel size `kernel` and no dropout instead. In the first level of the decoder (level len(channel_mult) - 1) there are two bottleneck layers, which are two `UNetBlock`s with kernel size `kernel`.\n",
    "- **At the very end of the model**, apply a `BatchNorm2d` layer, a ReLU activation and a `Conv2d` with kernel size 3 and no dropout.\n",
    "- **Skip connections:** Save the output of the last `UNetBlock` in each level in the encoder and concatenate it to the input of the first `UNetBlock` (though not the up-sample blocks) in each level in the decoder.\n",
    "\n",
    "Tips:\n",
    "\n",
    "- Check out `torch.nn.ModuleDict`. This lets you collect a number of named layers in the init and iterate through them in the forward, since this dictionary respects order of insertion. The fact that each layers gets a custom name here makes it easy to initialize them in a loop, using the indices of a loop over levels and a loop over `num_blocks` for the layer name. Additionally, you can do neat things like:\n",
    "\n",
    "```\n",
    "if 'block0' in name:  # First UNetBlock in each level\n",
    "    x = torch.cat([x, skips.pop()], dim=1)\n",
    "```\n",
    "\n",
    "in order to locate which blocks output/receive skip-connections.\n",
    "\n",
    "Make sure that your model works for different choices of `base_channels`, `channel_mult` and `num_blocks`. The only constraint here is that the maximum number of levels is 7, since our input height and width is 7 times divisible by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e022db",
   "metadata": {
    "id": "48e022db"
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, resolution, in_channels, out_channels, kernel_size=3, base_channels=32, channel_mult=[1,2,4], num_blocks=1, dropout=0.10,\n",
    "                 pooling=True, bilinear=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            resolution:     Image resolution at input/output.\n",
    "            in_channels:    Number of color channels at input.\n",
    "            out_channels:   Number of color channels at output.\n",
    "            kernel_size:    Convolution kernel size.\n",
    "            base_channels:  Base multiplier for the number of channels.\n",
    "            channel_mult:   Per-resolution multipliers for the number of channels.\n",
    "            num_blocks:     Number of residual blocks per resolution.\n",
    "            dropout:        Dropout probability of intermediate activations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ### (approx. 32 lines)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ### (approx. 10 lines)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e366da",
   "metadata": {
    "id": "d3e366da"
   },
   "source": [
    "Test your model's implementation below. If you used a ModuleDict, you can use the summary-function below to get a nice model summary with lots of useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b5bd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca1b5bd1",
    "outputId": "4771470a-d585-48e9-c978-fc698ad6ee23"
   },
   "outputs": [],
   "source": [
    "model = UNet(resolution=128, in_channels=3, out_channels=1, base_channels=32, channel_mult=[1,2,4], num_blocks=4)\n",
    "model.eval()\n",
    "summary(model, input_size=(3, 128, 64), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1c3ad",
   "metadata": {
    "id": "35d1c3ad"
   },
   "source": [
    "## Part III. Data Setup\n",
    "\n",
    "In this notebook, we will work with the Cityscapes dataset. It contains images with a 2:1 aspect ratio and corresponding segmentation maps, that classify the image content into different, pre-defined, classes pixel-wise.\n",
    "\n",
    "The orginal images are 1024x2048 resolution, which makes them difficult to process without expensive hardware. To remedy this, we preprocessed the data and downsampled both the images and segmentations maps to 64x128. When you load the data, images are already in the range $[-1,1]$. However, if you want to apply some intensity-based transformation, it is highly reccomended that you \"unnormalize\" the image back to [0,1] with `transforms.Normalize(-1, 2)` and then apply the transforms and then normalize/rescale back to $[0,1]$\n",
    "\n",
    "Below is a visualization of an image and its corresponding segmentation map. You can use the visualization methods of the class `CityscapesDownsampled` as illustrated in `test_data_loading`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f356d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "id": "090f356d",
    "outputId": "7fe808f5-0b6d-47ea-f821-a6d582885114"
   },
   "outputs": [],
   "source": [
    "transform_imgs = transforms.Compose([transforms.Normalize(-1, 2),\n",
    "                                    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "                                     transforms.Normalize(0.5,0.5),])\n",
    "\n",
    "train_dataset = CityscapesDownsampled(img_path=\"./resized_cityscapes/imgs_train.pth\",label_path=\"./resized_cityscapes/labels_train.pth\",\n",
    "                                      transform=transform_imgs)\n",
    "\n",
    "val_dataset = CityscapesDownsampled(img_path=\"./resized_cityscapes/imgs_val.pth\",label_path=\"./resized_cityscapes/labels_val.pth\")\n",
    "\n",
    "test_dataset = CityscapesDownsampled(img_path=\"./resized_cityscapes/imgs_test.pth\",label_path=\"./resized_cityscapes/labels_test.pth\")\n",
    "\n",
    "def test_data_loading(train_dataset):\n",
    "    img, seg = train_dataset[0]\n",
    "    _, seg2 = train_dataset[1]\n",
    "    print(img.shape, seg.shape)\n",
    "    assert img.shape[1:] == seg.shape\n",
    "    print(img.dtype, seg.dtype, img.min(), img.max(), seg.min(), seg.max())\n",
    "    # how to visualize seg maps\n",
    "    train_dataset.seg_show(seg)\n",
    "    plt.figure()\n",
    "    # how to visualize images\n",
    "    train_dataset.img_show(img, mean=torch.tensor([0.5]), std=torch.tensor([0.5]))\n",
    "    # how to visualize triples\n",
    "    # note: predicted would be the output of our unet\n",
    "    train_dataset.plot_triplet(img, seg, seg)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Train dataset size: \", len(train_dataset), \"Val dataset size: \", len(val_dataset), \"Test dataset size: \", len(test_dataset))\n",
    "test_data_loading(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226db4d1",
   "metadata": {
    "id": "226db4d1"
   },
   "source": [
    "## Part IV. Training and Validation\n",
    "\n",
    "With the data prepared and a model ready to be trained, all that is left is some training and validation code. As usual, we start with the validation function because we use it in the training function.\n",
    "\n",
    "**Task:** Implement the `validate` function below. It takes in the model and a validation data loader, computes the loss and an evaluation metric for all examples in the loader and returns the mean loss and metric results (mean per example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b60e1f",
   "metadata": {
    "id": "d3b60e1f"
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device, criterion, metric):\n",
    "    loss_step, metric_step = [], []\n",
    "    ### START CODE HERE ### (approx. 11 lines)\n",
    "    ### END CODE HERE ###\n",
    "    return val_loss_epoch, val_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846320f7",
   "metadata": {
    "id": "846320f7"
   },
   "source": [
    "**Task:** Next up is the `train_one_epoch` function which implements the training loop for one pass over the entire loader. Compute and return the mean loss and mean metric (mean per example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2ddfa",
   "metadata": {
    "id": "7de2ddfa"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, device, criterion, metric):\n",
    "    loss_step, metric_step = [], []\n",
    "    ### START CODE HERE ### (approx. 11 lines)\n",
    "    ### END CODE HERE ###\n",
    "    return loss_curr_epoch, train_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec663c4f",
   "metadata": {
    "id": "ec663c4f"
   },
   "source": [
    "For visualization during training, we want to plot an input image, the resulting prediction and corresponding target. Implement the function `show_preds`, so that it\n",
    "\n",
    "- Takes min(bs, num_samples) samples.\n",
    "- Converts the class scores into concrete predictions for all pixels.\n",
    "- Removes pixels in the prediction, which have the `ignore_index` in the target, i.e. set them to the `ignore_index` as well.\n",
    "\n",
    "\n",
    "\n",
    "#### What is the `ignore_index=250`?\n",
    "During the image annotation process, not all pixels are assigned one of the 19 classes. Therese are pixels belo to the `unlabeleed` class and have the annotation 250 in the segmentation maps. We only want to train with pixels that have a label from 0 to 19. The choice of the ignore_index value (here 250) is arbitary. Read the documentation of cross entropy understand how you can compute the loss while ignoring these pixel values.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fefd16",
   "metadata": {
    "id": "c6fefd16"
   },
   "outputs": [],
   "source": [
    "def show_preds(model, loader, device, ignore_index=250, num_samples=1):\n",
    "    ### START CODE HERE ### (approx. 11 lines)\n",
    "        ### END CODE HERE ###\n",
    "        loader.dataset.plot_triplet(img, seg, pred)  # Visualizes the three arguments in a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b41272",
   "metadata": {
    "id": "19b41272"
   },
   "source": [
    "Finally, we put it all together in the `train` function. We are saving the best model, by best metric performance, for later evaluation.\n",
    "\n",
    "**Task:**  Call the `train_one_epoch` and `validate` functions correctly to complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c192d4",
   "metadata": {
    "id": "b1c192d4"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs, train_loader, val_loader, device, criterion, metric, exp_name='unet', viz=False, viz_freq=20):\n",
    "    best_val_metric = -1\n",
    "    model = model.to(device)\n",
    "    dict_log = {\"train_metric\":[], \"val_metric\":[], \"train_loss\":[], \"val_loss\":[]}\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        ### START CODE HERE ### (approx. 11 lines)\n",
    "        ### END CODE HERE ###\n",
    "        msg = (f'Ep {epoch}/{num_epochs}: metric : Train:{train_metric:.3f} \\t Val:{val_metric:.2f}\\\n",
    "                || Loss: Train {train_loss:.3f} \\t Val {val_loss:.3f}')\n",
    "\n",
    "        pbar.set_description(msg)\n",
    "\n",
    "        dict_log[\"train_metric\"].append(train_metric)\n",
    "        dict_log[\"val_metric\"].append(val_metric)\n",
    "        dict_log[\"train_loss\"].append(train_loss)\n",
    "        dict_log[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_metric > best_val_metric:\n",
    "            best_val_metric = val_metric\n",
    "            torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': val_loss,\n",
    "                  'metric':val_metric,\n",
    "                  }, f'{exp_name}_best_model_min_val_loss.pth')\n",
    "        if viz and (epoch+1) % viz_freq==0:\n",
    "            show_preds(model, train_loader, device, num_samples=1)\n",
    "\n",
    "    return dict_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c66c5",
   "metadata": {
    "id": "803c66c5"
   },
   "source": [
    "## Part V. Training Preparations: Model testing and design\n",
    "\n",
    "### Test the model on a small subset of the data\n",
    "\n",
    "- Define a U-Net. Model parameters are irrelevant at this point.\n",
    "- Use `utils.CityscapesSubset` defined in utils.py to make a subset containing the first 16 training and 16 validation samples.\n",
    "- Define the dataloaders (batch_size 16).\n",
    "- Perform a forward pass on the train subset dataloader.\n",
    "- Visualize the triple (img, seg, prediction) of 3 samples in the batch (using `show_preds`)\n",
    "- Compute the mean intesection over union (mIoU), and the cross entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c77a2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "80c77a2d",
    "outputId": "8f8f7ce8-0094-475e-aa80-847a1e27bb0a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### START CODE HERE ### (approx. 16-17 lines)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e18eda",
   "metadata": {
    "id": "15e18eda"
   },
   "source": [
    "**Expected results**\n",
    "\n",
    "Since we rely on random init, results may vary here.\n",
    "You can cross-check the outcome of cross entropy, for instance, for a random guess in an N-class classification problem.\n",
    "```\n",
    "mIoU: 1.2005233205854893\n",
    "Cross-entropy loss 3.0224854946136475\n",
    "```\n",
    "![im2](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a10_unet/figs/exp_out_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325eeea6",
   "metadata": {
    "id": "325eeea6"
   },
   "source": [
    "# Part VI: Model design: Overfitting a small subset of data - design the basic structure of the UNet model\n",
    "\n",
    "In this part, we will design the model's parameters, by overfitting a small subset of the data.\n",
    "\n",
    "Assumption: A model that is able to score better performance metrics in the tiny subset would be better in the full dataset.\n",
    "This assumption is used all across the academia and industry.\n",
    "\n",
    "In this, way you can iterate over training the model multiple times.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Choose the U-Net parameters and initialise a model.\n",
    "- Train on the 16-examples subset for 200 epochs, with an optimizer of your choice.\n",
    "- The training needs the CrossEntropy-Loss, for comparison between models you can use a metric of your choice.\n",
    "- Tune the parameters to achieve better results.\n",
    "- If you can, use the `summary` function here to keep track of your models design.\n",
    "- Save the best model setup in a dictionary `best_setup`, where a parameter's name is the key.\n",
    "\n",
    "Remember that the goal here is to overfit! You won't be able to prevent overfitting, since we have such few training examples, so don't worry if the validation accuracy stays very bad.\n",
    "\n",
    "Normally, this step should strictly require less than 15 minutes. **Warning**: use the subsets of the train and val data!\n",
    "\n",
    "Take the UNet with the best setup in terms of `base_channels`, `channel_mult`, `num_blocks`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note: Use `plot_stats(dict_log, title=\"./figs/foo\", scale_metric=100, baseline=45)` to plot the results. During Overfitting a small subset of data the baseline mIoU is on the traindata (70%) while during the full experiment with the whole dataset the basline is the validation mIoU of 45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce278a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a7ce278a",
    "outputId": "a369aef7-72e7-4c65-e27d-33f93009be1c"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 11 lines)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ef8a3",
   "metadata": {
    "id": "6f8ef8a3"
   },
   "source": [
    "**Expected results**\n",
    "\n",
    "![im6](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a10_unet/figs/subset_overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcfe55",
   "metadata": {
    "id": "e3bcfe55"
   },
   "source": [
    "# Part VII: Train the UNet with the CE loss\n",
    "\n",
    "- Initialise a model with your best hyperparameter setup from before. You can use your dictionary as keyword arguments here with the `**` syntax.\n",
    "- Use CrossEntropy as your loss function.\n",
    "- Use the full dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272560dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "272560dd",
    "outputId": "a28154f9-63c1-4c65-a8a3-a23e5e05d581"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 5 lines)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8824d4a",
   "metadata": {
    "id": "f8824d4a"
   },
   "source": [
    "### Notes:\n",
    "\n",
    "\n",
    "- Time to execute for our unet: ~30mins\n",
    "- VRAM: 11GB for batch size 32\n",
    "\n",
    "Feel free to train for more epochs if you have the time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47947d33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "47947d33",
    "outputId": "88472e5c-adb0-4412-d3a0-1fbb766f4460"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 3 lines)\n",
    "### END CODE HERE ###\n",
    "dict_log = train(model, optimizer, 25, train_loader, val_loader, device, criterion, metric, exp_name='UNet-ce-pool')\n",
    "plot_stats(dict_log, modelname=\"CE\", title=\"./figs/unet_base\", scale_metric=100, baseline=45)\n",
    "show_preds(model, train_loader, device, num_samples=1)\n",
    "show_preds(model, val_loader, device, num_samples=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226e7a9",
   "metadata": {
    "id": "3226e7a9"
   },
   "source": [
    "### Expected result\n",
    "\n",
    "![im7](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a10_unet/figs/unet_pool_upsampling_ce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070f2c9",
   "metadata": {
    "id": "5070f2c9"
   },
   "source": [
    "# Part VIII (OPTIONAL). Train the U-Net with strided convolutions (down) and transpose convolutions (up)\n",
    "\n",
    "#### Downsampling:\n",
    "Strided convolutions can be used as an alternative of pooling + conv.    \n",
    "\n",
    "#### Upsampling:\n",
    "Interpolation VS transpose convolution. As stated in [FCN](https://arxiv.org/pdf/1411.4038.pdf) :  \n",
    "Simple bilinear interpolation computes each output pixel from the nearest four inputs by a linear map that depends only on the relative positions of the input and output cells. In a sense, upsampling with factor f is convolution with a fractional input stride of 1/f . So long as f is integral, a\n",
    "natural way to upsample is therefore backwards convolution (sometimes called deconvolution or transpose convolution) with an output stride of f . Such an operation is trivial to implement, since it simply reverses the forward and backward passes of convolution.\n",
    "\n",
    "---\n",
    "\n",
    "The originally proposed [Unet](https://arxiv.org/abs/1505.04597) used max pooling and bilinear upsampling. What works best depends on the application. In image generation, for instance, we often use strided convolutions for downsampling and transpose convolutions for upsampling.\n",
    "\n",
    "\n",
    "**Task:** Modify the `Conv2d` class, so that it supports transposed convolutions for up-sampling and an `nn.Conv2d` with `stride` 2 and `kernel_size` 2 (strided convolution) for downsampling. Then train the U-Net with those two options enabled. The finished `Conv2d` class should fulfill 3 cases as follows:\n",
    "\n",
    "\n",
    "- **Regular case:** This class becomes a regular `nn.Conv2d` layer using its argument for `kernel`. Think about the appropriate amount of `padding` here.\n",
    "- **Down-sampling case:** There are two options for down-sampling, depending on the `pooling` argument:\n",
    "    - `pooling=True`: This class becomes a series of a `nn.MaxPool2d` layer using `kernel_size=2` and a `nn.Conv2d`.\n",
    "    - `pooling=False`:  This class becomes a `nn.Conv2d` layer using `stride=2`.\n",
    "- **Up-sampling case:** There are two options for up-sampling, depending on the `bilinear` argument:\n",
    "    - `bilinear=True`: This class becomse a series of a `nn.Upsample` layer with `scale_factor=2`, `mode='bilinear'` and `align_corners=True`, and a `nn.Conv2d` layer using `kernel_size=3` and `padding=1`.\n",
    "    - `bilinear=False`: This class becomes a `nn.ConvTransposed2d` layer with `kernel_size=2` and `stride=2`.\n",
    "\n",
    "\n",
    "\n",
    "### Additional resource UNets, convolution arithmetic and image segmentation can be found in the links below:     \n",
    "\n",
    "\n",
    "- [In CNN, are upsampling and transpose convolution the same?](https://stats.stackexchange.com/questions/252810/in-cnn-are-upsampling-and-transpose-convolution-the-same)\n",
    "- [STRIVING FOR SIMPLICITY:\n",
    "THE ALL CONVOLUTIONAL NET](https://arxiv.org/pdf/1412.6806.pdf)\n",
    "- [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n",
    "- [An overview of Unet architectures for semantic segmentation and biomedical image segmentation](https://theaisummer.com/unet-architectures/)\n",
    "- [Introduction to Strided Convolutions](https://www.geeksforgeeks.org/ml-introduction-to-strided-convolutions/)\n",
    "- [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f684f",
   "metadata": {
    "id": "370f684f"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 6 lines)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfe95b",
   "metadata": {
    "id": "d5cfe95b"
   },
   "source": [
    "**Expected results**\n",
    "\n",
    "![im8](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a10_unet/figs/UNet-strided-transp-conv.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
