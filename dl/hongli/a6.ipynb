{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r6TJNtyN32l"
   },
   "source": [
    "HHU Deep Learning, WS2023/24, 17.11.2023\n",
    "\n",
    "Lecture: Prof. Dr. Markus Kollmann\n",
    "\n",
    "Exercises: Nikolas Adaloglou, Felix Michels\n",
    "\n",
    "# Assignment 6 - Neural Networks with PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "Submit the solved notebook (not a zip) with your full name plus assingment number for the filename as an indicator, e.g `max_mustermann_a1.ipynb` for assignment 1. If we feel like you have genuinely tried to solve the exercise, you will receive 1 point for this assignment, regardless of the quality of your solution.\n",
    "\n",
    "## <center> DUE FRIDAY 24.11.2023 2:30 pm </center>\n",
    "\n",
    "Drop-off link: [https://uni-duesseldorf.sciebo.de/s/1OQhEkR4QtlzkfF](https://uni-duesseldorf.sciebo.de/s/1OQhEkR4QtlzkfF)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3eZ4iXJIvzj"
   },
   "source": [
    "Until now, we have used the numpy package to write our own helper functions for forward-, backward-passes of layers, activation functions, wrote our own cost functions and optimizers, etc. Now, we want to automate all of this with the use of a Deep Learning Framework.\n",
    "\n",
    "There are various big packages (frameworks) that offer functionality specialized in Machine/Deep Learning.\n",
    "\n",
    "The two most well known ones are **PyTorch** from Meta and **TensorFlow** from Google. In this course, we will work with PyTorch as it provides a good tradeoff between ease of use for academic purposes and scalability to large projects in future reasearch.\n",
    "\n",
    "Next to implementations of functions like Neural Network layers, optimizers, etc., PyTorch has three main features:\n",
    "- The tensor datastructure, which is similar to the Numpy ndarray\n",
    "- Automatic differentiation for training neural networks\n",
    "- It allows us to run code on a GPU, which offers significantly  faster computations.\n",
    "\n",
    "![Pytorch_logo.png](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a06_pytorch_mlp/images/Pytorch_logo.png)\n",
    "\n",
    "Feel free to use the PyTorch Introduction from week 1 as a look-up for basic PyTorch functionality.\n",
    "\n",
    "---\n",
    "\n",
    "For this assignment, you will use PyTorch to implement a fully connected Neural Network, perform a set of experiments on the famous CIFAR10 dataset and report your results and findings.\n",
    "\n",
    "For your experiments, you will create a multilayer perceptron (MLP) Neural Network and train it on the CIFAR10 dataset to predict object class based on the input image. CIFAR10 image dimensions are 32 by 32, with 3 values for each pixel (R, G, B).\n",
    "\n",
    "**Goal: Your goal is to develop an image classifier that scores more than 50% accuracy on the validation data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAVSy4n6sPdR"
   },
   "source": [
    "# 1. Imports and preparing the dataset and dataloaders\n",
    "\n",
    "\n",
    "If you are using google colab remember to change the runtime type to GPU.\n",
    "\n",
    "To get started, run the following code to import the libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4684,
     "status": "ok",
     "timestamp": 1700314196447,
     "user": {
      "displayName": "Vinicius Sfredo",
      "userId": "09460668311934584348"
     },
     "user_tz": -60
    },
    "id": "NH-HBP0nuT30"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86Vr9megN32n"
   },
   "source": [
    "We will be using a GPU to accelerate training and testing. PyTorch lets you use the GPU with very little additional code. The `torch.cuda` package contains all available functions for GPU usage. Check out the docs on the [package](https://pytorch.org/docs/stable/cuda.html) and on [cuda semantics](https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics) if you want to learn more, this is not required for this assignment though.\n",
    "\n",
    "If you don't have a dedicated GPU in your own machine, the easiest way to get one is Google Colab! It's free, all you need is a Google account and you get ~8 hrs of GPU time per day.\n",
    "\n",
    "Below is a small, good-practice, GPU setup to check the availability of a GPU in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoVzxCrMN32o"
   },
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raYoo0yztIck"
   },
   "source": [
    "# 2. CIFAR-10 Dataset\n",
    "\n",
    "Today, we'll use the famous CIFAR-10 dataset for an image classification task. PyTorch provides many built-in datasets in the [torchvision.datasets](https://pytorch.org/vision/stable/datasets.html) module, as well as utility classes for building your own datasets.\n",
    "\n",
    "The code below:\n",
    "- Defines some basic transforms for preprocessing the data\n",
    "- Loads the CIFAR-10 dataset as a PyTorch dataset class. The raw data is stored locally on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jbFfa8ON32p"
   },
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRLd0p0gN32q"
   },
   "source": [
    "PyTorch provides a convenient solution for batch training. After loading the dataset and setting parameters like batch_size, train and an optional transformation (normalization in this case), we can use the DataLoader object as an iterator for the training loop. It will return the outputs of the `__getitem__(...)` function of the dataset class in a batched form, ready to be used for batch-gradient descent.\n",
    "\n",
    "**Exercise:** Instantiate two DataLoaders, one for the training set and one for the validation set. Adjust the batch size to your available GPU memory, usual choices are powers of 2, like 16, 32, 64, ..., for reasons that have to do with the hardware layout on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHPddxfvN32q"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 2 lines)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVDu_LAnN32r"
   },
   "source": [
    "Below you can see how to access information regarding the dataset and the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdaGflBhtIjs"
   },
   "outputs": [],
   "source": [
    "# dataset instance\n",
    "print(\"List of label names are:\", trainset.classes)\n",
    "print(\"Total training images:\", len(trainset))\n",
    "img, label = trainset[0]\n",
    "print(f\"Example image with shape {img.shape}, label {label}, which is a {trainset.classes[label]} \")\n",
    "\n",
    "# DataLoader instance\n",
    "print(f'The dataloader contains {len(train_loader)} batches.')\n",
    "imgs_batch , labels_batch = next(iter(train_loader))\n",
    "print(f\"A batch of images has shape {imgs_batch.shape}, labels {labels_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slXQK5mutP1g"
   },
   "source": [
    "# 3. Define the MLP model\n",
    "\n",
    "Now it's your turn to implement a Multi-Layer Perceptron (MLP / fully connected net) with variable depth, similar to the last exercise. Your Network needs to:\n",
    "\n",
    "- Have L-1 blocks of linear layers, followed by ReLU activations, check out nn.Linear and nn.ReLU\n",
    "- Have a final 'out_layer' with just a linear layer\n",
    "- Have choosable dimensions for all hidden layers, via the 'hidden_sizes' parameter, e.g. [32, 16] is a three-layer model.\n",
    "\n",
    "The `torch.nn` functions, like `nn.Linear`, have all the functionality you need for training the network already built into them. No more manual backward-computation or remembering intermediate results in a cache, PyTorch takes care of that.\n",
    "\n",
    "You need to define your network architecture in the `__init__`, by initialising things like layers and activation functions and then call them in the forward pass to process the input `x`.\n",
    "\n",
    "Tip: In the `__init__`, you can save 'nn.' objects in a list and then pass them to `nn.Sequential` as an unpacked list:\n",
    "\n",
    "```\n",
    "layers = nn.ModuleList()\n",
    "... fill the list ...\n",
    "self.layers = nn.Sequential(*layers)\n",
    "```\n",
    "\n",
    "See the `PyTorch Module API` and `PyTorch Sequential API` sections from the PyTorch Introduction from week 1 for help on the implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWozHQwWvoe5"
   },
   "outputs": [],
   "source": [
    "# Model structure\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, hidden_sizes=[64]):\n",
    "        super(MLP, self).__init__()\n",
    "        assert len(hidden_sizes) >= 1 , \"specify at least one hidden layer\"\n",
    "        ### START CODE HERE ### (approx. 7 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84Vl_xzzN32s"
   },
   "source": [
    "Use the function below to test your implementation. The parameter settings are not fitted to CIFAR-10, you will need different values later on!\n",
    "\n",
    "Below you also find an expected output of what the network structure should look like. Note that this print varies based on your syntax, even if the logic is correct. Use it as a guide, you don't need to exactly replicate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nC6QucXN32s"
   },
   "outputs": [],
   "source": [
    "# Test your implementation with random data\n",
    "test_model = MLP(in_channels=3*10*10, num_classes=4, hidden_sizes=[64, 32, 16]).to(device)\n",
    "\n",
    "a = torch.rand(16, 3, 10, 10).to(device)\n",
    "b = test_model(a)\n",
    "assert b.shape == (16,4)\n",
    "print('Shape test passed')\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCZ5oUINN32s"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Shape test passed\n",
    "Net(\n",
    "  (layers): Sequential(\n",
    "    (0): Linear(in_features=300, out_features=64, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
    "    (5): ReLU()\n",
    "  )\n",
    "  (out_layer): Linear(in_features=16, out_features=4, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfvCTJGZQQQl"
   },
   "source": [
    "# 4. Training and Validation\n",
    "\n",
    "Equipped with our MLP model, we now want to train it.\n",
    "\n",
    "First, we implement the validation function, so we can use it in the train function. The idea is to test our model on the (unseen) validation data in regular intervals during training, e.g. after every epoch (traversal of the whole dataset).\n",
    "\n",
    "**Exercise:** Implement the validation function, which reads in data from the `val_loader`, runs a forward pass and records the classification accuracy, as well as the loss. Both are returned for plotting purposes.\n",
    "\n",
    "We will be using the `nn.CrossEntropyLoss` as our loss function. Please have a look at the [docs](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for this loss function, you will use it more often in the future.\n",
    "\n",
    "Loss functions in PyTorch are instantiated as class objects (nn.CrossEntropyLoss is the class here) and can then be used like a function:\n",
    "\n",
    "```\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(input,target)\n",
    "```\n",
    "\n",
    "Note, that `nn.CrossEntropyLoss` combines a Softmax activation with the actual loss function, a negative log-likelihood loss. So, it expects **unnormalized!** scores from our network, i.e. no activation funciton after the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83XRLdRMN32t"
   },
   "outputs": [],
   "source": [
    "def validation(model, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            ### START CODE HERE ### (approx. 8 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4q_ak_B5N32t"
   },
   "outputs": [],
   "source": [
    "# Don't mind this, is just for testing\n",
    "class dummy_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.rand(16, 3, 10, 10)\n",
    "        self.labels = torch.randint(0, 4, (16,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYQ4xvo-N32t"
   },
   "source": [
    "Use the code below to test your implementation with the above initialized `test_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYcgaNLeN32t",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test validation with an untrained model\n",
    "test_set = dummy_dataset()\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4)\n",
    "\n",
    "# Needs to run without errors\n",
    "validation(test_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g05oRq04N32u"
   },
   "source": [
    "Now, we can implement the train loop itself.  You are already familiar with the basic training procedure:\n",
    "\n",
    "Loop for num_iterations:\n",
    "1. Forward propagation\n",
    "2. Compute loss function\n",
    "3. Backward propagation\n",
    "4. Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ugi-Di6YvtA_"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, lr, momentum, wd):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_acc_hist = []\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            ### START CODE HERE ### (approx. 7 lines)\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "        # stats\n",
    "        val_acc, val_loss = validation(model, val_loader)\n",
    "        val_acc_hist.append(val_acc)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} done. Train loss: {np.mean(train_loss_hist[-len(train_loader):]):.3f} val loss: {val_loss_hist[-1]:.3f} Val. Accuracy : {val_acc_hist[-1]*100:.3f}%.\")\n",
    "\n",
    "    dict_log = {\"train_loss_hist\": train_loss_hist, \"val_loss_hist\": val_loss_hist, \"val_acc_hist\": val_acc_hist}\n",
    "    return dict_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLh-SoRuWRpV"
   },
   "source": [
    "**Exercise:** Choose a single set of hyperparameters (lr, momentum, wd, epochs), as well as a model configuration and see how the model performs. Save the dict_log for plotting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPC_Jy9IRO_Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 4 lines)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "model = MLP(in_channels, num_classes, hidden_sizes).to(device)\n",
    "\n",
    "dict_log = train(model, train_loader, val_loader, epochs, lr, momentum, wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XfmCf1ZWPP5"
   },
   "source": [
    "# 6. Plotting results\n",
    "\n",
    "The code below plots the results of the training. We compare train and validation loss in the same plot to check for overfitting or other undesirable training behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egbgcnvnN32v"
   },
   "outputs": [],
   "source": [
    "def loss_plot(title):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cross Entropy Loss\")\n",
    "\n",
    "def acc_plot(title):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(title)\n",
    "    plt.axhline(y = 0.5, color = 'r', linestyle = '-', label='Good accuracy')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TklHSBTvWLZM"
   },
   "outputs": [],
   "source": [
    "x_val = np.arange(1, epochs+1)\n",
    "train_loss_hist_epochs = [np.mean(dict_log[\"train_loss_hist\"][i*len(train_loader):(i+1)*len(train_loader)]) for i in range(epochs)]\n",
    "\n",
    "loss_plot('Losses per Epoch')\n",
    "plt.scatter(x_val, dict_log[\"val_loss_hist\"], label='Validation loss')\n",
    "plt.scatter(x_val, train_loss_hist_epochs, label='Train loss')\n",
    "plt.legend()\n",
    "plt.savefig('Losses1.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "acc_plot('Validation Accuracy per epoch')\n",
    "plt.scatter(x_val, dict_log[\"val_acc_hist\"])\n",
    "plt.plot(x_val, dict_log[\"val_acc_hist\"], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('Accuracy1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZA9WJsXWbyk"
   },
   "source": [
    "**Expected Output:** Your plots should look somewhat similar to this:\n",
    "\n",
    "![Losses1.png](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a06_pytorch_mlp/images/Losses1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAHgD0arN32v"
   },
   "source": [
    "![Accuracy1.png](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a06_pytorch_mlp/images/Accuracy1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS5L1J6QWMOA"
   },
   "source": [
    "# 7. Hyperparameter tuning\n",
    "\n",
    "Lastly, we want to squeeze some more performance out of our model by tuning its hyperparameters.\n",
    "\n",
    "**Exercise:** Implement the function hparam_tuning below. It should:\n",
    "- Take in lists of hyperparameters\n",
    "- Test every possible combination of them.\n",
    "\n",
    "The epochs, in_channels and num_classes are fixed. You can experiment with the parameters hidden sizes, learning rate, momentum and weight decay.\n",
    "\n",
    "Save the dictionaries returned by the train function in the logs_out list. Don't forget to re-initialize your model before each run with new hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuoj0ejKN32w"
   },
   "outputs": [],
   "source": [
    "def hparam_tuning(epochs, in_channels, hidden_sizes, lrs, momentums, wds, num_classes=10):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    epochs: int\n",
    "    in_channels: int\n",
    "    num_classes: int\n",
    "    hidden_sizes: list of lists of ints\n",
    "    lrs/momentums/wds: list of floats\n",
    "    \"\"\"\n",
    "    logs_out = []\n",
    "    ### START CODE HERE ### (approx. 7 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return logs_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfTGuReAN32w"
   },
   "source": [
    "**Exercise:** Pick **at least 4 combinations of hyperparameters** and use them for a comparative series of training runs. Since you pass lists of hyperparameters, this means two of the lists need to be at least of length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MVmxfXcN32w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (approx. 4 lines)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "logs_out = hparam_tuning(epochs, in_channels, hidden_sizes, lrs, momentums, wds)\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "acc_plot(\"Validation accuracy for different hyperparameter sets\")\n",
    "for c, dict_out in enumerate(logs_out):\n",
    "    x_val = list(range(len(dict_out[\"val_acc_hist\"])))\n",
    "    plt.scatter(x_val, dict_out[\"val_acc_hist\"])\n",
    "    plt.plot(x_val, dict_out[\"val_acc_hist\"], label=f\"HP set {c+1}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--U4OoiRqCgf"
   },
   "source": [
    "**Expected Output:** Your plots should look somewhat similar to this:\n",
    "\n",
    "![Accuracy2.png](https://github.com/HHU-MMBS/Deep-Learning-Exercise-Extras/raw/main/a06_pytorch_mlp/images/Accuracy2.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
