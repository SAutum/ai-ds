{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HHU Deep Learning, WS2023/24, 13.10.2023\n",
    "\n",
    "Lecture: Prof. Dr. Markus Kollmann\n",
    "\n",
    "Exercises: Felix Michels, Nikolas Adaloglou\n",
    "\n",
    "# PyTorch Introduction\n",
    "\n",
    "by Tim Kaiser\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "Take a look at https://pytorch.org/get-started/locally/ to install PyTorch. Set your preferences in the \"Start Locally\" section and run the command.\n",
    "\n",
    "I recommend using Python 3.11 and PyTorch >= 2.0. You don't have to install CUDA seperatly! The PyTorch installation installs the cudatoolkit package.\n",
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is a Deep Learning library that has three main features: The tensor datastructure, which is similar to\n",
    "the Numpy ndarray, automatic differentiation for training neural networks and it allows us to run code on a GPU, which offers significantly faster computations. \n",
    "\n",
    "We will now take a look at part of the [PyTorch Tutorial](https://github.com/tuelwer/pytorch-tutorial) by Tobias Uelwer to get familiar with fundamentals like Tensors, automatic differentiation and function optimization in PyTorch:\n",
    "\n",
    "Use this as an introduction, as well as a lookup for later exercises. \n",
    "\n",
    "---\n",
    "\n",
    "## PyTorch Basics\n",
    "\n",
    "Author: Tobias Uelwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a new PyTorch tensor from a NumPy tensor\n",
    "torch.from_numpy(np.ones((2, 2)))\n",
    "# We can instantanly access its values unlike we are used to from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9912,  0.7258, -0.1109, -0.9342,  1.2320],\n",
       "        [ 2.5265, -1.3166,  1.6025, -0.2199,  1.1588],\n",
       "        [ 0.8623, -0.5271,  0.1942,  1.4443, -2.5663],\n",
       "        [-1.5697,  0.0790, -0.4856,  1.2007,  0.4859]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a random tensor\n",
    "A = torch.randn(4, 5)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9912)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing\n",
    "A[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1109,  1.6025,  0.1942, -0.4856])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More indexing\n",
    "A[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of A\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.1028)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traces and matrix transpose\n",
    "torch.trace(A.T @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2899,  0.1590, -1.5151,  0.7761,  1.1797]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random standard normal samples\n",
    "B = torch.randn(1, 5)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2874,  0.1154,  0.1680, -0.7251,  1.4534],\n",
       "        [-0.7325, -0.2093, -2.4280, -0.1707,  1.3670],\n",
       "        [-0.2500, -0.0838, -0.2942,  1.1210, -3.0274],\n",
       "        [ 0.4551,  0.0126,  0.7358,  0.9319,  0.5733]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcasting\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, float)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function item() returns a simple Python scalar\n",
    "type(torch.sum(A**2)), type(torch.sum(A**2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9912,  0.7258, -0.1109, -0.9342,  1.2320,  2.5265, -1.3166,\n",
       "           1.6025, -0.2199,  1.1588],\n",
       "         [ 0.8623, -0.5271,  0.1942,  1.4443, -2.5663, -1.5697,  0.0790,\n",
       "          -0.4856,  1.2007,  0.4859]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping\n",
    "A.view(-1, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 9, 1]), torch.Size([2, 9]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flattens a tensor to a vector (removes 'empty' dimensions)\n",
    "Ones = torch.ones((2, 1, 9, 1))\n",
    "Ones.shape, Ones.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]), torch.Size([9, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add dimensions\n",
    "torch.ones(9).unsqueeze(0).shape, torch.ones(9).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 5, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new dimensions (this also works with numpy arrays...)\n",
    "A[None, :, None, :, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.8251e-01, 5.2685e-01, 1.2296e-02, 8.7276e-01, 1.5179e+00],\n",
       "        [6.3832e+00, 1.7333e+00, 2.5679e+00, 4.8351e-02, 1.3428e+00],\n",
       "        [7.4354e-01, 2.7785e-01, 3.7713e-02, 2.0861e+00, 6.5858e+00],\n",
       "        [2.4639e+00, 6.2341e-03, 2.3583e-01, 1.4417e+00, 2.3614e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elementwise power\n",
    "A**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0088, -0.3204, -2.1992, -0.0680,  0.2087],\n",
       "        [ 0.9268,  0.2750,  0.4716, -1.5146,  0.1474],\n",
       "        [-0.1482, -0.6403, -1.6389,  0.3676,  0.9425],\n",
       "        [ 0.4509, -2.5389, -0.7223,  0.1829, -0.7217]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logarithm and absolute value\n",
    "torch.log(torch.abs(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All close\n",
    "torch.allclose(torch.log(torch.exp(A)), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [False,  True, False,  True,  True],\n",
       "        [ True, False,  True,  True, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.exp().log() == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: False\n",
      "tensor([[-0.9912,  0.7258, -0.1109, -0.9342,  1.2320],\n",
      "        [ 2.5265, -1.3166,  1.6025, -0.2199,  1.1588],\n",
      "        [ 0.8623, -0.5271,  0.1942,  1.4443, -2.5663],\n",
      "        [-1.5697,  0.0790, -0.4856,  1.2007,  0.4859]])\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(f\"Using GPU: {cuda}\")\n",
    "\n",
    "if cuda:\n",
    "    A = A.cuda()\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to calculate gradients we need to specify this\n",
    "# by setting requires_grad=True for each tensor\n",
    "# X = torch.ones(2, 2, requires_grad=True)\n",
    "X = torch.tensor([[1,1],[2,2]], dtype=torch.float32, requires_grad=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.2147, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We build a simple computational graph \n",
    "y = torch.sum(torch.exp(X))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 2.7183],\n",
       "        [7.3891, 7.3891]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In pytorch gradients are accumulated each time backward() is called.\n",
    "# Keep that in mind!\n",
    "y.backward(retain_graph=True) # don't free the graph buffers after backprop\n",
    "X.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 2.7183],\n",
       "        [7.3891, 7.3891]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To suppress this behavior we have to reset the gradients by\n",
    "X.grad.zero_()\n",
    "y.backward(retain_graph=True) # dont free the graph buffers after backprop\n",
    "X.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define variable\n",
    "x = torch.tensor([2., 2.], requires_grad=True).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the function $f(x_1, x_2) = 100(x_2-x_1^2)^2 + (1-x_1)^2$.\n",
    "\n",
    "The function has a minimum at $(1, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define objective\n",
    "f = lambda x: 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate optimizer\n",
    "optimizer = torch.optim.Adam([x])  # Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 15729 iterations.\n",
      "Minimum: [1.0009778 1.0019593]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    f_val = f(x)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    f_val.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if torch.norm(x.grad) < 0.001:\n",
    "        print(f\"Converged after {i} iterations.\")\n",
    "        print(f\"Minimum: {str(x.detach().numpy())}\")\n",
    "        break\n",
    "    if i > 100000:\n",
    "        print(\"Maximum number of iterations reached.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementing Neural Networks\n",
    "\n",
    "After familiarizing ourselves with the basics of Tensors and optimization in PyTorch, we want to build and train a \n",
    "Convolutional Network.\n",
    "\n",
    "There are three ways to implement a Neural Network in PyTorch:\n",
    " \n",
    "1. Barebones PyTorch: work directly with the lowest-level PyTorch Tensors. \n",
    "2. PyTorch Module API: use `nn.Module` to define arbitrary neural network architecture. \n",
    "3. PyTorch Sequential API: use `nn.Sequential` to define a linear feed-forward network very conveniently. \n",
    "\n",
    "Here is a table of comparison:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `nn.Module`     | High        | Medium      |\n",
    "| `nn.Sequential` | Low         | High        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/dxwg46tj107f_g5l18ynf3y80000gs/T/ipykernel_40824/1843555130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Barebones PyTorch\n",
    "\n",
    "We will start with the barebone implementation of a Two-Layer Fully Connected Network. This helps explore the overall\n",
    "code structure, the autograd engine and PyTorch's training conventions. \n",
    "\n",
    "First, we will define the network by defining its forward pass. Then, we have to initialize the weights for the entire\n",
    "model and finally define a training loop to train it. \n",
    "\n",
    "We test our forward pass by running it on a batch of tensors filled with zeros. This is to make sure that we don't get\n",
    "any errors and the network produces an output of the right shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "def two_layer_fc(x, params):\n",
    "    n = x.shape[0]\n",
    "    # first we flatten the image\n",
    "    x = x.view(n, -1)  # shape: [batch_size, C x H x W]\n",
    "    \n",
    "    w1, w2 = params\n",
    "    x = F.relu(x @ w1)\n",
    "    x = x @ w2\n",
    "    return x\n",
    "    \n",
    "def test_two_layer_fc():\n",
    "    hidden_layer_size = 42\n",
    "    x = torch.zeros((64, 50), dtype=torch.float32)  # minibatch size 64, feature dimension 50\n",
    "    w1 = torch.zeros((50, hidden_layer_size), dtype=torch.float32)\n",
    "    w2 = torch.zeros((hidden_layer_size, 10), dtype=torch.float32)\n",
    "    scores = two_layer_fc(x, [w1, w2])\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "\n",
    "test_two_layer_fc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the barebone approach, we need to manually initialize the network's weights. In order to make the parameters \n",
    "trainable, we have to set `requires_grad` to True. However, when modifying a tensor in any way (even moving them\n",
    "to the GPU) will remove the `requires_grad` attribute, so make sure it is the last thing you do when initializing\n",
    "weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "\n",
    "w1 = torch.zeros((input_size, hidden_size), dtype=torch.float32)\n",
    "nn.init.kaiming_normal_(w1)\n",
    "w2 = torch.zeros((hidden_size, num_classes), dtype=torch.float32)\n",
    "nn.init.kaiming_normal_(w2)\n",
    "\n",
    "if cuda:\n",
    "    w1 = w1.cuda()\n",
    "    w2 = w2.cuda()\n",
    "    \n",
    "w1.requires_grad_()\n",
    "w2.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PyTorch Module API\n",
    "\n",
    "The barebone approach quickly becomes inconvenient for larger networks, because we have to track the network's parameters\n",
    "by hand. The PyTorch `nn.Module` API relieves us of this work by tracking all learnable parameters. All we have to do\n",
    "is define the layers and forward pass of the network. Also, PyTorch now takes care of the weight's initialization for us.\n",
    "If we want to use a specific initialization, we can do so in the `__init__` function. \n",
    "\n",
    "Here is an example of the same network as above, but using the `nn.Module` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        # nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]\n",
    "        x = x.view(n, -1)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    input_size = 50\n",
    "    x = torch.zeros((64, input_size), dtype=torch.float32)  # minibatch size 64, feature dimension 50\n",
    "    model = TwoLayerFC(input_size, 42, 10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "    \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PyTorch Sequential API\n",
    "\n",
    "In case we want even more convenience in defining a model than with the `nn.Module` API, we can turn to the\n",
    "`nn.Sequential` API. It combines the steps of defining the layers and defining their connectivity in the forward pass. \n",
    "The trade-off is that we are limited to feed-forward architectures and cannot define more complex network structures. \n",
    "Everything about initialization, training and optimization remains the same as for the modular approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]\n",
    "        return x.view(n, -1)\n",
    "\n",
    "hidden_layer_size = 42\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(50, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10))\n",
    "\n",
    "def test_model(model):\n",
    "    input_size = 50\n",
    "    x = torch.zeros((64, input_size), dtype=torch.float32)  # minibatch size 64, feature dimension 50\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "    \n",
    "test_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MNIST Classifier\n",
    "\n",
    "Now we want to build and train a <b>Convolutional Network</b> on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "dataset. We will be using a three-layer architecture with Dilation, Batch Normalization and ReLU activations and one fully\n",
    "connected classification layer at the end. To see how convolutions with different parameter settings look like, have\n",
    "a look at [this](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) visualization. For more information\n",
    "on dilation, have a look at [this](https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5)\n",
    "article.\n",
    "\n",
    "We can make use of the `nn.Sequential` API to group blocks of conv-batchnorm-relu layers together. The last convolution\n",
    "has kernel_size 1x1 (known as a 1x1 convolution), which serves as a downsampling layer because it reduces the number\n",
    "of channels by taking a weighted sum of all input channels. \n",
    "\n",
    "Batch normalization is a way of improving the overall performance and stability of a network by normalizing inputs \n",
    "between layers. For a detailed explanation, have a look at [this](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "article. The `momentum` parameter controls the weighting of the running averages needed for the normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(16, momentum=0.1),\n",
    "            nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(32, momentum=0.1),\n",
    "            nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1, momentum=0.1),\n",
    "            nn.ReLU())\n",
    "        self.fc = nn.Linear(28*28, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this example we are using the PyTorch Dataloader. It's a convenient solution for batch training and provides easy\n",
    "access to popular datasets, such as the MNIST dataset. After loading the dataset and setting parameters like batch_size, \n",
    "train and an optional transformation (normalization in this case), we can use the dataloader object as an iterator for\n",
    "the training loop. \n",
    "\n",
    "We split the MNIST dataset into a portion for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "tr = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # Normalization for MNIST\n",
    "\n",
    "training_data = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                      download=True, transform=tr)\n",
    "training_loader = torch.utils.data.DataLoader(training_data, batch_size=128,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                     download=True, transform=tr)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128,\n",
    "                                         shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The function parameters() is implemented in nn.Module\n",
    "net = ConvNet()\n",
    "if cuda:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss() # instantiate loss \n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=5e-4) # instantiate optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "history = []\n",
    "\n",
    "for i in range(0, epochs):\n",
    "    for j,(inputs, labels) in enumerate(training_loader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # training loss\n",
    "        loss = cross_entropy(outputs, labels)\n",
    "        \n",
    "        # calculate total loss\n",
    "        history.append(loss.item())\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (j + 1) % 100 == 0:\n",
    "            print(f\"epoch: {i+1:2} batch: {j+1:4} loss: {history[-1]:3.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history);\n",
    "plt.title(\"Training Loss Plot\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set model to evaluation mode \n",
    "# (important for batchnorm/dropout)\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    with torch.no_grad():\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        predicted_class = outputs.max(dim=1)[1]\n",
    "        \n",
    "        correct += (predicted_class == labels).float().sum().item() \n",
    "        \n",
    "accuracy = correct / len(test_data)\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "\n",
    "# torch.save(net.state_dict(), \"net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# net = ConvNet()\n",
    "# net.load_state_dict(torch.load(\"net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Further Reading and Exercises\n",
    "\n",
    "For more reading on PyTorch, there is [this](https://github.com/jcjohnson/pytorch-examples) comprehensive PyTorch \n",
    "Introduction by Justin Johnson.\n",
    "Also, feel free to check out the rest of Tobias Uelwer's [PyTorch Tutorial](https://github.com/tuelwer/pytorch-tutorial).\n",
    "\n",
    "[This](https://github.com/pytorch/examples) is a collection of architectures and ML models implemented in PyTorch. Check\n",
    "it out if you want to take a look at some finished PyTorch example code.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
