{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AOTHKp8PYxZ"
   },
   "source": [
    "HHU Deep Learning, WS2023/24, 24.11.2023\n",
    "\n",
    "Lecture: Prof. Dr. Markus Kollmann\n",
    "\n",
    "Exercises: Nikolas Adaloglou, Felix Michels\n",
    "\n",
    "# Assignment 7 - Convolutional Neural Networks in medical image classification\n",
    "\n",
    "---\n",
    "\n",
    "Submit the solved notebook (not a zip) with your full name plus assignment number for the filename as an indicator, e.g `max_mustermann_a1.ipynb` for assignment 1. If we feel like you have genuinely tried to solve the exercise, you will receive **2** point for this assignment, regardless of the quality of your solution.\n",
    "\n",
    "## <center> DUE FRIDAY 08.12.2023 2:30 pm </center>\n",
    "\n",
    "Drop-off link: [https://uni-duesseldorf.sciebo.de/s/uAOgn8nTNMgeBC6](https://uni-duesseldorf.sciebo.de/s/uAOgn8nTNMgeBC6)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y2pSVIa3mnh"
   },
   "source": [
    "We will use the infamous `medmnist` dataset that contains multiple low-resolution medical imaging datasets for experimentation.\n",
    "\n",
    "You can install it locally via `pip install medmnist` or in google colab with `!pip install medmnist`. To visualize the confusion matrix we suggest using seaborn, it can be install via pip.\n",
    "\n",
    "\n",
    "\n",
    "You will learn how to perform training on the GPU. For this exercise, we highly reccomend using the free GPU resources from [https://colab.research.google.com](https://colab.research.google.com)\n",
    "\n",
    "\n",
    "PS: for the confusion matrix you may use seaborn. You can install it via pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bv-xt81QPYxc"
   },
   "source": [
    "# 1. The story\n",
    "\n",
    "There exists a model that is deployed, which scores around 80% test accuracy in the bloodmnist dataset.\n",
    "\n",
    "Our aim is to create a better model that scores around 90% test accuracy.\n",
    "\n",
    "Your task is to build and train a convolutional network (convnet) classification model for this purpose.\n",
    "\n",
    "Note: This time we also got a test set, so 3 data splits in total. We will **only use the test split once** to measure the performance of the trained model.\n",
    "\n",
    "Finally, we will analyze the predictions of the model via a confusion matrix.\n",
    "\n",
    "## 1.1 Imports and data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7a2TbzwPfuG",
    "outputId": "1960a132-8cac-46f6-a243-8bfd20a1318c"
   },
   "outputs": [],
   "source": [
    "!pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPl8_2axorg6",
    "outputId": "35a96c73-7ec3-40dc-cd26-8f5b3f893f55"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify dataset\n",
    "data_flag = 'bloodmnist'\n",
    "download = True\n",
    "bs = 128\n",
    "info = INFO[data_flag]\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "print(\"n_classes\",n_classes)\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkzbTjqOWBvi"
   },
   "source": [
    "## 1.2 Creating the dataloaders and visuilizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "TvrS7jszV7to",
    "outputId": "b8ee8ffe-b596-4cb6-fdc3-ecccc857b587"
   },
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.5], dtype=torch.float32)\n",
    "std = torch.tensor([0.5], dtype=torch.float32)\n",
    "\n",
    "# needs to be defined\n",
    "plain_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(list(mean), list(std))])\n",
    "\n",
    "\n",
    "# This is a helper function that you can use to visualize the images.\n",
    "# Not necessary to use.\n",
    "def imshow(img, i=0, mean=torch.tensor([0.0], dtype=torch.float32), std=torch.tensor([1], dtype=torch.float32)):\n",
    "    \"\"\"\n",
    "    shows an image on the screen. mean of 0 and variance of 1 will show the images unchanged in the screen\n",
    "    \"\"\"\n",
    "    # undoes the normalization\n",
    "    unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    plt.subplot(1, 10 ,i+1)\n",
    "    npimg = unnormalize(img).numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# load the data\n",
    "train_ds = DataClass(split='train', transform=plain_transform, download=download)\n",
    "val_ds = DataClass(split='val', transform=plain_transform, download=download)\n",
    "test_ds = DataClass(split='test', transform=plain_transform, download=download)\n",
    "\n",
    "# create the data loaders\n",
    "### START CODE HERE ### (approx. 3 lines)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# check image shape and labels\n",
    "img1, lab = next(iter(train_loader))\n",
    "print(img1.shape)\n",
    "print(lab.shape)\n",
    "\n",
    "\n",
    "# How I used the imshow function to visualize samples:\n",
    "plt.figure(figsize = (50,20))\n",
    "for i in range(10):\n",
    "    imshow(train_ds[i][0], i, mean, std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CObgDaboVbd5"
   },
   "source": [
    "## 1.3 Implement a small Conv Net\n",
    "\n",
    "Define the following model:\n",
    "\n",
    "- 2 Blocks of Conv-MaxPool-ReLU Layers\n",
    "- 1 Block Conv-ReLU\n",
    "- 1 Block of Linear-ReLU-Linear Layers\n",
    "\n",
    "`base_channels` is a parameter that you should choose. It controls the width (number of parameters per layer) of the whole network.\n",
    "\n",
    "\n",
    "The model will be initialized as follows:\n",
    "\n",
    "```\n",
    "model = Net(base_channels=2)  # 2 is for illustration purposes\n",
    "```\n",
    "\n",
    "A test function is provided so that you can check if you model can perform a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbuRWwRMUhwM",
    "outputId": "49117cbb-b19f-48e5-fdf6-31f92e6eddc5"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, base_channels=32, n_classes=8):\n",
    "        super(Net, self).__init__()\n",
    "        ### START CODE HERE ### (approx. 11 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ### (approx. 6 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        return x3\n",
    "\n",
    "\n",
    "def test_model_fw():\n",
    "    model = Net(base_channels=4)\n",
    "    x = torch.rand(1,3,28,28)\n",
    "    out = model(x)\n",
    "    assert list(out.shape) == [1,8]\n",
    "    print(\"Test passed\")\n",
    "    print(model)\n",
    "test_model_fw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtj_T-CuUgpk"
   },
   "source": [
    "## 1.4 Write the train/val code to train the model\n",
    "\n",
    " - Train the convnet model. Specify the hyperparameters and write the train/val code to train the model.\n",
    " - Save the model with the lowest validation loss for testing.\n",
    " - You need to keep track of the train accuracy, val accuracy and train loss and val loss for plotting the results of your training.\n",
    "\n",
    "\n",
    "Some help to save and load your model:\n",
    "```\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, path)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wij0eMZTPYxi"
   },
   "source": [
    "In the `validate` function you need to:\n",
    "\n",
    "1. Move data and labels to the correct device\n",
    "2. Get a model output\n",
    "3. Compute a prediction based on the model output\n",
    "4. Compute the number of correct predictions and accumulate it in the `correct` variable\n",
    "5. Accumulate the total number of examples in the `total` variable\n",
    "5. Get the Loss for the model output and save it\n",
    "6. After the loop, compute the model's accuracy from `correct` and `total`\n",
    "7. Return the accuracy and mean loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4DvWfPSpmru"
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct, total = 0, 0\n",
    "    loss_step = []\n",
    "    bs = val_loader.batch_size\n",
    "    with torch.no_grad():\n",
    "        for inp_data, labels in val_loader:\n",
    "            ### START CODE HERE ### (approx. 10 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        return val_acc , val_loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSGBpbyHPYxi"
   },
   "source": [
    "In the `train_one_epoch` function tou need to:\n",
    "\n",
    "1. Move data and labels to the correct device\n",
    "2. Get model outputs\n",
    "3. Compute the loss\n",
    "4. Zero the optimizer's gradients\n",
    "5. Compute the backward pass\n",
    "6. Update the model's parameters\n",
    "7. Compute the model's accuracy, similar to the `validate` function, use `correct` and `total` for accumulation\n",
    "8. After the epoch, compute the model's accuracy and mean loss for this epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6VDls0OPYxj"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_step = []\n",
    "    correct, total = 0, 0\n",
    "    bs = train_loader.batch_size\n",
    "    for (inp_data, labels) in train_loader:\n",
    "        ### START CODE HERE ### (approx. 14 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return loss_curr_epoch, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL-DGCoTPYxj"
   },
   "source": [
    "The `train` function you need to:\n",
    "\n",
    "1. Get the model's accuracy and mean loss for an epoch, using the `train_one_epoch` function\n",
    "2. Get validation accuracy and loss, using the `validate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDH08C96PYxj"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs, train_loader, val_loader, device):\n",
    "    best_val_loss = 100\n",
    "    best_val_acc = 0\n",
    "    dict_log = {\"train_acc_epoch\":[], \"val_acc_epoch\":[], \"loss_epoch\":[], \"val_loss\":[]}\n",
    "    train_acc, _ = validate(model, train_loader, device)\n",
    "    val_acc, _ = validate(model, val_loader, device)\n",
    "    print(f'Init Accuracy of the model: Train:{train_acc:.3f} \\t Val:{val_acc:3f}')\n",
    "    for epoch in range(num_epochs):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print epoch results to screen\n",
    "        print(f'Ep {epoch}/{num_epochs}: Accuracy : Train:{train_acc:.2f} \\t Val:{val_acc:.2f} || Loss: Train {loss_curr_epoch:.3f} \\t Val {val_loss:.3f}')\n",
    "        # Track stats\n",
    "        dict_log[\"train_acc_epoch\"].append(train_acc)\n",
    "        dict_log[\"val_acc_epoch\"].append(val_acc)\n",
    "        dict_log[\"loss_epoch\"].append(loss_curr_epoch)\n",
    "        dict_log[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': val_loss,\n",
    "                  }, f'best_model_min_val_loss.pth')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': val_loss,\n",
    "                  }, f'best_model_max_val_acc.pth')\n",
    "    return dict_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rr7cfErPYxk"
   },
   "source": [
    "## 1.5 Choose a set of hyperparameters and train the convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JEUSx_APYxk"
   },
   "source": [
    "Choose a learning rate, weight decay, number of epochs and number of base channels. Initialize your network and run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xF3XVLfsPYxk",
    "outputId": "ed5b69b7-81d4-464b-8303-3cc1132b3a9e"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "### START CODE HERE ### (approx. 5 lines)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "dict_log = train(model, optimizer, num_epochs, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CwJDG7-PYxk"
   },
   "source": [
    "#### Expected results\n",
    "```\n",
    "Ep 0/100: Accuracy : Train:16.95 \t Val:17.01 || Loss: Train 2.070 \t Val 2.056\n",
    "...................................\n",
    "Ep 20/100: Accuracy : Train:75.65 \t Val:75.30 || Loss: Train 0.702 \t Val 0.647\n",
    "...................................\n",
    "Ep 74/100: Accuracy : Train:89.46 \t Val:88.40 || Loss: Train 0.313 \t Val 0.317\n",
    "...................................\n",
    "Ep 96/100: Accuracy : Train:91.65 \t Val:89.66 || Loss: Train 0.257 \t Val 0.272\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYB13GZXfgcZ"
   },
   "source": [
    "## 1.6 Load your best model and test it on the test set\n",
    "\n",
    "The code below load the checkpoint for the best models, based on validation loss and accuracy and finds the test loss and test accuracy.\n",
    "\n",
    "Is it close to the train and val set?\n",
    "\n",
    "You should get around 90% val accuracy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U16Jv3g5ffYJ",
    "outputId": "8cf2de3e-44d5-4a2f-e2e2-1bb12726f81e"
   },
   "outputs": [],
   "source": [
    "path = \"best_model_min_val_loss.pth\"\n",
    "path2 = \"best_model_max_val_acc.pth\"\n",
    "def load_model(model, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model {path} is loaded from epoch {checkpoint['epoch']} , loss {checkpoint['loss']}\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, path, test_loader, device='cuda'):\n",
    "    model = load_model(model, path)\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    return validate(model, test_loader, device)\n",
    "\n",
    "### START CODE HERE ### (approx. 3 lines)\n",
    "\n",
    " ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64W8YCNP1r2h"
   },
   "source": [
    "## 1.7 Plot train/val accuracies and train/val loss\n",
    "\n",
    "Complete the `plot_stats` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hnSOqyjy1tCp",
    "outputId": "47efd94c-9ae4-4ddf-ef7b-487bee1fce22"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_stats(dict_log, baseline=90, title=None):\n",
    "    fontsize = 14\n",
    "    figsize = (15,10)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "    plt.subplot(2,1,1)\n",
    "    ### START CODE HERE ### (approx. 5 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    plt.ylabel('Accuracy in %')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.title(\"Accuracy over epochs\", fontsize=fontsize)\n",
    "    plt.axhline(y=baseline, color='red', label=\"Acceptable accuracy\")\n",
    "    plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(dict_log[\"loss_epoch\"] , label=\"Training\")\n",
    "\n",
    "    ### START CODE HERE ### (approx. 3 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    plt.ylabel('Loss value')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.title(\"Loss over epochs\", fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    if title is not None:\n",
    "        plt.savefig(title)\n",
    "\n",
    "plot_stats(dict_log, baseline=90, title='figs/exp_1_bloddmnist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6wSjbYsPYxm"
   },
   "source": [
    "\n",
    "![exp_1_bloddmnist.png](https://raw.githubusercontent.com/HHU-MMBS/Deep-Learning-Exercise-Extras/main/a07-08_mednist/figs/exp_1_bloddmnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzcAxO8_A21P"
   },
   "source": [
    "## 1.8 Use sklearn.metrics to get insights on per class predictions\n",
    "```\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "```\n",
    "You will need to iterate through the test data with the trained model and generate `y_true` and `y_pred` lists ( labels and predictions respectivly)\n",
    "\n",
    "\n",
    "### Questions to think about:\n",
    "\n",
    "Analyze and try to understand the outputs:\n",
    "\n",
    "- Which class is the harder to predict?\n",
    "\n",
    "- Are the predictions equally good for all classes? Which class(es) is misclassified more frequently?\n",
    "\n",
    "- Is accuracy always a good metric? And if not why are the other metrics better to infer the behaviour of the model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2I2f0DFnU85Q",
    "outputId": "8c872466-d546-4077-b815-d0cc3763bc81"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "fontsize = 14\n",
    "figsize=(14,14)\n",
    "\n",
    "# Load the best model\n",
    "### START CODE HERE ### (approx. 2 lines)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# get test accuracy\n",
    "test_acc, test_loss = validate(model, test_loader, device)\n",
    "print(test_acc, test_loss)\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data[0].to(device),\n",
    "            test_data[1].to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "print(classification_report(\n",
    "    y_true, y_pred,  digits=3))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "sns.heatmap(conf_matrix/np.sum(conf_matrix), annot=True,\n",
    "            fmt='.2%', cmap='Blues')\n",
    "plt.ylabel('Actual Category', fontsize=fontsize)\n",
    "plt.xlabel('Predicted Category', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"figs/exp_1_conf_mat.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nKXO3CQPYxn"
   },
   "source": [
    "![exp_1_conf.png](https://raw.githubusercontent.com/HHU-MMBS/Deep-Learning-Exercise-Extras/main/a07-08_mednist/figs/exp_1_conf_mat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZVsXf1yPYxn"
   },
   "source": [
    "# 2. Training tricks and scaling up our model and data\n",
    "\n",
    "Well, we have trained our small convnet that scores around 90% test accuracy on bloodmnist (17,092 images).\n",
    "\n",
    "Some questions still arise:\n",
    "- Is our convet good enough?\n",
    "- How can we further increase our performance?\n",
    "- Does it work well enoough with larger amounts of data?\n",
    "- What is the critical component of larger scale convnets like Resnets?\n",
    "\n",
    "To this end, in the 2nd part we will:\n",
    "\n",
    "- First we will first add some image augmentations to our training data.\n",
    "- Then we will see the impact of [Batch normalization](https://arxiv.org/abs/1502.03167) to our small convnet, which is a core component of large scale convnet like resnet and observe the training dynamics.\n",
    "- Afterwards, we will scale-up our model from the small convnet to the famous resnet-18.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**End Goal**: We would like to have a validation accuracy over 94% for bloodmnist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry9nBQPNPYxn"
   },
   "source": [
    "## 2.1 Define the data augmentation pipelines\n",
    "\n",
    "Training data augmentations you should implement:\n",
    "- Horizontal flip with 50% probability\n",
    "- Vertical flip with 50% probability\n",
    "- Random crop images to 80-100 % of the initial size\n",
    "- Resize images to 28x28\n",
    "- intensity jitter: 20% brightness and 20% contrast with 80% probability\n",
    "- Mean/std norm with mean=0.5 and std=0.5 for all channels\n",
    "\n",
    "At val/test time, only mean/std normalization will be applied.\n",
    "\n",
    "Hint: use [torchvision](https://pytorch.org/vision/stable/transforms.html) for creating the data augmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1ooXCkYPYxn",
    "outputId": "feeeeb69-2aa1-47b6-ae96-5d9b4868923a"
   },
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.5], dtype=torch.float32)\n",
    "std = torch.tensor([0.5], dtype=torch.float32)\n",
    "batch_size = 128\n",
    "\n",
    "### START CODE HERE ### (approx. 8 lines)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "plain_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(list(mean), list(std))])\n",
    "\n",
    "# This is a helper function that you can use to visualize the images.\n",
    "# Not necessary to use.\n",
    "def imshow(img, i=0, mean=torch.tensor([0.0], dtype=torch.float32), std=torch.tensor([1], dtype=torch.float32)):\n",
    "    \"\"\"\n",
    "    shows an image on the screen. mean of 0 and variance of 1 will show the images unchanged in the screen\n",
    "    \"\"\"\n",
    "    # undoes the normalization\n",
    "    unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    plt.subplot(1, 10 ,i+1)\n",
    "    npimg = unnormalize(img).numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# load the data\n",
    "train_ds = DataClass(split='train', transform=train_transform_augs, download=download)\n",
    "train_plain = DataClass(split='train', transform=plain_transform, download=download)\n",
    "val_ds = DataClass(split='val', transform=plain_transform, download=download)\n",
    "test_ds = DataClass(split='test', transform=plain_transform, download=download)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "train_loader_plain = data.DataLoader(dataset=train_plain, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = data.DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = data.DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "img1, lab = next(iter(train_loader))\n",
    "\n",
    "# How I used the imshow function to visualize samples:\n",
    "plt.figure(figsize = (50,20))\n",
    "for i in range(10):\n",
    "    imshow(train_ds[i][0], i, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8qI_gudPYxo"
   },
   "source": [
    "## 2.2 Add batch normalization to our convnet\n",
    "\n",
    "Take the convnet from our previous exercise and add 3 batch normalization layers after the convolutions or activation functions (you should decide where).\n",
    "\n",
    "Questions that may help:\n",
    "\n",
    "- Should you use `BatchNorm1d`, `BatchNorm2d`, or `BatchNorm3d`? What are the arguments that you should specify? Check the documentation.\n",
    "- In which order should you apply the normalization , i.e. before or after the activation function?\n",
    "- What training behaviour do you expect? Guess or check the published paper.\n",
    "\n",
    "The network WITHOUT batch norm should look like this:\n",
    "\n",
    "\n",
    "```\n",
    " Net(\n",
    "  (conv1): Conv2d(in_channels, base_channels, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (pool1): MaxPool2d(kernel_size=2, stride=2)\n",
    "  (relu1): ReLU(inplace=True)\n",
    "  (conv2): Conv2d(in_channels, base_channels*2 , kernel_size=(3, 3), stride=(1, 1))\n",
    "  (pool2): MaxPool2d(kernel_size=2, stride=2)\n",
    "  (relu2): ReLU(inplace=True)\n",
    "  (conv3): Conv2d(in_channels, base_channels*4, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (relu3): ReLU(inplace=True)\n",
    "\n",
    "    (fc0): Linear(in_channels, out_features=64)\n",
    "    (act0): ReLU(inplace=True)\n",
    "    (fc2): Linear(in_channels, n_classes)\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "The model can be initialized as follows:\n",
    "\n",
    "```\n",
    "model = Net(base_channels=2, n_classes=2, batch_norm=True) # batch_norm is Optional to implement.\n",
    "```\n",
    "Use the same number of base channels as in the previous model without batchnorm so that you can see the differences while training.\n",
    "\n",
    "- `batch_norm` is a boolean argument. It is optional to implement it this way. The idea is to specify whether we want to add batch norm in the architecture or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWZhgM6LPYxo",
    "outputId": "94aa3dc7-8708-4968-bc7f-47d939480589"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, base_channels=2, n_classes=2, batch_norm=False):\n",
    "        super(Net, self).__init__()\n",
    "        ### START CODE HERE ### (approx. 15 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        return x3\n",
    "\n",
    "def test():\n",
    "    c = 8\n",
    "    model = Net(base_channels=4,n_classes=c, batch_norm=True)\n",
    "    x = torch.rand(1,3,28,28)\n",
    "    out = model(x)\n",
    "    assert list(out.shape) == [1,c]\n",
    "    model = Net(base_channels=4,n_classes=c, batch_norm=False)\n",
    "    assert list(model(x).shape) == [1,c]\n",
    "    print(\"Test passed\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cONQn_NLPYxo"
   },
   "source": [
    "## 2.3 Train the convnet with batch normalization\n",
    "\n",
    "Use the already implemented functions from part 1 to train the model with the new data and batch norm.\n",
    "\n",
    "Anything above 50 epochs should be enough. Use the same hyperparameter setup that you found in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCL4Ws_lPYxo",
    "outputId": "0f9d0860-b6d8-4da4-9044-b6b8b4f90ea2"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "num_epochs = 80\n",
    "\n",
    "print(\"Training with Batch Normalization and image augmentations\")\n",
    "### START CODE HERE ### (approx. 4 lines)\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zweuofysPYxp"
   },
   "source": [
    "**Expected results:**\n",
    "\n",
    "```\n",
    "Ep 0/80: Accuracy : Train:56.88 \t Val:73.62 || Loss: Train 1.403 \t Val 0.872\n",
    "Ep 1/80: Accuracy : Train:73.20 \t Val:76.74 || Loss: Train 0.812 \t Val 0.665\n",
    "Ep 2/80: Accuracy : Train:76.00 \t Val:79.21 || Loss: Train 0.675 \t Val 0.575\n",
    "Ep 3/80: Accuracy : Train:78.16 \t Val:80.29 || Loss: Train 0.602 \t Val 0.528\n",
    "Ep 4/80: Accuracy : Train:80.00 \t Val:80.41 || Loss: Train 0.554 \t Val 0.512\n",
    "Ep 5/80: Accuracy : Train:81.17 \t Val:83.59 || Loss: Train 0.519 \t Val 0.459\n",
    "Ep 6/80: Accuracy : Train:82.01 \t Val:83.47 || Loss: Train 0.496 \t Val 0.443\n",
    "Ep 7/80: Accuracy : Train:83.48 \t Val:85.28 || Loss: Train 0.467 \t Val 0.414\n",
    "Ep 8/80: Accuracy : Train:84.24 \t Val:86.36 || Loss: Train 0.442 \t Val 0.389\n",
    "Ep 9/80: Accuracy : Train:84.81 \t Val:85.88 || Loss: Train 0.419 \t Val 0.385\n",
    "Ep 10/80: Accuracy : Train:85.66 \t Val:88.46 || Loss: Train 0.398 \t Val 0.336\n",
    "Ep 11/80: Accuracy : Train:86.48 \t Val:87.38 || Loss: Train 0.387 \t Val 0.356\n",
    "Ep 12/80: Accuracy : Train:87.00 \t Val:88.22 || Loss: Train 0.369 \t Val 0.339\n",
    "Ep 13/80: Accuracy : Train:87.05 \t Val:89.06 || Loss: Train 0.359 \t Val 0.313\n",
    "Ep 14/80: Accuracy : Train:87.42 \t Val:88.10 || Loss: Train 0.345 \t Val 0.310\n",
    "Ep 15/80: Accuracy : Train:87.89 \t Val:90.81 || Loss: Train 0.337 \t Val 0.273\n",
    "Ep 16/80: Accuracy : Train:88.71 \t Val:90.44 || Loss: Train 0.324 \t Val 0.275\n",
    "Ep 17/80: Accuracy : Train:88.63 \t Val:88.64 || Loss: Train 0.311 \t Val 0.325\n",
    "Ep 18/80: Accuracy : Train:89.21 \t Val:91.29 || Loss: Train 0.306 \t Val 0.259\n",
    "Ep 19/80: Accuracy : Train:89.04 \t Val:91.71 || Loss: Train 0.301 \t Val 0.247\n",
    "Ep 20/80: Accuracy : Train:89.50 \t Val:90.56 || Loss: Train 0.291 \t Val 0.259\n",
    "Ep 21/80: Accuracy : Train:89.75 \t Val:90.56 || Loss: Train 0.286 \t Val 0.254\n",
    "Ep 22/80: Accuracy : Train:90.05 \t Val:87.20 || Loss: Train 0.279 \t Val 0.324\n",
    "...\n",
    "Ep 30/80: Accuracy : Train:91.23 \t Val:92.37 || Loss: Train 0.242 \t Val 0.206\n",
    "Ep 31/80: Accuracy : Train:91.43 \t Val:92.79 || Loss: Train 0.239 \t Val 0.202\n",
    "Ep 32/80: Accuracy : Train:91.58 \t Val:91.59 || Loss: Train 0.233 \t Val 0.220\n",
    "Ep 33/80: Accuracy : Train:91.89 \t Val:90.38 || Loss: Train 0.232 \t Val 0.266\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2_1H3D0PYxp"
   },
   "source": [
    "## 2.4 Test the model and plot the training stats.\n",
    "\n",
    "Use the same function from part 1 to:\n",
    "- Plot the training stats.\n",
    "- Test the model on the test data and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sDTXjgRPYxp",
    "outputId": "aedb961e-1108-4112-a60d-1ceef02f7183"
   },
   "outputs": [],
   "source": [
    "\n",
    "### START CODE HERE ### (approx. 5 lines)\n",
    "\n",
    " ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh4OMzCNPYxq"
   },
   "source": [
    "**Expected Results:**\n",
    "\n",
    "![exp_2.png](https://raw.githubusercontent.com/HHU-MMBS/Deep-Learning-Exercise-Extras/main/a07-08_mednist/figs/exp_2_bloddmnist.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVRSEZ_wPYxq"
   },
   "source": [
    "## 2.5 Load, mofidy and train the resnet18\n",
    "\n",
    "Now we will use `torchvision.models`.\n",
    "\n",
    "- Load Resnet 18 from torchvision. How many trainble parameters does the network have?\n",
    "- Important: Do **not** use pretrained weights. We will train the model from scratch.\n",
    "\n",
    "Print the model.\n",
    "A. How should you modify the last linear layer?\n",
    "B. How is the first layer look like? If you were to modify the input conv layer how would you modify it for `bloddmnist` (3-channeled 28x28 images) ?\n",
    "Think about the size of the input image and the first conv layer. Is it a good choice? If not, what would you replace it with? Hint: resnets are designed for higher resolution images, i.e. 224x224.\n",
    "\n",
    "\n",
    "Train for at least 40 epochs and save the model with the best val score Compare the results with our small convnet.\n",
    "\n",
    "Expected results: 94-95 % validation set accuracy.\n",
    "No augmentations should been applied in the validation set. In this way, we can make an estimation of the test set performance (given the i.i.d. assumption).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e03QbsD_PYxq",
    "outputId": "f47320e0-a171-4af3-88e8-c6a66dee9db7"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def get_resnet18(in_channels, n_classes):\n",
    "    ### START CODE HERE ### (approx. 3 lines)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return model\n",
    "\n",
    "model_3 = get_resnet18(3, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tYAuTz6PYxq"
   },
   "source": [
    "## 2.6 Train the resnet18\n",
    "\n",
    "- Train the resnet for at least 40 epochs (we trained for 80 epochs)\n",
    "- Plot train stats\n",
    "- Compute test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ghGL_zZPYxr",
    "outputId": "b593cf34-a35d-4d07-8e32-7e93807b8dbc"
   },
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "path = \"best_model_min_val_loss.pth\"\n",
    "### START CODE HERE ### (approx. 6 lines)\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI6fBOJgPYxr"
   },
   "source": [
    "### Expected results\n",
    "```\n",
    "Ep 1/80: Accuracy : Train:78.91 \t Val:82.63 || Loss: Train 0.643 \t Val 0.484\n",
    ".........\n",
    "Ep 13/80: Accuracy : Train:90.65 \t Val:92.55 || Loss: Train 0.276 \t Val 0.212\n",
    ".........\n",
    "Ep 32/80: Accuracy : Train:94.26 \t Val:94.47 || Loss: Train 0.176 \t Val 0.154\n",
    ".........\n",
    "Ep 42/80: Accuracy : Train:94.99 \t Val:94.95 || Loss: Train 0.152 \t Val 0.139\n",
    ".........\n",
    "Ep 60/80: Accuracy : Train:95.98 \t Val:95.31 || Loss: Train 0.126 \t Val 0.132\n",
    ".........\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OudDra9_PYxr"
   },
   "source": [
    "\n",
    "![exp_3.png](https://raw.githubusercontent.com/HHU-MMBS/Deep-Learning-Exercise-Extras/main/a07-08_mednist/figs/exp_3_bloddmnist_resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62BrMMx7PYxr"
   },
   "source": [
    "# 3. Scaling up to a larger dataset: pathmnist\n",
    "\n",
    "In this last part, we will scale up our dataset. We will use 'pathmnist' that has  `107,180 images`.\n",
    "\n",
    "**End Goal**: We would like to have a validation accuracy over 90% on pathmnist.\n",
    "\n",
    "## 3.1 Load the new dataset and create the dataloaders\n",
    "We will use the same augmentations from part 2. Use a batch size of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4H5Awr0PYxr",
    "outputId": "6ec2df76-142c-441d-9ce4-baf9cd793948"
   },
   "outputs": [],
   "source": [
    "# Specify dataset\n",
    "data_flag = 'pathmnist'\n",
    "download = True\n",
    "batch_size = 256\n",
    "info = INFO[data_flag]\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "print(\"n_classes\",n_classes)\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "mean = torch.tensor([0.5], dtype=torch.float32)\n",
    "std = torch.tensor([0.5], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Note: train_transform_augs needs to be already defined from part 2\n",
    "\n",
    "plain_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(list(mean), list(std))])\n",
    "\n",
    "# load the data\n",
    "train_ds = DataClass(split='train', transform=train_transform_augs, download=download)\n",
    "train_plain = DataClass(split='train', transform=plain_transform, download=download)\n",
    "val_ds = DataClass(split='val', transform=plain_transform, download=download)\n",
    "test_ds = DataClass(split='test', transform=plain_transform, download=download)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "train_loader_plain = data.DataLoader(dataset=train_plain, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = data.DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = data.DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "plt.figure(figsize = (50,20))\n",
    "for i in range(10):\n",
    "    imshow(train_ds[i][0], i, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cK0NIdjPYxr"
   },
   "source": [
    "## 3.2 Train resnet18 on the pathmnist data\n",
    "\n",
    "The small convnet with batch-normalization we built in part 2 scores around 87% validation accuracy on pathmnist.\n",
    "\n",
    "We would like to have at least 90% though.\n",
    "\n",
    "You task is to surpass the performance of the convnet with resnet18. Train for at least 15 epochs (we trained for 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiUzBXNwPYxs",
    "outputId": "280e505b-4f51-463f-eeed-842d8c0df3d1"
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# get the model and optimizer and launch training\n",
    "### START CODE HERE ### (approx. 4 lines)\n",
    "\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "395Y-IfQPYxs"
   },
   "source": [
    "### Expected results\n",
    "\n",
    "```\n",
    "Ep 0/30: Accuracy : Train:54.65 \t Val:59.77 || Loss: Train 1.250 \t Val 1.109\n",
    "Ep 1/30: Accuracy : Train:68.83 \t Val:67.56 || Loss: Train 0.846 \t Val 0.856\n",
    "Ep 2/30: Accuracy : Train:74.67 \t Val:77.20 || Loss: Train 0.683 \t Val 0.626\n",
    "Ep 3/30: Accuracy : Train:78.21 \t Val:77.19 || Loss: Train 0.592 \t Val 0.616\n",
    "Ep 4/30: Accuracy : Train:80.79 \t Val:75.09 || Loss: Train 0.523 \t Val 0.701\n",
    "Ep 5/30: Accuracy : Train:82.82 \t Val:77.75 || Loss: Train 0.473 \t Val 0.612\n",
    "Ep 6/30: Accuracy : Train:84.26 \t Val:75.41 || Loss: Train 0.432 \t Val 0.812\n",
    "Ep 7/30: Accuracy : Train:85.42 \t Val:82.15 || Loss: Train 0.403 \t Val 0.493\n",
    "Ep 8/30: Accuracy : Train:86.25 \t Val:85.77 || Loss: Train 0.380 \t Val 0.376\n",
    "Ep 9/30: Accuracy : Train:86.97 \t Val:84.85 || Loss: Train 0.359 \t Val 0.434\n",
    "Ep 10/30: Accuracy : Train:87.64 \t Val:87.33 || Loss: Train 0.344 \t Val 0.348\n",
    "Ep 11/30: Accuracy : Train:88.25 \t Val:82.01 || Loss: Train 0.327 \t Val 0.498\n",
    "Ep 12/30: Accuracy : Train:88.48 \t Val:85.16 || Loss: Train 0.318 \t Val 0.402\n",
    "Ep 13/30: Accuracy : Train:89.02 \t Val:64.41 || Loss: Train 0.302 \t Val 1.363\n",
    "Ep 14/30: Accuracy : Train:89.69 \t Val:87.13 || Loss: Train 0.288 \t Val 0.355\n",
    "Ep 15/30: Accuracy : Train:89.99 \t Val:76.81 || Loss: Train 0.278 \t Val 0.824\n",
    "Ep 16/30: Accuracy : Train:90.34 \t Val:85.36 || Loss: Train 0.270 \t Val 0.422\n",
    "Ep 17/30: Accuracy : Train:90.33 \t Val:88.70 || Loss: Train 0.266 \t Val 0.318\n",
    "Ep 18/30: Accuracy : Train:90.86 \t Val:90.31 || Loss: Train 0.255 \t Val 0.270\n",
    "Ep 19/30: Accuracy : Train:91.02 \t Val:88.05 || Loss: Train 0.252 \t Val 0.338\n",
    "Ep 20/30: Accuracy : Train:91.33 \t Val:85.67 || Loss: Train 0.242 \t Val 0.465\n",
    "Ep 21/30: Accuracy : Train:91.67 \t Val:88.29 || Loss: Train 0.234 \t Val 0.332\n",
    "Ep 22/30: Accuracy : Train:91.70 \t Val:88.51 || Loss: Train 0.234 \t Val 0.320\n",
    "Ep 23/30: Accuracy : Train:91.96 \t Val:86.76 || Loss: Train 0.226 \t Val 0.363\n",
    "...\n",
    "Ep 26/30: Accuracy : Train:92.47 \t Val:83.11 || Loss: Train 0.209 \t Val 0.488\n",
    "Ep 27/30: Accuracy : Train:92.68 \t Val:90.78 || Loss: Train 0.205 \t Val 0.266\n",
    "Ep 28/30: Accuracy : Train:92.88 \t Val:91.98 || Loss: Train 0.198 \t Val 0.225\n",
    "Ep 29/30: Accuracy : Train:92.99 \t Val:90.18 || Loss: Train 0.196 \t Val 0.271\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G94PS6f5PYxs",
    "outputId": "98befda0-927c-4f01-dfdd-c74ef94cb063"
   },
   "outputs": [],
   "source": [
    "path = \"best_model_min_val_loss.pth\"\n",
    "# Load model and plot training curves and compute test accuracy\n",
    "### START CODE HERE ### (approx. 4 lines)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvveDStyPYxt"
   },
   "source": [
    "## Expected results\n",
    "\n",
    "Model best_model_min_val_loss.pth is loaded from epoch 28 , loss 0.22461184859275818\n",
    "Test acc 83.314735 , test loss 0.58688146\n",
    "\n",
    "![exp_4_pathmnist_resnet.png](https://raw.githubusercontent.com/HHU-MMBS/Deep-Learning-Exercise-Extras/main/a07-08_mednist/figs/exp_4_pathmnist_resnet.png)\n",
    "\n",
    "\n",
    "How is the train/val/test accuracies are related? What differences do you observe compared to using when scaling up to the larger dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI9dGYDFPYxt"
   },
   "source": [
    "# Further reading\n",
    "- [Skip connections](https://theaisummer.com/skip-connections/)\n",
    "- [Why does batch norm work](https://www.youtube.com/watch?v=nUUqwaxLnWs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
